// Integration tests for the complete training pipeline
//
// Tests end-to-end training workflows including data generation,
// LoRA training, compression, and export.

use simplex_std::dual;
use simplex_std::vec::Vec;
use simplex_std::string::String;
use simplex_learning::tensor::DualTensor;
use simplex_training::{
 SpecialistPipeline, PipelineConfig, TrainedSpecialist, PipelineResult,
 BatchTrainer, BatchConfig, BatchResult,
 SpecialistDomain, TrainingConfig,
 LoRAConfig, LoRAModel, LoRALayer,
 DataGenerator, DataLoader, DataLoaderConfig,
 DocumentGenerator, CodeGenerator, ReasoningGenerator,
 AnnealOptimizer, AnnealTrainingState,
 GgufExporter, GgufConfig,
};

#[cfg(test)]
mod pipeline_creation_tests {
 use super::*;

 #[test]
 fn test_pipeline_for_document() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document);
 let pipeline = SpecialistPipeline::new(config);

 let metrics = pipeline.metrics();
 assert_eq!(metrics.step, 0);
 assert_eq!(metrics.epoch, 0);
 }

 #[test]
 fn test_pipeline_for_code() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Code);
 let pipeline = SpecialistPipeline::new(config);

 // Code specialist should have higher-rank LoRA
 assert!(config.lora.rank >= 16);
 }

 #[test]
 fn test_pipeline_for_classification() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Classification);
 let pipeline = SpecialistPipeline::new(config);

 // Classification should have simpler config
 assert!(config.lora.rank <= 16);
 }

 #[test]
 fn test_pipeline_for_reasoning() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Reasoning);
 let pipeline = SpecialistPipeline::new(config);

 let metrics = pipeline.metrics();
 assert!((metrics.best_loss - f64::MAX).abs() < 1.0);
 }

 #[test]
 fn test_pipeline_custom_samples() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .with_samples(100, 20);

 assert_eq!(config.train_samples, 100);
 assert_eq!(config.val_samples, 20);
 }

 #[test]
 fn test_pipeline_without_compression() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .without_compression();

 assert!(!config.compress);
 }

 #[test]
 fn test_pipeline_custom_output() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Code)
 .with_output("custom/path");

 assert_eq!(config.output_dir, "custom/path");
 }
}

#[cfg(test)]
mod training_loop_tests {
 use super::*;

 #[test]
 fn test_mini_training_run() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .with_samples(10, 5);

 let mut pipeline = SpecialistPipeline::new(config);

 // Run a very short training
 let result = pipeline.train();

 // Should have completed some steps
 assert!(result.specialist.steps > 0);
 assert!(!result.specialist.history.is_empty());
 }

 #[test]
 fn test_loss_decreases() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Classification)
 .with_samples(50, 10);

 let mut pipeline = SpecialistPipeline::new(config);
 let result = pipeline.train();

 // First loss vs last loss (with simulated training)
 if result.specialist.history.len() >= 10 {
 let first_avg: f64 = result.specialist.history[..5].iter().sum::<f64>() / 5.0;
 let last_avg: f64 = result.specialist.history[result.specialist.history.len()-5..].iter().sum::<f64>() / 5.0;

 // Loss should generally decrease (or at least not explode)
 assert!(last_avg < first_avg * 2.0);
 }
 }

 #[test]
 fn test_metrics_update() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Code)
 .with_samples(20, 5);

 let mut pipeline = SpecialistPipeline::new(config);

 // Train
 pipeline.train();

 let metrics = pipeline.metrics();
 assert!(metrics.step > 0);
 assert!(metrics.loss < f64::MAX);
 }

 #[test]
 fn test_best_loss_tracking() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Reasoning)
 .with_samples(30, 10);

 let mut pipeline = SpecialistPipeline::new(config);
 let result = pipeline.train();

 // Best loss should be the minimum from validation
 assert!(result.specialist.best_val_loss < f64::MAX);
 assert!(result.specialist.best_val_loss > 0.0);
 }

 #[test]
 fn test_pipeline_reset() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .with_samples(10, 5);

 let mut pipeline = SpecialistPipeline::new(config);

 // Train once
 pipeline.train();
 assert!(pipeline.metrics().step > 0);

 // Reset
 pipeline.reset();

 let metrics = pipeline.metrics();
 assert_eq!(metrics.step, 0);
 assert_eq!(metrics.epoch, 0);
 }
}

#[cfg(test)]
mod batch_training_tests {
 use super::*;

 #[test]
 fn test_batch_config_default() {
 let config = BatchConfig::default_domains();

 assert_eq!(config.domains.len(), 4);
 assert!(config.sequential);
 }

 #[test]
 fn test_batch_config_add_domain() {
 let config = BatchConfig::default_domains()
 .add_domain(SpecialistDomain::Math);

 assert_eq!(config.domains.len(), 5);
 }

 #[test]
 fn test_batch_trainer_creation() {
 let config = BatchConfig::default_domains();
 let trainer = BatchTrainer::new(config);

 assert_eq!(trainer.results().len(), 0);
 assert_eq!(trainer.failures().len(), 0);
 }

 #[test]
 fn test_batch_train_single() {
 let config = BatchConfig::default_domains()
 .with_samples(10, 5);

 let mut trainer = BatchTrainer::new(config);
 let result = trainer.train_one(&SpecialistDomain::Document);

 assert!(result.is_some());
 assert_eq!(trainer.results().len(), 1);
 }

 #[test]
 fn test_batch_trainer_reset() {
 let config = BatchConfig::default_domains()
 .with_samples(5, 2);

 let mut trainer = BatchTrainer::new(config);
 trainer.train_one(&SpecialistDomain::Document);

 trainer.reset();

 assert_eq!(trainer.results().len(), 0);
 }
}

#[cfg(test)]
mod data_pipeline_tests {
 use super::*;

 #[test]
 fn test_generator_to_loader() {
 let mut gen = DocumentGenerator::new();
 let examples = gen.generate_batch(100);

 let config = DataLoaderConfig {
 batch_size: 10,
 shuffle: true,
 drop_last: false,
 };
 let loader = DataLoader::new(examples, config);

 let batches: Vec<_> = loader.iter().collect();
 assert_eq!(batches.len(), 10);
 }

 #[test]
 fn test_train_val_split() {
 let mut gen = CodeGenerator::new();
 let examples = gen.generate_batch(100);

 let (train, val) = DataLoader::split(examples, 0.8);

 assert_eq!(train.len(), 80);
 assert_eq!(val.len(), 20);
 }

 #[test]
 fn test_multiple_epochs() {
 let mut gen = ReasoningGenerator::new();
 let examples = gen.generate_batch(50);

 let config = DataLoaderConfig {
 batch_size: 10,
 shuffle: true,
 drop_last: false,
 };
 let mut loader = DataLoader::new(examples, config);

 // Epoch 1
 let epoch1_batches: Vec<_> = loader.iter().collect();
 assert_eq!(epoch1_batches.len(), 5);

 // Reset for epoch 2
 loader.reset();

 // Epoch 2
 let epoch2_batches: Vec<_> = loader.iter().collect();
 assert_eq!(epoch2_batches.len(), 5);
 }
}

#[cfg(test)]
mod lora_pipeline_tests {
 use super::*;

 #[test]
 fn test_lora_with_pipeline_config() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Code);

 // Code domain should have complex LoRA
 let lora_model = LoRAModel::new(config.lora.clone());

 assert!(config.lora.rank >= 16);
 }

 #[test]
 fn test_lora_layer_in_training() {
 let config = LoRAConfig::simple();
 let layer = LoRALayer::new(64, 64, &config);

 // Forward pass
 let input = DualTensor::randn(&[4, 64], 42);
 let output = layer.forward(&input);

 assert_eq!(output.shape(), &[4, 64]);
 }

 #[test]
 fn test_lora_merge_after_training() {
 let config = LoRAConfig::simple();
 let mut layer = LoRALayer::new(64, 64, &config);

 // Simulate training (just do forward passes)
 for _ in 0..10 {
 let input = DualTensor::randn(&[2, 64], 42);
 let _ = layer.forward(&input);
 }

 // Merge weights
 layer.merge();
 assert!(layer.is_merged());
 }
}

#[cfg(test)]
mod annealing_pipeline_tests {
 use super::*;

 #[test]
 fn test_annealing_with_training() {
 let mut state = AnnealTrainingState::new();

 // Simulate training loop
 for epoch in 0..3 {
 state.new_epoch();

 for batch in 0..10 {
 let loss = 2.0 / (1.0 + (epoch * 10 + batch) as f64 / 10.0);
 state.record(loss);
 }
 }

 assert_eq!(state.epoch, 3);
 assert_eq!(state.step, 30);
 assert!(state.temperature() < 1.0); // Should have cooled
 }

 #[test]
 fn test_lr_schedule_in_pipeline() {
 let mut opt = AnnealOptimizer::new();

 let mut lrs = vec![];
 for step in 0..100 {
 let loss = 2.0 / (1.0 + step as f64 / 50.0);
 opt.step(loss);
 lrs.push(opt.current_lr());
 }

 // LR should vary (not constant)
 let unique_lrs: std::collections::HashSet<_> = lrs.iter()
 .map(|lr| (lr * 1000.0) as i64)
 .collect();
 assert!(unique_lrs.len() > 1);
 }
}

#[cfg(test)]
mod export_tests {
 use super::*;

 #[test]
 fn test_gguf_config() {
 let config = GgufConfig::default();

 assert!(config.quantization_bits <= 16);
 }

 #[test]
 fn test_gguf_exporter_creation() {
 let config = GgufConfig::default();
 let exporter = GgufExporter::new(config);

 // Just verify creation works
 }

 #[test]
 fn test_pipeline_export_gguf() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .with_samples(5, 2);

 let mut pipeline = SpecialistPipeline::new(config);
 pipeline.train();

 // Export should succeed (even if simulated)
 let result = pipeline.export_gguf("/tmp/test.gguf");
 assert!(result.is_ok());
 }
}

#[cfg(test)]
mod end_to_end_tests {
 use super::*;

 #[test]
 fn test_full_specialist_workflow() {
 // 1. Configure pipeline
 let config = PipelineConfig::for_domain(SpecialistDomain::Document)
 .with_samples(20, 5)
 .without_compression()
 .with_output("test_output");

 // 2. Create pipeline
 let mut pipeline = SpecialistPipeline::new(config);

 // 3. Train
 let result = pipeline.train();

 // 4. Verify result
 assert!(result.specialist.steps > 0);
 assert!(result.specialist.final_loss < f64::MAX);
 assert!(result.specialist.best_val_loss < f64::MAX);

 // 5. Check domain
 assert!(matches!(result.specialist.domain, SpecialistDomain::Document));
 }

 #[test]
 fn test_multi_specialist_workflow() {
 let domains = vec![
 SpecialistDomain::Document,
 SpecialistDomain::Classification,
 ];

 let config = BatchConfig {
 domains,
 output_dir: "test_specialists".to_string(),
 samples_per_domain: 10,
 val_samples: 5,
 sequential: true,
 };

 let mut trainer = BatchTrainer::new(config);

 // Train first specialist
 let result1 = trainer.train_one(&SpecialistDomain::Document);
 assert!(result1.is_some());

 // Train second specialist
 let result2 = trainer.train_one(&SpecialistDomain::Classification);
 assert!(result2.is_some());

 // Verify both trained
 assert_eq!(trainer.results().len(), 2);
 }

 #[test]
 fn test_training_with_early_stopping() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Reasoning)
 .with_samples(100, 20);

 let mut pipeline = SpecialistPipeline::new(config);

 // Training should not run forever
 let result = pipeline.train();

 // Should have stopped at some point
 assert!(result.specialist.steps > 0);
 assert!(result.specialist.steps < 100000);
 }

 #[test]
 fn test_training_reproducibility() {
 // Two identical pipelines with same seed should produce similar results
 let config1 = PipelineConfig::for_domain(SpecialistDomain::Classification)
 .with_samples(20, 5);
 let config2 = config1.clone();

 let mut pipeline1 = SpecialistPipeline::new(config1);
 let mut pipeline2 = SpecialistPipeline::new(config2);

 let result1 = pipeline1.train();
 let result2 = pipeline2.train();

 // Should have same number of steps
 assert_eq!(result1.specialist.steps, result2.specialist.steps);
 }
}

#[cfg(test)]
mod metrics_tests {
 use super::*;

 #[test]
 fn test_training_metrics_format() {
 let config = PipelineConfig::for_domain(SpecialistDomain::Code)
 .with_samples(10, 5);

 let mut pipeline = SpecialistPipeline::new(config);
 pipeline.train();

 let metrics = pipeline.metrics();
 let formatted = metrics.to_string();

 // Should contain key information
 assert!(formatted.contains("Step"));
 assert!(formatted.contains("Loss"));
 assert!(formatted.contains("LR"));
 }

 #[test]
 fn test_batch_stats() {
 let config = BatchConfig::default_domains()
 .with_samples(5, 2);

 let mut trainer = BatchTrainer::new(config);

 // Train one domain
 trainer.train_one(&SpecialistDomain::Document);

 let results = trainer.results();
 assert_eq!(results.len(), 1);

 // Get the trained specialist
 let (domain, specialist) = &results[0];
 assert!(matches!(domain, SpecialistDomain::Document));
 assert!(specialist.steps > 0);
 }
}
