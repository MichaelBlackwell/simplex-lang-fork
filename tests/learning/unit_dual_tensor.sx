// Unit tests for DualTensor
//
// Tests forward-mode automatic differentiation on tensor operations.
// DualTensor enables meta-gradient computation (∂output/∂hyperparameter).

use simplex_learning::tensor::{DualTensor, Shape, TensorError};
use simplex_learning::dual::Dual;

// ============================================================================
// Construction Tests
// ============================================================================

fn test_dual_tensor_zeros() {
 let t = DualTensor::zeros(Shape::new(vec![2, 3]));

 assert_eq!(t.numel(), 6);
 assert_eq!(t.shape().dims(), &[2, 3]);
 assert_eq!(t.ndims(), 2);

 for i in 0..6 {
 assert_eq!(t.get(i).val, 0.0);
 assert_eq!(t.get(i).der, 0.0);
 }

 println("PASS: test_dual_tensor_zeros");
}

fn test_dual_tensor_ones() {
 let t = DualTensor::ones(Shape::new(vec![3]));

 assert_eq!(t.numel(), 3);
 for i in 0..3 {
 assert_eq!(t.get(i).val, 1.0);
 assert_eq!(t.get(i).der, 0.0); // constants have der=0
 }

 println("PASS: test_dual_tensor_ones");
}

fn test_dual_tensor_from_values() {
 let t = DualTensor::from_values(
 vec![1.0, 2.0, 3.0, 4.0],
 Shape::new(vec![2, 2])
 ).unwrap();

 assert_eq!(t.get(0).val, 1.0);
 assert_eq!(t.get(1).val, 2.0);
 assert_eq!(t.get(2).val, 3.0);
 assert_eq!(t.get(3).val, 4.0);

 // All derivatives should be 0 (constants)
 for i in 0..4 {
 assert_eq!(t.get(i).der, 0.0);
 }

 println("PASS: test_dual_tensor_from_values");
}

fn test_dual_tensor_variable() {
 // Create tensor where all elements are variables (der=1)
 let t = DualTensor::variable(
 vec![1.0, 2.0, 3.0],
 Shape::new(vec![3])
 ).unwrap();

 for i in 0..3 {
 assert_eq!(t.get(i).der, 1.0); // All seeded with der=1
 }

 println("PASS: test_dual_tensor_variable");
}

fn test_dual_tensor_variable_at() {
 // Create tensor where only element at index 1 is a variable
 let t = DualTensor::variable_at(
 vec![1.0, 2.0, 3.0],
 Shape::new(vec![3]),
 1 // seed index
 ).unwrap();

 assert_eq!(t.get(0).der, 0.0); // constant
 assert_eq!(t.get(1).der, 1.0); // variable (seeded)
 assert_eq!(t.get(2).der, 0.0); // constant

 println("PASS: test_dual_tensor_variable_at");
}

fn test_dual_tensor_scalar() {
 let t = DualTensor::scalar(Dual::variable(5.0));

 assert!(t.is_scalar());
 assert_eq!(t.numel(), 1);

 let d = t.item().unwrap();
 assert_eq!(d.val, 5.0);
 assert_eq!(d.der, 1.0);

 println("PASS: test_dual_tensor_scalar");
}

// ============================================================================
// Element-wise Operation Tests
// ============================================================================

fn test_dual_tensor_add() {
 // x + c where x is variable
 let x = DualTensor::variable(vec![1.0, 2.0], Shape::new(vec![2])).unwrap();
 let c = DualTensor::from_values(vec![3.0, 4.0], Shape::new(vec![2])).unwrap();

 let y = x.add(&c).unwrap();

 assert_eq!(y.get(0).val, 4.0); // 1 + 3
 assert_eq!(y.get(1).val, 6.0); // 2 + 4
 assert_eq!(y.get(0).der, 1.0); // d(x+c)/dx = 1
 assert_eq!(y.get(1).der, 1.0);

 println("PASS: test_dual_tensor_add");
}

fn test_dual_tensor_sub() {
 let x = DualTensor::variable(vec![5.0, 7.0], Shape::new(vec![2])).unwrap();
 let c = DualTensor::from_values(vec![2.0, 3.0], Shape::new(vec![2])).unwrap();

 let y = x.sub(&c).unwrap();

 assert_eq!(y.get(0).val, 3.0); // 5 - 2
 assert_eq!(y.get(1).val, 4.0); // 7 - 3
 assert_eq!(y.get(0).der, 1.0); // d(x-c)/dx = 1

 println("PASS: test_dual_tensor_sub");
}

fn test_dual_tensor_mul_elementwise() {
 // x * x = x^2
 let x = DualTensor::variable(vec![2.0, 3.0], Shape::new(vec![2])).unwrap();
 let y = x.mul(&x).unwrap();

 assert_eq!(y.get(0).val, 4.0); // 2^2 = 4
 assert_eq!(y.get(1).val, 9.0); // 3^2 = 9
 assert_eq!(y.get(0).der, 4.0); // d(x^2)/dx = 2x = 4
 assert_eq!(y.get(1).der, 6.0); // d(x^2)/dx = 2x = 6

 println("PASS: test_dual_tensor_mul_elementwise");
}

fn test_dual_tensor_mul_scalar() {
 let x = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();
 let y = x.mul_scalar(Dual::constant(2.0));

 assert_eq!(y.get(0).val, 2.0); // 1 * 2
 assert_eq!(y.get(1).val, 4.0); // 2 * 2
 assert_eq!(y.get(2).val, 6.0); // 3 * 2
 assert_eq!(y.get(0).der, 2.0); // d(2x)/dx = 2

 println("PASS: test_dual_tensor_mul_scalar");
}

fn test_dual_tensor_div() {
 let x = DualTensor::variable(vec![4.0, 9.0], Shape::new(vec![2])).unwrap();
 let c = DualTensor::from_values(vec![2.0, 3.0], Shape::new(vec![2])).unwrap();

 let y = x.div(&c).unwrap();

 assert_eq!(y.get(0).val, 2.0); // 4 / 2
 assert_eq!(y.get(1).val, 3.0); // 9 / 3
 assert_eq!(y.get(0).der, 0.5); // d(x/2)/dx = 1/2
 assert!((y.get(1).der - 1.0/3.0).abs() < 1e-10);

 println("PASS: test_dual_tensor_div");
}

fn test_dual_tensor_neg() {
 let x = DualTensor::variable(vec![1.0, -2.0], Shape::new(vec![2])).unwrap();
 let y = x.neg();

 assert_eq!(y.get(0).val, -1.0);
 assert_eq!(y.get(1).val, 2.0);
 assert_eq!(y.get(0).der, -1.0); // d(-x)/dx = -1

 println("PASS: test_dual_tensor_neg");
}

// ============================================================================
// Reduction Operation Tests
// ============================================================================

fn test_dual_tensor_sum() {
 let x = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();
 let s = x.sum();

 assert_eq!(s.val, 6.0); // 1 + 2 + 3
 assert_eq!(s.der, 3.0); // d(sum(x))/dx = n (all elements contribute 1)

 println("PASS: test_dual_tensor_sum");
}

fn test_dual_tensor_mean() {
 let x = DualTensor::variable(vec![2.0, 4.0, 6.0], Shape::new(vec![3])).unwrap();
 let m = x.mean();

 assert_eq!(m.val, 4.0); // (2 + 4 + 6) / 3
 assert_eq!(m.der, 1.0); // d(mean(x))/dx = n/n = 1

 println("PASS: test_dual_tensor_mean");
}

fn test_dual_tensor_sum_axis() {
 // 2x3 matrix, sum along axis 1 (columns)
 let x = DualTensor::variable(
 vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
 Shape::new(vec![2, 3])
 ).unwrap();

 let s = x.sum_axis(1).unwrap();

 assert_eq!(s.shape().dims(), &[2]);
 assert_eq!(s.get(0).val, 6.0); // 1 + 2 + 3
 assert_eq!(s.get(1).val, 15.0); // 4 + 5 + 6

 println("PASS: test_dual_tensor_sum_axis");
}

// ============================================================================
// Activation Function Tests
// ============================================================================

fn test_dual_tensor_relu() {
 let x = DualTensor::variable(vec![-1.0, 0.0, 1.0, 2.0], Shape::new(vec![4])).unwrap();
 let y = x.relu();

 assert_eq!(y.get(0).val, 0.0); // relu(-1) = 0
 assert_eq!(y.get(1).val, 0.0); // relu(0) = 0
 assert_eq!(y.get(2).val, 1.0); // relu(1) = 1
 assert_eq!(y.get(3).val, 2.0); // relu(2) = 2

 assert_eq!(y.get(0).der, 0.0); // d(relu)/dx = 0 for x < 0
 assert_eq!(y.get(2).der, 1.0); // d(relu)/dx = 1 for x > 0

 println("PASS: test_dual_tensor_relu");
}

fn test_dual_tensor_sigmoid() {
 let x = DualTensor::variable(vec![0.0], Shape::new(vec![1])).unwrap();
 let y = x.sigmoid();

 // sigmoid(0) = 0.5
 assert!((y.get(0).val - 0.5).abs() < 1e-10);
 // sigmoid'(0) = sigmoid(0) * (1 - sigmoid(0)) = 0.25
 assert!((y.get(0).der - 0.25).abs() < 1e-10);

 println("PASS: test_dual_tensor_sigmoid");
}

fn test_dual_tensor_tanh() {
 let x = DualTensor::variable(vec![0.0], Shape::new(vec![1])).unwrap();
 let y = x.tanh();

 // tanh(0) = 0
 assert!(y.get(0).val.abs() < 1e-10);
 // tanh'(0) = 1 - tanh(0)^2 = 1
 assert!((y.get(0).der - 1.0).abs() < 1e-10);

 println("PASS: test_dual_tensor_tanh");
}

fn test_dual_tensor_exp() {
 let x = DualTensor::variable(vec![0.0, 1.0], Shape::new(vec![2])).unwrap();
 let y = x.exp();

 assert!((y.get(0).val - 1.0).abs() < 1e-10); // exp(0) = 1
 assert!((y.get(1).val - 2.71828).abs() < 0.001); // exp(1) ≈ e
 assert!((y.get(0).der - 1.0).abs() < 1e-10); // d(exp(x))/dx = exp(x)
 assert!((y.get(1).der - y.get(1).val).abs() < 1e-10);

 println("PASS: test_dual_tensor_exp");
}

fn test_dual_tensor_softmax() {
 let x = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![1, 3])).unwrap();
 let y = x.softmax();

 // Softmax values should sum to 1
 let sum = y.get(0).val + y.get(1).val + y.get(2).val;
 assert!((sum - 1.0).abs() < 1e-10);

 // Largest input should have largest probability
 assert!(y.get(2).val > y.get(1).val);
 assert!(y.get(1).val > y.get(0).val);

 println("PASS: test_dual_tensor_softmax");
}

// ============================================================================
// Matrix Operation Tests
// ============================================================================

fn test_dual_tensor_matmul_2d() {
 // [1, 2] @ [3] = 1*3 + 2*4 = 11
 // [4]
 let a = DualTensor::variable(vec![1.0, 2.0], Shape::new(vec![1, 2])).unwrap();
 let b = DualTensor::from_values(vec![3.0, 4.0], Shape::new(vec![2, 1])).unwrap();

 let c = a.matmul(&b).unwrap();

 assert_eq!(c.numel(), 1);
 assert_eq!(c.shape().dims(), &[1, 1]);
 assert_eq!(c.get(0).val, 11.0); // 1*3 + 2*4
 assert_eq!(c.get(0).der, 7.0); // d/dx(3x₁ + 4x₂) = 3 + 4 = 7

 println("PASS: test_dual_tensor_matmul_2d");
}

fn test_dual_tensor_matmul_2x2() {
 // [1, 2] @ [5, 6] = [1*5+2*7, 1*6+2*8] = [19, 22]
 // [3, 4] [7, 8] [3*5+4*7, 3*6+4*8] [43, 50]
 let a = DualTensor::variable(
 vec![1.0, 2.0, 3.0, 4.0],
 Shape::new(vec![2, 2])
 ).unwrap();
 let b = DualTensor::from_values(
 vec![5.0, 6.0, 7.0, 8.0],
 Shape::new(vec![2, 2])
 ).unwrap();

 let c = a.matmul(&b).unwrap();

 assert_eq!(c.shape().dims(), &[2, 2]);
 assert_eq!(c.get(0).val, 19.0); // 1*5 + 2*7
 assert_eq!(c.get(1).val, 22.0); // 1*6 + 2*8
 assert_eq!(c.get(2).val, 43.0); // 3*5 + 4*7
 assert_eq!(c.get(3).val, 50.0); // 3*6 + 4*8

 println("PASS: test_dual_tensor_matmul_2x2");
}

fn test_dual_tensor_transpose() {
 let a = DualTensor::variable(
 vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
 Shape::new(vec![2, 3])
 ).unwrap();

 let at = a.transpose().unwrap();

 assert_eq!(at.shape().dims(), &[3, 2]);
 assert_eq!(at.get(0).val, 1.0); // (0,0)
 assert_eq!(at.get(1).val, 4.0); // (0,1) from original (1,0)
 assert_eq!(at.get(2).val, 2.0); // (1,0) from original (0,1)

 println("PASS: test_dual_tensor_transpose");
}

// ============================================================================
// Shape Operation Tests
// ============================================================================

fn test_dual_tensor_reshape() {
 let x = DualTensor::variable(
 vec![1.0, 2.0, 3.0, 4.0, 5.0, 6.0],
 Shape::new(vec![2, 3])
 ).unwrap();

 let y = x.reshape(Shape::new(vec![3, 2])).unwrap();

 assert_eq!(y.shape().dims(), &[3, 2]);
 assert_eq!(y.numel(), 6);

 // Data should be unchanged
 for i in 0..6 {
 assert_eq!(x.get(i).val, y.get(i).val);
 assert_eq!(x.get(i).der, y.get(i).der);
 }

 println("PASS: test_dual_tensor_reshape");
}

fn test_dual_tensor_flatten() {
 let x = DualTensor::variable(
 vec![1.0, 2.0, 3.0, 4.0],
 Shape::new(vec![2, 2])
 ).unwrap();

 let y = x.flatten();

 assert_eq!(y.shape().dims(), &[4]);

 println("PASS: test_dual_tensor_flatten");
}

// ============================================================================
// Loss Function Tests
// ============================================================================

fn test_dual_tensor_mse_loss() {
 let pred = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();
 let target = DualTensor::from_values(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();

 let loss = pred.mse_loss(&target).unwrap();

 // Perfect prediction: MSE = 0
 assert!(loss.val.abs() < 1e-10);

 println("PASS: test_dual_tensor_mse_loss");
}

fn test_dual_tensor_mse_loss_gradient() {
 let pred = DualTensor::variable(vec![2.0], Shape::new(vec![1])).unwrap();
 let target = DualTensor::from_values(vec![0.0], Shape::new(vec![1])).unwrap();

 let loss = pred.mse_loss(&target).unwrap();

 // MSE = (2 - 0)^2 = 4
 assert_eq!(loss.val, 4.0);
 // d(MSE)/dx = 2*(x - target) = 2*2 = 4
 assert_eq!(loss.der, 4.0);

 println("PASS: test_dual_tensor_mse_loss_gradient");
}

// ============================================================================
// Layer Normalization Tests
// ============================================================================

fn test_dual_tensor_layer_norm() {
 let x = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![1, 3])).unwrap();
 let y = x.layer_norm(1e-5);

 // After layer norm, mean should be ~0 and variance ~1
 let mean = y.mean();
 assert!(mean.val.abs() < 1e-5);

 println("PASS: test_dual_tensor_layer_norm");
}

// ============================================================================
// Conversion Tests
// ============================================================================

fn test_dual_tensor_to_tensor() {
 let dt = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();
 let t = dt.to_tensor();

 assert_eq!(t.numel(), 3);
 assert_eq!(t.get(0), 1.0);
 assert_eq!(t.get(1), 2.0);
 assert_eq!(t.get(2), 3.0);

 println("PASS: test_dual_tensor_to_tensor");
}

fn test_dual_tensor_derivative_tensor() {
 let dt = DualTensor::variable(vec![1.0, 2.0, 3.0], Shape::new(vec![3])).unwrap();
 let y = dt.mul_scalar(Dual::constant(2.0)); // 2x
 let der_t = y.derivative_tensor();

 // All derivatives should be 2 (d(2x)/dx = 2)
 assert_eq!(der_t.get(0), 2.0);
 assert_eq!(der_t.get(1), 2.0);
 assert_eq!(der_t.get(2), 2.0);

 println("PASS: test_dual_tensor_derivative_tensor");
}

// ============================================================================
// Meta-Gradient Test (key use case)
// ============================================================================

fn test_meta_gradient_temperature() {
 // Simulate computing ∂Loss/∂τ (derivative of loss w.r.t. temperature)
 // This is the key use case for self-learning annealing

 // Temperature τ is our variable
 let tau = Dual::variable(1.0);

 // Create a tensor of logits
 let logits = DualTensor::from_values(
 vec![1.0, 2.0, 3.0],
 Shape::new(vec![1, 3])
 ).unwrap();

 // Scale logits by 1/τ (temperature scaling)
 let scaled = logits.div_scalar(tau);

 // Apply softmax
 let probs = scaled.softmax();

 // Compute loss (negative log prob of correct class, say class 2)
 let loss = probs.get(2).neg().ln();

 // loss.der contains ∂Loss/∂τ
 println(format!("Loss value: {}", loss.val));
 println(format!("∂Loss/∂τ: {}", loss.der));

 // The gradient should be non-zero (temperature affects the loss)
 // Note: At τ=1, this is just regular softmax

 println("PASS: test_meta_gradient_temperature");
}

// ============================================================================
// Main Test Runner
// ============================================================================

fn main() {
 println("Running DualTensor unit tests...\n");

 // Construction tests
 test_dual_tensor_zeros();
 test_dual_tensor_ones();
 test_dual_tensor_from_values();
 test_dual_tensor_variable();
 test_dual_tensor_variable_at();
 test_dual_tensor_scalar();

 // Element-wise operation tests
 test_dual_tensor_add();
 test_dual_tensor_sub();
 test_dual_tensor_mul_elementwise();
 test_dual_tensor_mul_scalar();
 test_dual_tensor_div();
 test_dual_tensor_neg();

 // Reduction tests
 test_dual_tensor_sum();
 test_dual_tensor_mean();
 test_dual_tensor_sum_axis();

 // Activation tests
 test_dual_tensor_relu();
 test_dual_tensor_sigmoid();
 test_dual_tensor_tanh();
 test_dual_tensor_exp();
 test_dual_tensor_softmax();

 // Matrix operation tests
 test_dual_tensor_matmul_2d();
 test_dual_tensor_matmul_2x2();
 test_dual_tensor_transpose();

 // Shape operation tests
 test_dual_tensor_reshape();
 test_dual_tensor_flatten();

 // Loss function tests
 test_dual_tensor_mse_loss();
 test_dual_tensor_mse_loss_gradient();

 // Layer norm tests
 test_dual_tensor_layer_norm();

 // Conversion tests
 test_dual_tensor_to_tensor();
 test_dual_tensor_derivative_tensor();

 // Meta-gradient test
 test_meta_gradient_temperature();

 println("\nAll DualTensor tests passed!");
}
