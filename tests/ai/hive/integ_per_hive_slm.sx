// Test file for Per-Hive SLM Architecture
// v0.5.0: ONE shared model per hive, not per specialist

// External declarations
fn hive_slm_new(hive_name: i64, model: i64, mnemonic: i64) -> i64;
fn hive_slm_get_model(hive_slm: i64) -> i64;
fn hive_slm_get_mnemonic(hive_slm: i64) -> i64;
fn hive_slm_get_specialist_count(hive_slm: i64) -> i64;
fn hive_slm_add_specialist(hive_slm: i64, specialist_id: i64) -> i64;
fn hive_slm_infer(hive_slm: i64, specialist_id: i64, prompt: i64) -> i64;
fn hive_slm_get_memory_usage(hive_slm: i64) -> i64;
fn hive_slm_close(hive_slm: i64);

fn specialist_create(name: i64, hive_slm: i64) -> i64;
fn specialist_get_id(spec: i64) -> i64;
fn specialist_get_hive_slm(spec: i64) -> i64;
fn specialist_infer(spec: i64, prompt: i64) -> i64;
fn specialist_get_anima(spec: i64) -> i64;
fn specialist_close(spec: i64);

fn anima_memory_new(capacity: i64) -> i64;
fn anima_remember(mem: i64, content: i64, importance: f64) -> i64;
fn anima_format_context(mem: i64) -> i64;
fn anima_memory_close(mem: i64);

fn hive_mnemonic_new(episodic_cap: i64, semantic_cap: i64, belief_threshold: i64) -> i64;
fn hive_mnemonic_learn(mnemonic: i64, content: i64, confidence: f64) -> i64;
fn hive_mnemonic_close(mnemonic: i64);

fn slm_instance_get_model_name(instance: i64) -> i64;
fn slm_instance_get_memory_bytes(instance: i64) -> i64;

fn string_from(s: i64) -> i64;
fn string_eq(a: i64, b: i64) -> i64;
fn print(s: i64);
fn println(s: i64);
fn print_i64(n: i64);
fn print_string(s: i64);

fn main() {
    println("=== Per-Hive SLM Architecture Test ===");
    println("v0.5.0: ONE model per hive, shared by all specialists");
    println("");

    // Test 1: Create Hive SLM
    println("--- Test 1: Create Hive SLM ---");
    let mnemonic: i64 = hive_mnemonic_new(100, 500, 50);
    let hive_slm: i64 = hive_slm_new(
        string_from("AnalyticsHive"),
        string_from("simplex-cognitive-7b"),
        mnemonic
    );

    if hive_slm != 0 {
        println("PASS: Created Hive SLM");
        let model: i64 = hive_slm_get_model(hive_slm);
        print("  Model: ");
        print_string(model);
        println("");
    } else {
        println("FAIL: Could not create Hive SLM");
    }
    println("");

    // Test 2: Add multiple specialists to same hive
    println("--- Test 2: Add Multiple Specialists ---");
    let spec1: i64 = specialist_create(string_from("Analyzer"), hive_slm);
    let spec2: i64 = specialist_create(string_from("Summarizer"), hive_slm);
    let spec3: i64 = specialist_create(string_from("Critic"), hive_slm);
    let spec4: i64 = specialist_create(string_from("Writer"), hive_slm);
    let spec5: i64 = specialist_create(string_from("Researcher"), hive_slm);

    let specialist_count: i64 = hive_slm_get_specialist_count(hive_slm);
    print("Specialists in hive: ");
    print_i64(specialist_count);
    println("");

    if specialist_count == 5 {
        println("PASS: All 5 specialists registered with hive");
    } else {
        println("FAIL: Specialist count mismatch");
    }
    println("");

    // Test 3: Verify all specialists share same SLM
    println("--- Test 3: Verify Shared SLM ---");
    let slm1: i64 = specialist_get_hive_slm(spec1);
    let slm2: i64 = specialist_get_hive_slm(spec2);
    let slm3: i64 = specialist_get_hive_slm(spec3);
    let slm4: i64 = specialist_get_hive_slm(spec4);
    let slm5: i64 = specialist_get_hive_slm(spec5);

    print("Specialist 1 hive_slm: ");
    print_i64(slm1);
    println("");
    print("Specialist 2 hive_slm: ");
    print_i64(slm2);
    println("");
    print("Specialist 3 hive_slm: ");
    print_i64(slm3);
    println("");
    print("Specialist 4 hive_slm: ");
    print_i64(slm4);
    println("");
    print("Specialist 5 hive_slm: ");
    print_i64(slm5);
    println("");

    // All should reference the same hive_slm
    if slm1 == slm2 && slm2 == slm3 && slm3 == slm4 && slm4 == slm5 {
        println("PASS: All specialists share the SAME Hive SLM");
    } else {
        println("FAIL: Specialists have different SLM references");
    }
    println("");

    // Test 4: Memory efficiency check
    println("--- Test 4: Memory Efficiency ---");
    let memory_usage: i64 = hive_slm_get_memory_usage(hive_slm);
    print("Total memory usage for hive: ");
    print_i64(memory_usage);
    println(" bytes");

    // 5 specialists sharing 1 model should use ~same memory as 1 model
    // For 7B model: ~8-12 GB
    // For per-specialist: would be 5x that (40-60 GB)
    let expected_max: i64 = 15000000000;  // 15 GB max for shared model
    if memory_usage < expected_max {
        println("PASS: Memory usage is efficient (one shared model)");
    } else {
        println("WARN: Memory usage higher than expected for shared model");
    }

    // Calculate theoretical per-specialist cost
    let per_specialist_cost: i64 = memory_usage * 5;
    print("Theoretical per-specialist cost: ");
    print_i64(per_specialist_cost);
    println(" bytes");
    print("Savings from sharing: ");
    print_i64(per_specialist_cost - memory_usage);
    println(" bytes");
    println("");

    // Test 5: Each specialist has its own Anima
    println("--- Test 5: Separate Anima Per Specialist ---");
    let anima1: i64 = specialist_get_anima(spec1);
    let anima2: i64 = specialist_get_anima(spec2);
    let anima3: i64 = specialist_get_anima(spec3);

    if anima1 != anima2 && anima2 != anima3 {
        println("PASS: Each specialist has its own Anima");
    } else {
        println("FAIL: Specialists should have separate Animas");
    }

    // Add different memories to each anima
    anima_remember(anima1, string_from("Analyzer experience 1"), 0.8);
    anima_remember(anima2, string_from("Summarizer experience 1"), 0.8);
    anima_remember(anima3, string_from("Critic experience 1"), 0.8);

    println("Added unique experiences to each specialist's Anima");
    println("");

    // Test 6: Inference routes through shared SLM
    println("--- Test 6: Inference Through Shared SLM ---");

    // Add some context to mnemonic
    hive_mnemonic_learn(mnemonic, string_from("Shared fact: team uses TDD"), 0.9);

    // Each specialist's infer() should go through the same hive SLM
    // with their individual anima + shared mnemonic context
    let prompt1: i64 = string_from("Analyze this code.");
    let prompt2: i64 = string_from("Summarize this document.");
    let prompt3: i64 = string_from("Critique this design.");

    println("Specialist 1 (Analyzer) inference...");
    let result1: i64 = specialist_infer(spec1, prompt1);
    if result1 != 0 {
        println("  PASS: Inference completed");
    }

    println("Specialist 2 (Summarizer) inference...");
    let result2: i64 = specialist_infer(spec2, prompt2);
    if result2 != 0 {
        println("  PASS: Inference completed");
    }

    println("Specialist 3 (Critic) inference...");
    let result3: i64 = specialist_infer(spec3, prompt3);
    if result3 != 0 {
        println("  PASS: Inference completed");
    }
    println("");

    // Test 7: Concurrent inference (simulated)
    println("--- Test 7: Concurrent Access ---");
    // In real implementation, multiple specialists can infer concurrently
    // The hive SLM should handle queuing/batching
    println("Simulating concurrent inference from multiple specialists...");

    // All 5 specialists issue inference at "same time"
    let r1: i64 = specialist_infer(spec1, string_from("Query 1"));
    let r2: i64 = specialist_infer(spec2, string_from("Query 2"));
    let r3: i64 = specialist_infer(spec3, string_from("Query 3"));
    let r4: i64 = specialist_infer(spec4, string_from("Query 4"));
    let r5: i64 = specialist_infer(spec5, string_from("Query 5"));

    if r1 != 0 && r2 != 0 && r3 != 0 && r4 != 0 && r5 != 0 {
        println("PASS: All concurrent inferences completed");
    } else {
        println("WARN: Some inferences may have failed");
    }
    println("");

    // Test 8: Create second hive to verify isolation
    println("--- Test 8: Multiple Hives Are Isolated ---");
    let mnemonic2: i64 = hive_mnemonic_new(50, 200, 50);
    let hive_slm2: i64 = hive_slm_new(
        string_from("ResearchHive"),
        string_from("simplex-cognitive-1b"),  // Different model
        mnemonic2
    );

    let spec_research: i64 = specialist_create(string_from("Investigator"), hive_slm2);

    // Verify different hives have different SLMs
    let research_slm: i64 = specialist_get_hive_slm(spec_research);
    if research_slm != slm1 {
        println("PASS: Different hives have different SLMs");
    } else {
        println("FAIL: Hives should have separate SLMs");
    }

    // Verify different models
    let model1: i64 = hive_slm_get_model(hive_slm);
    let model2: i64 = hive_slm_get_model(hive_slm2);
    print("Hive 1 model: ");
    print_string(model1);
    println("");
    print("Hive 2 model: ");
    print_string(model2);
    println("");

    if string_eq(model1, model2) == 0 {
        println("PASS: Hives using different models");
    } else {
        println("INFO: Both hives using same model (which is valid)");
    }
    println("");

    // Test 9: Specialist removal
    println("--- Test 9: Specialist Removal ---");
    let before_count: i64 = hive_slm_get_specialist_count(hive_slm);
    print("Specialists before removal: ");
    print_i64(before_count);
    println("");

    specialist_close(spec5);  // Remove one specialist

    let after_count: i64 = hive_slm_get_specialist_count(hive_slm);
    print("Specialists after removal: ");
    print_i64(after_count);
    println("");

    if after_count == before_count - 1 {
        println("PASS: Specialist properly removed from hive");
    } else {
        println("WARN: Specialist count unchanged after removal");
    }
    println("");

    // Test 10: Hive SLM survives specialist removal
    println("--- Test 10: Hive SLM Persistence ---");
    // Even with fewer specialists, hive SLM should still work
    let persist_result: i64 = specialist_infer(spec1, string_from("Post-removal query"));
    if persist_result != 0 {
        println("PASS: Hive SLM still functional after specialist removal");
    } else {
        println("FAIL: Hive SLM should persist");
    }
    println("");

    // Test 11: Model switching (for testing/development)
    println("--- Test 11: Architecture Verification ---");
    println("Per-Hive SLM Architecture Summary:");
    println("  - ONE SLM instance per hive");
    println("  - All specialists share this SLM");
    println("  - Each specialist has its own Anima (personal memory)");
    println("  - HiveMnemonic provides shared consciousness");
    println("  - Context: Anima + Mnemonic + Prompt -> Hive SLM");
    println("");

    let total_specialists: i64 = hive_slm_get_specialist_count(hive_slm) + hive_slm_get_specialist_count(hive_slm2);
    let total_models: i64 = 2;  // Two hives = two models

    print("Total specialists: ");
    print_i64(total_specialists);
    println("");
    print("Total SLM instances: ");
    print_i64(total_models);
    println("");
    print("Efficiency ratio: ");
    print_i64(total_specialists);
    print(" specialists / ");
    print_i64(total_models);
    println(" models");

    println("");
    println("PASS: Per-Hive SLM architecture verified");

    // Cleanup
    specialist_close(spec_research);
    hive_slm_close(hive_slm2);
    hive_mnemonic_close(mnemonic2);

    specialist_close(spec1);
    specialist_close(spec2);
    specialist_close(spec3);
    specialist_close(spec4);
    hive_slm_close(hive_slm);
    hive_mnemonic_close(mnemonic);

    println("");
    println("=== Per-Hive SLM Architecture Test Complete! ===");
}
