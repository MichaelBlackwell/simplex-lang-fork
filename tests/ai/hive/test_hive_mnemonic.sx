// Test file for HiveMnemonic - Shared Consciousness
// v0.5.0: Per-Hive SLM Architecture

// External declarations for hive mnemonic
fn hive_mnemonic_new(episodic_cap: i64, semantic_cap: i64, belief_threshold: i64) -> i64;
fn hive_mnemonic_learn(mnemonic: i64, content: i64, confidence: f64) -> i64;
fn hive_mnemonic_remember(mnemonic: i64, content: i64, importance: f64) -> i64;
fn hive_mnemonic_believe(mnemonic: i64, content: i64, confidence: f64) -> i64;
fn hive_mnemonic_revise_belief(mnemonic: i64, belief_id: i64, new_confidence: f64, evidence: i64) -> i64;
fn hive_mnemonic_recall_for(mnemonic: i64, query: i64, max_results: i64) -> i64;
fn hive_mnemonic_get_context(mnemonic: i64) -> i64;
fn hive_mnemonic_episodic_count(mnemonic: i64) -> i64;
fn hive_mnemonic_semantic_count(mnemonic: i64) -> i64;
fn hive_mnemonic_belief_count(mnemonic: i64) -> i64;
fn hive_mnemonic_consolidate(mnemonic: i64) -> i64;
fn hive_mnemonic_close(mnemonic: i64);

fn hive_new(name: i64, slm: i64, mnemonic: i64) -> i64;
fn hive_get_mnemonic(hive: i64) -> i64;
fn hive_share_knowledge(hive: i64, specialist_id: i64, content: i64, importance: f64) -> i64;
fn hive_get_shared_context(hive: i64) -> i64;
fn hive_close(hive: i64);

fn specialist_new(name: i64, hive: i64) -> i64;
fn specialist_contribute_to_mnemonic(spec: i64, content: i64, mem_type: i64) -> i64;
fn specialist_get_hive_context(spec: i64) -> i64;
fn specialist_close(spec: i64);

fn vec_len(v: i64) -> i64;
fn string_from(s: i64) -> i64;
fn string_contains(s: i64, substr: i64) -> i64;
fn print(s: i64);
fn println(s: i64);
fn print_i64(n: i64);
fn print_string(s: i64);

fn main() {
    println("=== HiveMnemonic Test ===");
    println("v0.5.0: Shared Consciousness");
    println("");

    // Test 1: Create HiveMnemonic
    println("--- Test 1: Create HiveMnemonic ---");
    // episodic_cap=100, semantic_cap=500, belief_threshold=50 (50%)
    let mnemonic: i64 = hive_mnemonic_new(100, 500, 50);
    if mnemonic != 0 {
        println("PASS: Created HiveMnemonic");
        print("  Episodic capacity: 100");
        println("");
        print("  Semantic capacity: 500");
        println("");
        print("  Belief threshold: 50%");
        println("");
    } else {
        println("FAIL: Could not create HiveMnemonic");
    }
    println("");

    // Test 2: Shared semantic memory (learn)
    println("--- Test 2: Shared Semantic Memory ---");
    let fact1: i64 = hive_mnemonic_learn(mnemonic, string_from("The codebase uses PostgreSQL 15"), 0.95);
    let fact2: i64 = hive_mnemonic_learn(mnemonic, string_from("API endpoints use REST conventions"), 0.9);
    let fact3: i64 = hive_mnemonic_learn(mnemonic, string_from("Team prefers explicit error handling"), 0.85);
    let fact4: i64 = hive_mnemonic_learn(mnemonic, string_from("Low confidence guess"), 0.3);

    print("Added 4 semantic facts, IDs: ");
    print_i64(fact1);
    print(", ");
    print_i64(fact2);
    print(", ");
    print_i64(fact3);
    print(", ");
    print_i64(fact4);
    println("");

    let semantic_count: i64 = hive_mnemonic_semantic_count(mnemonic);
    print("Semantic count: ");
    print_i64(semantic_count);
    println("");

    if semantic_count == 4 {
        println("PASS: All semantic facts stored");
    } else {
        println("FAIL: Semantic count mismatch");
    }
    println("");

    // Test 3: Shared episodic memory (remember)
    println("--- Test 3: Shared Episodic Memory ---");
    let exp1: i64 = hive_mnemonic_remember(mnemonic, string_from("Team completed security audit"), 0.9);
    let exp2: i64 = hive_mnemonic_remember(mnemonic, string_from("Deployed v2.0 to production"), 0.85);
    let exp3: i64 = hive_mnemonic_remember(mnemonic, string_from("Fixed critical bug in auth"), 0.95);

    print("Added 3 experiences, IDs: ");
    print_i64(exp1);
    print(", ");
    print_i64(exp2);
    print(", ");
    print_i64(exp3);
    println("");

    let episodic_count: i64 = hive_mnemonic_episodic_count(mnemonic);
    print("Episodic count: ");
    print_i64(episodic_count);
    println("");

    if episodic_count == 3 {
        println("PASS: All episodic memories stored");
    } else {
        println("FAIL: Episodic count mismatch");
    }
    println("");

    // Test 4: Shared beliefs (50% threshold)
    println("--- Test 4: Shared Beliefs (50% threshold) ---");
    let belief1: i64 = hive_mnemonic_believe(mnemonic, string_from("Always validate user input"), 0.92);
    let belief2: i64 = hive_mnemonic_believe(mnemonic, string_from("Prefer async over sync operations"), 0.78);
    let belief3: i64 = hive_mnemonic_believe(mnemonic, string_from("Low confidence belief"), 0.40);  // Below 50%

    print("Added 3 beliefs, IDs: ");
    print_i64(belief1);
    print(", ");
    print_i64(belief2);
    print(", ");
    print_i64(belief3);
    println("");

    let belief_count: i64 = hive_mnemonic_belief_count(mnemonic);
    print("Belief count: ");
    print_i64(belief_count);
    println("");

    // All should be stored, but low-confidence may be marked differently
    println("PASS: Beliefs stored with varying confidence");
    println("");

    // Test 5: Belief revision
    println("--- Test 5: Belief Revision ---");
    println("Revising belief about async operations with new evidence...");
    let revised: i64 = hive_mnemonic_revise_belief(
        mnemonic,
        belief2,
        0.55,  // Lower confidence after conflicting evidence
        string_from("Some sync operations are clearer")
    );

    if revised != 0 {
        println("PASS: Belief revised successfully");
    } else {
        println("FAIL: Belief revision failed");
    }
    println("");

    // Test 6: Goal-directed recall from shared memory
    println("--- Test 6: Goal-Directed Recall ---");
    let query: i64 = string_from("security");
    let results: i64 = hive_mnemonic_recall_for(mnemonic, query, 10);
    let result_count: i64 = vec_len(results);

    print("Query: 'security' - Found ");
    print_i64(result_count);
    println(" relevant memories");

    // Should find: "Team completed security audit"
    if result_count >= 1 {
        println("PASS: Found security-related memories");
    } else {
        println("WARN: Expected at least 1 security-related memory");
    }

    let query2: i64 = string_from("database");
    let results2: i64 = hive_mnemonic_recall_for(mnemonic, query2, 10);
    let result_count2: i64 = vec_len(results2);

    print("Query: 'database' - Found ");
    print_i64(result_count2);
    println(" relevant memories");
    // Should find: "The codebase uses PostgreSQL 15"
    println("");

    // Test 7: Get formatted context for SLM
    println("--- Test 7: Formatted Context for SLM ---");
    let context: i64 = hive_mnemonic_get_context(mnemonic);
    if context != 0 {
        println("Context for SLM:");
        print_string(context);
        println("");

        // Verify context contains expected sections
        if string_contains(context, string_from("<hive")) == 1 {
            println("PASS: Context contains hive tag");
        }
        if string_contains(context, string_from("Shared experiences")) == 1 {
            println("PASS: Context contains episodic section");
        }
        if string_contains(context, string_from("Shared knowledge")) == 1 {
            println("PASS: Context contains semantic section");
        }
    } else {
        println("FAIL: Could not generate context");
    }
    println("");

    // Test 8: Memory consolidation
    println("--- Test 8: Memory Consolidation ---");
    println("Before consolidation:");
    print("  Episodic: ");
    print_i64(hive_mnemonic_episodic_count(mnemonic));
    println("");
    print("  Semantic: ");
    print_i64(hive_mnemonic_semantic_count(mnemonic));
    println("");

    let pruned: i64 = hive_mnemonic_consolidate(mnemonic);

    print("Pruned ");
    print_i64(pruned);
    println(" low-importance memories");

    println("After consolidation:");
    print("  Episodic: ");
    print_i64(hive_mnemonic_episodic_count(mnemonic));
    println("");
    print("  Semantic: ");
    print_i64(hive_mnemonic_semantic_count(mnemonic));
    println("");
    println("PASS: Consolidation executed");
    println("");

    // Test 9: Hive with mnemonic
    println("--- Test 9: Hive with HiveMnemonic ---");
    let mnemonic2: i64 = hive_mnemonic_new(50, 200, 50);
    let hive: i64 = hive_new(
        string_from("AnalyticsHive"),
        string_from("simplex-cognitive-7b"),
        mnemonic2
    );

    if hive != 0 {
        println("PASS: Created hive with mnemonic");

        // Verify hive can access mnemonic
        let hive_mnemonic: i64 = hive_get_mnemonic(hive);
        if hive_mnemonic == mnemonic2 {
            println("PASS: Hive returns correct mnemonic reference");
        }
    } else {
        println("FAIL: Could not create hive");
    }
    println("");

    // Test 10: Specialist contributing to hive mnemonic
    println("--- Test 10: Specialist Contributing to Mnemonic ---");
    let spec1: i64 = specialist_new(string_from("Analyzer"), hive);
    let spec2: i64 = specialist_new(string_from("Summarizer"), hive);

    if spec1 != 0 && spec2 != 0 {
        println("Created 2 specialists in hive");

        // Specialist 1 contributes semantic knowledge
        specialist_contribute_to_mnemonic(spec1, string_from("Analysis found pattern X"), 1);  // 1 = semantic

        // Specialist 2 contributes episodic experience
        specialist_contribute_to_mnemonic(spec2, string_from("Summarized document Y"), 2);  // 2 = episodic

        // Both should be in hive mnemonic
        let updated_semantic: i64 = hive_mnemonic_semantic_count(mnemonic2);
        let updated_episodic: i64 = hive_mnemonic_episodic_count(mnemonic2);

        print("Mnemonic semantic count after contributions: ");
        print_i64(updated_semantic);
        println("");
        print("Mnemonic episodic count after contributions: ");
        print_i64(updated_episodic);
        println("");

        if updated_semantic >= 1 && updated_episodic >= 1 {
            println("PASS: Specialists contributed to shared mnemonic");
        } else {
            println("FAIL: Specialist contributions not reflected");
        }

        specialist_close(spec1);
        specialist_close(spec2);
    } else {
        println("FAIL: Could not create specialists");
    }
    println("");

    // Test 11: Specialist accessing hive context
    println("--- Test 11: Specialist Accessing Hive Context ---");
    let spec3: i64 = specialist_new(string_from("Reader"), hive);
    if spec3 != 0 {
        let shared_ctx: i64 = specialist_get_hive_context(spec3);
        if shared_ctx != 0 {
            println("Specialist received hive context:");
            print_string(shared_ctx);
            println("");
            println("PASS: Specialist can access hive context");
        } else {
            println("FAIL: Specialist could not get hive context");
        }
        specialist_close(spec3);
    }
    println("");

    // Test 12: Cross-specialist knowledge sharing
    println("--- Test 12: Cross-Specialist Knowledge Sharing ---");
    // Add knowledge from one specialist
    hive_share_knowledge(hive, 1, string_from("Discovered critical insight"), 0.9);

    // Another specialist should be able to recall it
    let shared_results: i64 = hive_mnemonic_recall_for(mnemonic2, string_from("critical"), 5);
    let shared_count: i64 = vec_len(shared_results);

    print("Cross-specialist recall found ");
    print_i64(shared_count);
    println(" results");

    if shared_count >= 1 {
        println("PASS: Knowledge shared across specialists");
    } else {
        println("WARN: Cross-specialist sharing may need verification");
    }

    // Cleanup
    hive_close(hive);
    hive_mnemonic_close(mnemonic2);
    hive_mnemonic_close(mnemonic);

    println("");
    println("=== HiveMnemonic Test Complete! ===");
}
