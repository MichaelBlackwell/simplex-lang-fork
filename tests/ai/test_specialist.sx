// Test file for Specialist Enhancements
// Phase 4.9: Multi-Provider Support, Streaming, Structured Output, Reliability

// External declarations
fn provider_registry_new() -> i64;
fn provider_config_new(ty: i64, name: i64) -> i64;
fn provider_config_set_key(cfg: i64, key: i64);
fn provider_config_set_model(cfg: i64, model: i64);
fn provider_config_set_url(cfg: i64, url: i64);
fn provider_config_set_temp(cfg: i64, temp: f64);
fn provider_config_set_max_tokens(cfg: i64, max_tokens: i64);
fn provider_config_set_timeout(cfg: i64, timeout: i64);
fn provider_config_set_priority(cfg: i64, priority: i64);
fn provider_config_set_cost(cfg: i64, input: f64, output: f64);
fn provider_registry_add(reg: i64, cfg: i64) -> i64;
fn provider_registry_get(reg: i64, id: i64) -> i64;
fn provider_registry_count(reg: i64) -> i64;
fn provider_registry_set_default(reg: i64, id: i64);
fn provider_get_by_tier(reg: i64, tier: i64) -> i64;
fn provider_registry_list(reg: i64) -> i64;
fn provider_registry_close(reg: i64);

fn estimate_tokens(text: i64) -> i64;
fn count_tokens_accurate(text: i64) -> i64;
fn calculate_cost(cfg: i64, input: i64, output: i64) -> f64;
fn provider_get_stats(id: i64) -> i64;
fn provider_record_request(id: i64, success: i64, input: i64, output: i64, cost: f64, latency: f64);
fn provider_total_cost(reg: i64) -> f64;

fn retry_config_new() -> i64;
fn retry_config_set_max(cfg: i64, max: i64);
fn retry_config_set_delay(cfg: i64, delay: i64);
fn retry_config_set_backoff(cfg: i64, mult: f64);
fn retry_calculate_delay(cfg: i64, attempt: i64) -> i64;
fn retry_should_retry(cfg: i64, error_type: i64, attempt: i64) -> i64;
fn retry_config_close(cfg: i64);

fn fallback_chain_new() -> i64;
fn fallback_chain_add(chain: i64, provider_id: i64) -> i64;
fn fallback_chain_next(chain: i64, current: i64) -> i64;
fn fallback_chain_get(chain: i64, index: i64) -> i64;
fn fallback_chain_size(chain: i64) -> i64;
fn fallback_chain_close(chain: i64);

fn stream_context_new(callback: i64, user_data: i64) -> i64;
fn stream_process_chunk(ctx: i64, chunk: i64);
fn stream_complete(ctx: i64);
fn stream_error(ctx: i64, error: i64);
fn stream_get_content(ctx: i64) -> i64;
fn stream_is_complete(ctx: i64) -> i64;
fn stream_has_error(ctx: i64) -> i64;
fn stream_get_error(ctx: i64) -> i64;
fn stream_token_count(ctx: i64) -> i64;
fn stream_context_close(ctx: i64);

fn output_schema_new(name: i64, schema: i64) -> i64;
fn output_schema_set_strict(schema: i64, strict: i64);
fn output_schema_get_json(schema: i64) -> i64;
fn validate_json_output(output: i64, schema: i64) -> i64;
fn output_schema_close(schema: i64);

fn llm_request_new(provider_id: i64) -> i64;
fn llm_request_set_system(req: i64, prompt: i64);
fn llm_request_set_prompt(req: i64, prompt: i64);
fn llm_request_set_model(req: i64, model: i64);
fn llm_request_set_max_tokens(req: i64, max: i64);
fn llm_request_set_temperature(req: i64, temp: f64);
fn llm_request_set_schema(req: i64, schema: i64);
fn llm_request_enable_stream(req: i64, ctx: i64);
fn llm_request_set_tools(req: i64, tools: i64);
fn llm_request_set_retry(req: i64, retry: i64);
fn llm_request_to_json(req: i64, reg: i64) -> i64;
fn llm_request_close(req: i64);

fn llm_response_new() -> i64;
fn llm_response_set_success(resp: i64, success: i64);
fn llm_response_set_content(resp: i64, content: i64);
fn llm_response_set_error(resp: i64, error: i64);
fn llm_response_set_tokens(resp: i64, input: i64, output: i64);
fn llm_response_set_cost(resp: i64, cost: f64);
fn llm_response_set_latency(resp: i64, latency: f64);
fn llm_response_is_success(resp: i64) -> i64;
fn llm_response_get_content(resp: i64) -> i64;
fn llm_response_get_error(resp: i64) -> i64;
fn llm_response_input_tokens(resp: i64) -> i64;
fn llm_response_output_tokens(resp: i64) -> i64;
fn llm_response_get_cost(resp: i64) -> f64;
fn llm_response_get_latency(resp: i64) -> f64;
fn llm_response_to_json(resp: i64) -> i64;
fn llm_response_close(resp: i64);

fn main() {
    println("=== Specialist Enhancements Test ===");
    println("");

    // Test Provider Registry
    println("--- Testing Provider Registry ---");
    let reg: i64 = provider_registry_new();
    if reg != 0 {
        println("Created provider registry");
    }

    // Create Anthropic provider
    let anthropic: i64 = provider_config_new(1, string_from("anthropic"));  // 1 = ANTHROPIC
    provider_config_set_key(anthropic, string_from("sk-test-key"));
    provider_config_set_temp(anthropic, 0.7);
    provider_config_set_max_tokens(anthropic, 4096);
    provider_config_set_priority(anthropic, 1);
    let anthropic_id: i64 = provider_registry_add(reg, anthropic);
    print("Added Anthropic provider with ID: ");
    print_i64(anthropic_id);
    println("");

    // Create OpenAI provider
    let openai: i64 = provider_config_new(2, string_from("openai"));  // 2 = OPENAI
    provider_config_set_model(openai, string_from("gpt-4o-mini"));
    provider_config_set_priority(openai, 2);
    let openai_id: i64 = provider_registry_add(reg, openai);
    print("Added OpenAI provider with ID: ");
    print_i64(openai_id);
    println("");

    // Create Ollama provider (local, free)
    let ollama: i64 = provider_config_new(3, string_from("ollama"));  // 3 = OLLAMA
    provider_config_set_url(ollama, string_from("http://localhost:11434"));
    provider_config_set_cost(ollama, 0.0, 0.0);
    let ollama_id: i64 = provider_registry_add(reg, ollama);
    print("Added Ollama provider with ID: ");
    print_i64(ollama_id);
    println("");

    // Check provider count
    print("Total providers: ");
    print_i64(provider_registry_count(reg));
    println("");

    // List providers
    println("");
    println("--- Listing Providers ---");
    let providers: i64 = provider_registry_list(reg);
    print_string(providers);
    println("");

    // Set default provider
    provider_registry_set_default(reg, anthropic_id);
    println("Set Anthropic as default provider");

    // Test Token Counting
    println("");
    println("--- Testing Token Counting ---");
    let text: i64 = string_from("Hello, this is a test of token counting in Simplex!");
    print("Text: ");
    print_string(text);
    println("");
    print("Estimated tokens: ");
    print_i64(estimate_tokens(text));
    println("");
    print("Accurate tokens: ");
    print_i64(count_tokens_accurate(text));
    println("");

    // Record some stats
    provider_record_request(anthropic_id, 1, 100, 200, 0.00045, 250.5);
    provider_record_request(anthropic_id, 1, 150, 300, 0.00067, 300.2);
    provider_record_request(openai_id, 0, 100, 0, 0.0, 500.0);  // Failed request

    let stats: i64 = provider_get_stats(anthropic_id);
    println("Anthropic stats:");
    print_string(stats);
    println("");

    // Test Retry Config
    println("");
    println("--- Testing Retry Config ---");
    let retry: i64 = retry_config_new();
    retry_config_set_max(retry, 5);
    retry_config_set_delay(retry, 1000);
    retry_config_set_backoff(retry, 2.0);

    print("Delay for attempt 0: ");
    print_i64(retry_calculate_delay(retry, 0));
    println(" ms");
    print("Delay for attempt 1: ");
    print_i64(retry_calculate_delay(retry, 1));
    println(" ms");
    print("Delay for attempt 2: ");
    print_i64(retry_calculate_delay(retry, 2));
    println(" ms");

    // Test should_retry (1=timeout, 2=rate_limit, 3=server_error)
    print("Should retry timeout (attempt 0): ");
    print_i64(retry_should_retry(retry, 1, 0));
    println("");
    print("Should retry rate_limit (attempt 4): ");
    print_i64(retry_should_retry(retry, 2, 4));
    println("");
    print("Should retry rate_limit (attempt 5): ");
    print_i64(retry_should_retry(retry, 2, 5));
    println(" (exceeds max)");

    // Test Fallback Chain
    println("");
    println("--- Testing Fallback Chain ---");
    let chain: i64 = fallback_chain_new();
    fallback_chain_add(chain, anthropic_id);
    fallback_chain_add(chain, openai_id);
    fallback_chain_add(chain, ollama_id);

    print("Chain size: ");
    print_i64(fallback_chain_size(chain));
    println("");
    print("First provider: ");
    print_i64(fallback_chain_get(chain, 0));
    println("");
    print("Next after first: ");
    print_i64(fallback_chain_next(chain, 0));
    println("");
    print("Next after second: ");
    print_i64(fallback_chain_next(chain, 1));
    println("");
    print("Next after third: ");
    print_i64(fallback_chain_next(chain, 2));
    println(" (end of chain)");

    // Test Streaming
    println("");
    println("--- Testing Streaming ---");
    let stream_ctx: i64 = stream_context_new(0, 0);  // No callback

    stream_process_chunk(stream_ctx, string_from("Hello, "));
    stream_process_chunk(stream_ctx, string_from("this is "));
    stream_process_chunk(stream_ctx, string_from("streaming!"));
    stream_complete(stream_ctx);

    print("Is complete: ");
    print_i64(stream_is_complete(stream_ctx));
    println("");
    print("Has error: ");
    print_i64(stream_has_error(stream_ctx));
    println("");
    print("Token count: ");
    print_i64(stream_token_count(stream_ctx));
    println("");
    print("Accumulated content: ");
    print_string(stream_get_content(stream_ctx));
    println("");

    // Test Structured Output
    println("");
    println("--- Testing Structured Output ---");
    let schema: i64 = output_schema_new(
        string_from("analysis"),
        string_from("{\"type\":\"object\",\"properties\":{\"sentiment\":{\"type\":\"string\"}}}")
    );

    let schema_json: i64 = output_schema_get_json(schema);
    print("Schema: ");
    print_string(schema_json);
    println("");

    // Test validation
    let valid_json: i64 = string_from("{\"sentiment\":\"positive\"}");
    let invalid_json: i64 = string_from("not json at all");

    print("Valid JSON passes: ");
    print_i64(validate_json_output(valid_json, schema));
    println("");
    print("Invalid JSON passes: ");
    print_i64(validate_json_output(invalid_json, schema));
    println("");

    // Test Request Builder
    println("");
    println("--- Testing Request Builder ---");
    let req: i64 = llm_request_new(anthropic_id);
    llm_request_set_system(req, string_from("You are a helpful assistant."));
    llm_request_set_prompt(req, string_from("What is 2+2?"));
    llm_request_set_max_tokens(req, 100);
    llm_request_set_temperature(req, 0.5);
    llm_request_set_retry(req, retry);

    let req_json: i64 = llm_request_to_json(req, reg);
    print("Request JSON: ");
    print_string(req_json);
    println("");

    // Test Response Handler
    println("");
    println("--- Testing Response Handler ---");
    let resp: i64 = llm_response_new();
    llm_response_set_success(resp, 1);
    llm_response_set_content(resp, string_from("The answer is 4."));
    llm_response_set_tokens(resp, 50, 20);
    llm_response_set_cost(resp, 0.00021);
    llm_response_set_latency(resp, 125.5);

    print("Success: ");
    print_i64(llm_response_is_success(resp));
    println("");
    print("Content: ");
    print_string(llm_response_get_content(resp));
    println("");
    print("Input tokens: ");
    print_i64(llm_response_input_tokens(resp));
    println("");
    print("Output tokens: ");
    print_i64(llm_response_output_tokens(resp));
    println("");

    let resp_json: i64 = llm_response_to_json(resp);
    print("Response JSON: ");
    print_string(resp_json);
    println("");

    // Cleanup
    llm_response_close(resp);
    llm_request_close(req);
    output_schema_close(schema);
    stream_context_close(stream_ctx);
    fallback_chain_close(chain);
    retry_config_close(retry);
    provider_registry_close(reg);

    println("");
    println("=== Specialist Enhancements Test Complete! ===");
}
