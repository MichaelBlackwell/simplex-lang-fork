{"prompt": "How do I create a basic Dockerfile for a Node.js application?", "response": "```dockerfile\n# Use official Node.js LTS image\nFROM node:20-alpine\n\n# Set working directory\nWORKDIR /app\n\n# Copy package files first (better layer caching)\nCOPY package*.json ./\n\n# Install dependencies\nRUN npm ci --only=production\n\n# Copy application code\nCOPY . .\n\n# Create non-root user for security\nRUN addgroup -g 1001 -S nodejs && \\\n    adduser -S nodejs -u 1001\nUSER nodejs\n\n# Expose port\nEXPOSE 3000\n\n# Health check\nHEALTHCHECK --interval=30s --timeout=3s \\\n  CMD wget -q --spider http://localhost:3000/health || exit 1\n\n# Start application\nCMD [\"node\", \"server.js\"]\n```\n\n**Best practices:**\n- Use alpine images for smaller size\n- Copy package.json first for layer caching\n- Use `npm ci` instead of `npm install`\n- Run as non-root user\n- Add health checks"}
{"prompt": "How do I set up a GitHub Actions CI/CD pipeline?", "response": "**.github/workflows/ci.yml:**\n```yaml\nname: CI/CD Pipeline\n\non:\n  push:\n    branches: [main, develop]\n  pull_request:\n    branches: [main]\n\nenv:\n  REGISTRY: ghcr.io\n  IMAGE_NAME: ${{ github.repository }}\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: actions/setup-node@v4\n        with:\n          node-version: 20\n          cache: 'npm'\n      - run: npm ci\n      - run: npm run lint\n      - run: npm test\n\n  build:\n    needs: test\n    runs-on: ubuntu-latest\n    permissions:\n      contents: read\n      packages: write\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Log in to Container Registry\n        uses: docker/login-action@v3\n        with:\n          registry: ${{ env.REGISTRY }}\n          username: ${{ github.actor }}\n          password: ${{ secrets.GITHUB_TOKEN }}\n      \n      - name: Build and push Docker image\n        uses: docker/build-push-action@v5\n        with:\n          context: .\n          push: ${{ github.ref == 'refs/heads/main' }}\n          tags: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.sha }}\n\n  deploy:\n    needs: build\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy to production\n        run: |\n          # Add deployment commands here\n          echo \"Deploying ${{ github.sha }}\"\n```"}
{"prompt": "How do I write a Kubernetes deployment manifest?", "response": "```yaml\n# deployment.yaml\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: myapp\n  labels:\n    app: myapp\nspec:\n  replicas: 3\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: myregistry/myapp:v1.0.0\n        ports:\n        - containerPort: 3000\n        resources:\n          requests:\n            memory: \"128Mi\"\n            cpu: \"100m\"\n          limits:\n            memory: \"256Mi\"\n            cpu: \"500m\"\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 10\n          periodSeconds: 10\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n        env:\n        - name: NODE_ENV\n          value: \"production\"\n        - name: DB_PASSWORD\n          valueFrom:\n            secretKeyRef:\n              name: db-secrets\n              key: password\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: myapp-service\nspec:\n  selector:\n    app: myapp\n  ports:\n  - port: 80\n    targetPort: 3000\n  type: ClusterIP\n```"}
{"prompt": "How do I set up Terraform for AWS infrastructure?", "response": "**main.tf:**\n```hcl\nterraform {\n  required_version = \">= 1.0\"\n  required_providers {\n    aws = {\n      source  = \"hashicorp/aws\"\n      version = \"~> 5.0\"\n    }\n  }\n  backend \"s3\" {\n    bucket = \"my-terraform-state\"\n    key    = \"prod/terraform.tfstate\"\n    region = \"us-east-1\"\n    encrypt = true\n    dynamodb_table = \"terraform-locks\"\n  }\n}\n\nprovider \"aws\" {\n  region = var.aws_region\n  default_tags {\n    tags = {\n      Environment = var.environment\n      ManagedBy   = \"terraform\"\n    }\n  }\n}\n\n# VPC\nmodule \"vpc\" {\n  source  = \"terraform-aws-modules/vpc/aws\"\n  version = \"5.0.0\"\n  \n  name = \"${var.project}-vpc\"\n  cidr = \"10.0.0.0/16\"\n  \n  azs             = [\"us-east-1a\", \"us-east-1b\"]\n  private_subnets = [\"10.0.1.0/24\", \"10.0.2.0/24\"]\n  public_subnets  = [\"10.0.101.0/24\", \"10.0.102.0/24\"]\n  \n  enable_nat_gateway = true\n  single_nat_gateway = var.environment != \"prod\"\n}\n\n# ECS Cluster\nresource \"aws_ecs_cluster\" \"main\" {\n  name = \"${var.project}-cluster\"\n  \n  setting {\n    name  = \"containerInsights\"\n    value = \"enabled\"\n  }\n}\n```\n\n**variables.tf:**\n```hcl\nvariable \"aws_region\" {\n  default = \"us-east-1\"\n}\n\nvariable \"environment\" {\n  type = string\n}\n\nvariable \"project\" {\n  type = string\n}\n```"}
{"prompt": "How do I configure nginx as a reverse proxy?", "response": "```nginx\n# /etc/nginx/nginx.conf\nuser nginx;\nworker_processes auto;\nerror_log /var/log/nginx/error.log warn;\npid /var/run/nginx.pid;\n\nevents {\n    worker_connections 1024;\n    use epoll;\n}\n\nhttp {\n    include /etc/nginx/mime.types;\n    default_type application/octet-stream;\n    \n    # Logging\n    log_format main '$remote_addr - $remote_user [$time_local] \"$request\" '\n                    '$status $body_bytes_sent \"$http_referer\" '\n                    '\"$http_user_agent\" \"$http_x_forwarded_for\"';\n    access_log /var/log/nginx/access.log main;\n    \n    # Performance\n    sendfile on;\n    tcp_nopush on;\n    tcp_nodelay on;\n    keepalive_timeout 65;\n    gzip on;\n    gzip_types text/plain application/json application/javascript text/css;\n    \n    # Rate limiting\n    limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;\n    \n    # Upstream servers\n    upstream backend {\n        least_conn;\n        server app1:3000 weight=3;\n        server app2:3000 weight=2;\n        server app3:3000 backup;\n        keepalive 32;\n    }\n    \n    server {\n        listen 80;\n        server_name example.com;\n        return 301 https://$server_name$request_uri;\n    }\n    \n    server {\n        listen 443 ssl http2;\n        server_name example.com;\n        \n        ssl_certificate /etc/ssl/certs/cert.pem;\n        ssl_certificate_key /etc/ssl/private/key.pem;\n        ssl_protocols TLSv1.2 TLSv1.3;\n        \n        location / {\n            proxy_pass http://backend;\n            proxy_http_version 1.1;\n            proxy_set_header Host $host;\n            proxy_set_header X-Real-IP $remote_addr;\n            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n            proxy_set_header X-Forwarded-Proto $scheme;\n            proxy_set_header Connection \"\";\n        }\n        \n        location /api/ {\n            limit_req zone=api burst=20 nodelay;\n            proxy_pass http://backend;\n        }\n    }\n}\n```"}
{"prompt": "How do I set up Docker Compose for local development?", "response": "**docker-compose.yml:**\n```yaml\nversion: '3.8'\n\nservices:\n  app:\n    build:\n      context: .\n      dockerfile: Dockerfile.dev\n    ports:\n      - \"3000:3000\"\n    volumes:\n      - .:/app\n      - /app/node_modules\n    environment:\n      - NODE_ENV=development\n      - DATABASE_URL=postgres://user:pass@db:5432/myapp\n      - REDIS_URL=redis://redis:6379\n    depends_on:\n      db:\n        condition: service_healthy\n      redis:\n        condition: service_started\n    command: npm run dev\n\n  db:\n    image: postgres:15-alpine\n    ports:\n      - \"5432:5432\"\n    environment:\n      POSTGRES_USER: user\n      POSTGRES_PASSWORD: pass\n      POSTGRES_DB: myapp\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n      - ./init.sql:/docker-entrypoint-initdb.d/init.sql\n    healthcheck:\n      test: [\"CMD-SHELL\", \"pg_isready -U user -d myapp\"]\n      interval: 5s\n      timeout: 5s\n      retries: 5\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n    volumes:\n      - redis_data:/data\n\n  nginx:\n    image: nginx:alpine\n    ports:\n      - \"80:80\"\n    volumes:\n      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n    depends_on:\n      - app\n\nvolumes:\n  postgres_data:\n  redis_data:\n\nnetworks:\n  default:\n    name: myapp-network\n```\n\n**Commands:**\n```bash\ndocker-compose up -d      # Start all services\ndocker-compose logs -f    # Follow logs\ndocker-compose down -v    # Stop and remove volumes\n```"}
{"prompt": "How do I implement blue-green deployments?", "response": "**Blue-green deployment** runs two identical production environments, switching traffic between them.\n\n**AWS ALB approach:**\n```hcl\n# Terraform\nresource \"aws_lb_target_group\" \"blue\" {\n  name     = \"app-blue\"\n  port     = 3000\n  protocol = \"HTTP\"\n  vpc_id   = var.vpc_id\n  \n  health_check {\n    path = \"/health\"\n    healthy_threshold = 2\n    unhealthy_threshold = 3\n  }\n}\n\nresource \"aws_lb_target_group\" \"green\" {\n  name     = \"app-green\"\n  port     = 3000\n  protocol = \"HTTP\"\n  vpc_id   = var.vpc_id\n  \n  health_check {\n    path = \"/health\"\n  }\n}\n\n# Switch traffic\nresource \"aws_lb_listener_rule\" \"app\" {\n  listener_arn = aws_lb_listener.main.arn\n  \n  action {\n    type             = \"forward\"\n    target_group_arn = var.active_color == \"blue\" ? \n      aws_lb_target_group.blue.arn : \n      aws_lb_target_group.green.arn\n  }\n  \n  condition {\n    path_pattern { values = [\"/*\"] }\n  }\n}\n```\n\n**Deployment script:**\n```bash\n#!/bin/bash\nset -e\n\nCURRENT=$(aws ssm get-parameter --name /app/active-color --query 'Parameter.Value' --output text)\nNEW=$([ \"$CURRENT\" = \"blue\" ] && echo \"green\" || echo \"blue\")\n\necho \"Current: $CURRENT, Deploying to: $NEW\"\n\n# Deploy to inactive environment\naws ecs update-service --cluster prod --service app-$NEW --force-new-deployment\naws ecs wait services-stable --cluster prod --services app-$NEW\n\n# Run smoke tests\n./smoke-test.sh $NEW\n\n# Switch traffic\naws ssm put-parameter --name /app/active-color --value $NEW --overwrite\nterraform apply -var=\"active_color=$NEW\" -auto-approve\n\necho \"Deployment complete. Active: $NEW\"\necho \"Rollback: aws ssm put-parameter --name /app/active-color --value $CURRENT --overwrite\"\n```"}
{"prompt": "How do I set up monitoring with Prometheus and Grafana?", "response": "**docker-compose.monitoring.yml:**\n```yaml\nversion: '3.8'\n\nservices:\n  prometheus:\n    image: prom/prometheus:latest\n    ports:\n      - \"9090:9090\"\n    volumes:\n      - ./prometheus.yml:/etc/prometheus/prometheus.yml\n      - prometheus_data:/prometheus\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.retention.time=15d'\n\n  grafana:\n    image: grafana/grafana:latest\n    ports:\n      - \"3001:3000\"\n    environment:\n      - GF_SECURITY_ADMIN_PASSWORD=admin\n      - GF_USERS_ALLOW_SIGN_UP=false\n    volumes:\n      - grafana_data:/var/lib/grafana\n      - ./grafana/dashboards:/etc/grafana/provisioning/dashboards\n      - ./grafana/datasources:/etc/grafana/provisioning/datasources\n\n  alertmanager:\n    image: prom/alertmanager:latest\n    ports:\n      - \"9093:9093\"\n    volumes:\n      - ./alertmanager.yml:/etc/alertmanager/alertmanager.yml\n\nvolumes:\n  prometheus_data:\n  grafana_data:\n```\n\n**prometheus.yml:**\n```yaml\nglobal:\n  scrape_interval: 15s\n  evaluation_interval: 15s\n\nalerting:\n  alertmanagers:\n    - static_configs:\n        - targets: ['alertmanager:9093']\n\nrule_files:\n  - 'alerts/*.yml'\n\nscrape_configs:\n  - job_name: 'prometheus'\n    static_configs:\n      - targets: ['localhost:9090']\n\n  - job_name: 'node-exporter'\n    static_configs:\n      - targets: ['node-exporter:9100']\n\n  - job_name: 'app'\n    metrics_path: '/metrics'\n    static_configs:\n      - targets: ['app:3000']\n\n  - job_name: 'kubernetes-pods'\n    kubernetes_sd_configs:\n      - role: pod\n    relabel_configs:\n      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n        action: keep\n        regex: true\n```\n\n**Node.js metrics endpoint:**\n```javascript\nimport { Registry, collectDefaultMetrics, Counter, Histogram } from 'prom-client';\n\nconst register = new Registry();\ncollectDefaultMetrics({ register });\n\nconst httpRequestDuration = new Histogram({\n  name: 'http_request_duration_seconds',\n  help: 'HTTP request duration',\n  labelNames: ['method', 'route', 'status'],\n  buckets: [0.1, 0.5, 1, 2, 5]\n});\nregister.registerMetric(httpRequestDuration);\n\napp.get('/metrics', async (req, res) => {\n  res.set('Content-Type', register.contentType);\n  res.end(await register.metrics());\n});\n```"}
{"prompt": "How do I configure AWS ECS with Fargate?", "response": "**Terraform ECS Fargate setup:**\n```hcl\n# ECS Task Definition\nresource \"aws_ecs_task_definition\" \"app\" {\n  family                   = \"myapp\"\n  network_mode             = \"awsvpc\"\n  requires_compatibilities = [\"FARGATE\"]\n  cpu                      = 256\n  memory                   = 512\n  execution_role_arn       = aws_iam_role.ecs_execution.arn\n  task_role_arn            = aws_iam_role.ecs_task.arn\n\n  container_definitions = jsonencode([{\n    name  = \"app\"\n    image = \"${aws_ecr_repository.app.repository_url}:latest\"\n    \n    portMappings = [{\n      containerPort = 3000\n      protocol      = \"tcp\"\n    }]\n    \n    environment = [\n      { name = \"NODE_ENV\", value = \"production\" }\n    ]\n    \n    secrets = [\n      {\n        name      = \"DATABASE_URL\"\n        valueFrom = aws_secretsmanager_secret.db_url.arn\n      }\n    ]\n    \n    logConfiguration = {\n      logDriver = \"awslogs\"\n      options = {\n        awslogs-group         = aws_cloudwatch_log_group.app.name\n        awslogs-region        = var.aws_region\n        awslogs-stream-prefix = \"ecs\"\n      }\n    }\n    \n    healthCheck = {\n      command     = [\"CMD-SHELL\", \"curl -f http://localhost:3000/health || exit 1\"]\n      interval    = 30\n      timeout     = 5\n      retries     = 3\n      startPeriod = 60\n    }\n  }])\n}\n\n# ECS Service\nresource \"aws_ecs_service\" \"app\" {\n  name            = \"myapp-service\"\n  cluster         = aws_ecs_cluster.main.id\n  task_definition = aws_ecs_task_definition.app.arn\n  desired_count   = 2\n  launch_type     = \"FARGATE\"\n\n  network_configuration {\n    subnets          = var.private_subnet_ids\n    security_groups  = [aws_security_group.ecs.id]\n    assign_public_ip = false\n  }\n\n  load_balancer {\n    target_group_arn = aws_lb_target_group.app.arn\n    container_name   = \"app\"\n    container_port   = 3000\n  }\n\n  deployment_circuit_breaker {\n    enable   = true\n    rollback = true\n  }\n}\n\n# Auto Scaling\nresource \"aws_appautoscaling_target\" \"ecs\" {\n  max_capacity       = 10\n  min_capacity       = 2\n  resource_id        = \"service/${aws_ecs_cluster.main.name}/${aws_ecs_service.app.name}\"\n  scalable_dimension = \"ecs:service:DesiredCount\"\n  service_namespace  = \"ecs\"\n}\n\nresource \"aws_appautoscaling_policy\" \"cpu\" {\n  name               = \"cpu-autoscaling\"\n  policy_type        = \"TargetTrackingScaling\"\n  resource_id        = aws_appautoscaling_target.ecs.resource_id\n  scalable_dimension = aws_appautoscaling_target.ecs.scalable_dimension\n  service_namespace  = aws_appautoscaling_target.ecs.service_namespace\n\n  target_tracking_scaling_policy_configuration {\n    target_value       = 70\n    predefined_metric_specification {\n      predefined_metric_type = \"ECSServiceAverageCPUUtilization\"\n    }\n  }\n}\n```"}
{"prompt": "How do I implement GitOps with ArgoCD?", "response": "**ArgoCD GitOps setup:**\n\n**1. Install ArgoCD:**\n```bash\nkubectl create namespace argocd\nkubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml\n\n# Get admin password\nkubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=\"{.data.password}\" | base64 -d\n```\n\n**2. Application manifest (app.yaml):**\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Application\nmetadata:\n  name: myapp\n  namespace: argocd\nspec:\n  project: default\n  source:\n    repoURL: https://github.com/myorg/myapp-k8s.git\n    targetRevision: main\n    path: overlays/production\n  destination:\n    server: https://kubernetes.default.svc\n    namespace: production\n  syncPolicy:\n    automated:\n      prune: true\n      selfHeal: true\n    syncOptions:\n      - CreateNamespace=true\n    retry:\n      limit: 5\n      backoff:\n        duration: 5s\n        factor: 2\n        maxDuration: 3m\n```\n\n**3. Kustomize structure:**\n```\nmyapp-k8s/\n├── base/\n│   ├── deployment.yaml\n│   ├── service.yaml\n│   └── kustomization.yaml\n└── overlays/\n    ├── staging/\n    │   ├── kustomization.yaml\n    │   └── replicas-patch.yaml\n    └── production/\n        ├── kustomization.yaml\n        ├── replicas-patch.yaml\n        └── resources-patch.yaml\n```\n\n**base/kustomization.yaml:**\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nresources:\n  - deployment.yaml\n  - service.yaml\n```\n\n**overlays/production/kustomization.yaml:**\n```yaml\napiVersion: kustomize.config.k8s.io/v1beta1\nkind: Kustomization\nbases:\n  - ../../base\nnamePrefix: prod-\nnamespace: production\nimages:\n  - name: myapp\n    newTag: v1.2.3\npatchesStrategicMerge:\n  - replicas-patch.yaml\n  - resources-patch.yaml\n```\n\n**CI pipeline updates image tag:**\n```yaml\n# GitHub Actions\n- name: Update image tag\n  run: |\n    cd myapp-k8s\n    kustomize edit set image myapp=myregistry/myapp:${{ github.sha }}\n    git commit -am \"Deploy ${{ github.sha }}\"\n    git push\n```"}
{"prompt": "How do I set up centralized logging with ELK stack?", "response": "**docker-compose.elk.yml:**\n```yaml\nversion: '3.8'\n\nservices:\n  elasticsearch:\n    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0\n    environment:\n      - discovery.type=single-node\n      - xpack.security.enabled=false\n      - \"ES_JAVA_OPTS=-Xms512m -Xmx512m\"\n    volumes:\n      - elasticsearch_data:/usr/share/elasticsearch/data\n    ports:\n      - \"9200:9200\"\n    healthcheck:\n      test: curl -s http://localhost:9200 >/dev/null || exit 1\n      interval: 30s\n      timeout: 10s\n      retries: 5\n\n  logstash:\n    image: docker.elastic.co/logstash/logstash:8.11.0\n    volumes:\n      - ./logstash.conf:/usr/share/logstash/pipeline/logstash.conf\n    ports:\n      - \"5044:5044\"\n      - \"5000:5000/udp\"\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n\n  kibana:\n    image: docker.elastic.co/kibana/kibana:8.11.0\n    environment:\n      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200\n    ports:\n      - \"5601:5601\"\n    depends_on:\n      elasticsearch:\n        condition: service_healthy\n\n  filebeat:\n    image: docker.elastic.co/beats/filebeat:8.11.0\n    user: root\n    volumes:\n      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml:ro\n      - /var/lib/docker/containers:/var/lib/docker/containers:ro\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n    depends_on:\n      - logstash\n\nvolumes:\n  elasticsearch_data:\n```\n\n**logstash.conf:**\n```\ninput {\n  beats {\n    port => 5044\n  }\n  tcp {\n    port => 5000\n    codec => json\n  }\n}\n\nfilter {\n  if [message] =~ /^\\{/ {\n    json {\n      source => \"message\"\n    }\n  }\n  \n  mutate {\n    add_field => { \"environment\" => \"production\" }\n  }\n  \n  date {\n    match => [\"timestamp\", \"ISO8601\"]\n  }\n}\n\noutput {\n  elasticsearch {\n    hosts => [\"elasticsearch:9200\"]\n    index => \"logs-%{+YYYY.MM.dd}\"\n  }\n}\n```\n\n**Application logging:**\n```javascript\nimport winston from 'winston';\n\nconst logger = winston.createLogger({\n  format: winston.format.combine(\n    winston.format.timestamp(),\n    winston.format.json()\n  ),\n  transports: [\n    new winston.transports.Console(),\n    new winston.transports.Http({\n      host: 'logstash',\n      port: 5000\n    })\n  ]\n});\n\nlogger.info('User logged in', { userId: 123, action: 'login' });\n```"}
{"prompt": "How do I manage secrets in Kubernetes?", "response": "**Multiple approaches for K8s secrets:**\n\n**1. Native Kubernetes Secrets (base64 encoded, not encrypted):**\n```yaml\napiVersion: v1\nkind: Secret\nmetadata:\n  name: app-secrets\ntype: Opaque\nstringData:  # Plain text, K8s encodes it\n  DATABASE_URL: \"postgres://user:pass@host:5432/db\"\n  API_KEY: \"secret-key-here\"\n---\n# Usage in deployment\nenv:\n  - name: DATABASE_URL\n    valueFrom:\n      secretKeyRef:\n        name: app-secrets\n        key: DATABASE_URL\n```\n\n**2. Sealed Secrets (GitOps-safe):**\n```bash\n# Install\nkubectl apply -f https://github.com/bitnami-labs/sealed-secrets/releases/download/v0.24.0/controller.yaml\n\n# Create sealed secret\nkubectl create secret generic app-secrets --dry-run=client --from-literal=API_KEY=secret -o yaml | \\\n  kubeseal --format yaml > sealed-secret.yaml\n```\n\n**3. External Secrets Operator (AWS Secrets Manager):**\n```yaml\napiVersion: external-secrets.io/v1beta1\nkind: ExternalSecret\nmetadata:\n  name: app-secrets\nspec:\n  refreshInterval: 1h\n  secretStoreRef:\n    name: aws-secrets-manager\n    kind: ClusterSecretStore\n  target:\n    name: app-secrets\n  data:\n    - secretKey: DATABASE_URL\n      remoteRef:\n        key: prod/myapp/database\n        property: url\n---\napiVersion: external-secrets.io/v1beta1\nkind: ClusterSecretStore\nmetadata:\n  name: aws-secrets-manager\nspec:\n  provider:\n    aws:\n      service: SecretsManager\n      region: us-east-1\n      auth:\n        jwt:\n          serviceAccountRef:\n            name: external-secrets-sa\n            namespace: external-secrets\n```\n\n**4. HashiCorp Vault:**\n```yaml\napiVersion: secrets.hashicorp.com/v1beta1\nkind: VaultStaticSecret\nmetadata:\n  name: app-secrets\nspec:\n  vaultAuthRef: vault-auth\n  mount: secret\n  path: myapp/config\n  destination:\n    name: app-secrets\n    create: true\n  refreshAfter: 30s\n```\n\n**Best practices:**\n- Never commit plain secrets to Git\n- Use external secret managers in production\n- Rotate secrets regularly\n- Limit secret access with RBAC"}
{"prompt": "How do I implement infrastructure as code testing?", "response": "**Testing IaC with multiple tools:**\n\n**1. Terraform validation and linting:**\n```bash\n# Format check\nterraform fmt -check -recursive\n\n# Validate syntax\nterraform validate\n\n# TFLint for best practices\ntflint --init\ntflint --recursive\n```\n\n**.tflint.hcl:**\n```hcl\nconfig {\n  module = true\n}\n\nplugin \"aws\" {\n  enabled = true\n  version = \"0.27.0\"\n  source  = \"github.com/terraform-linters/tflint-ruleset-aws\"\n}\n\nrule \"terraform_naming_convention\" {\n  enabled = true\n}\n```\n\n**2. Checkov for security scanning:**\n```bash\ncheckov -d . --framework terraform\n```\n\n**3. Terratest for integration testing:**\n```go\n// test/vpc_test.go\npackage test\n\nimport (\n    \"testing\"\n    \"github.com/gruntwork-io/terratest/modules/terraform\"\n    \"github.com/stretchr/testify/assert\"\n)\n\nfunc TestVPC(t *testing.T) {\n    t.Parallel()\n\n    terraformOptions := &terraform.Options{\n        TerraformDir: \"../modules/vpc\",\n        Vars: map[string]interface{}{\n            \"environment\": \"test\",\n            \"cidr_block\":  \"10.0.0.0/16\",\n        },\n    }\n\n    defer terraform.Destroy(t, terraformOptions)\n    terraform.InitAndApply(t, terraformOptions)\n\n    vpcID := terraform.Output(t, terraformOptions, \"vpc_id\")\n    assert.NotEmpty(t, vpcID)\n\n    // Verify VPC exists\n    vpc := aws.GetVpcById(t, vpcID, \"us-east-1\")\n    assert.Equal(t, \"10.0.0.0/16\", vpc.CidrBlock)\n}\n```\n\n**4. GitHub Actions pipeline:**\n```yaml\nname: Terraform CI\n\non: [pull_request]\n\njobs:\n  validate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - uses: hashicorp/setup-terraform@v3\n      \n      - name: Terraform Format\n        run: terraform fmt -check -recursive\n      \n      - name: Terraform Init\n        run: terraform init -backend=false\n      \n      - name: Terraform Validate\n        run: terraform validate\n      \n      - name: TFLint\n        uses: terraform-linters/setup-tflint@v4\n      - run: tflint --recursive\n      \n      - name: Checkov\n        uses: bridgecrewio/checkov-action@v12\n        with:\n          directory: .\n          framework: terraform\n      \n      - name: Terraform Plan\n        run: terraform plan -no-color\n        env:\n          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}\n          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}\n```"}
{"prompt": "How do I set up a CI/CD pipeline for a microservices architecture?", "response": "**Monorepo CI/CD with path-based triggers:**\n\n**.github/workflows/ci.yml:**\n```yaml\nname: Microservices CI/CD\n\non:\n  push:\n    branches: [main]\n    paths:\n      - 'services/**'\n      - '.github/workflows/**'\n  pull_request:\n    branches: [main]\n\njobs:\n  detect-changes:\n    runs-on: ubuntu-latest\n    outputs:\n      services: ${{ steps.changes.outputs.services }}\n    steps:\n      - uses: actions/checkout@v4\n      - uses: dorny/paths-filter@v2\n        id: changes\n        with:\n          filters: |\n            user-service:\n              - 'services/user/**'\n            order-service:\n              - 'services/order/**'\n            payment-service:\n              - 'services/payment/**'\n            shared:\n              - 'libs/shared/**'\n      \n      - id: set-services\n        run: |\n          services=()\n          if [ \"${{ steps.changes.outputs.user-service }}\" == \"true\" ] || [ \"${{ steps.changes.outputs.shared }}\" == \"true\" ]; then\n            services+=(\"user\")\n          fi\n          if [ \"${{ steps.changes.outputs.order-service }}\" == \"true\" ] || [ \"${{ steps.changes.outputs.shared }}\" == \"true\" ]; then\n            services+=(\"order\")\n          fi\n          if [ \"${{ steps.changes.outputs.payment-service }}\" == \"true\" ] || [ \"${{ steps.changes.outputs.shared }}\" == \"true\" ]; then\n            services+=(\"payment\")\n          fi\n          echo \"services=$(jq -cn '$ARGS.positional' --args \"${services[@]}\")\" >> $GITHUB_OUTPUT\n\n  build-and-test:\n    needs: detect-changes\n    if: needs.detect-changes.outputs.services != '[]'\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        service: ${{ fromJson(needs.detect-changes.outputs.services) }}\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Build and test ${{ matrix.service }}\n        run: |\n          cd services/${{ matrix.service }}\n          docker build -t ${{ matrix.service }}:test .\n          docker run --rm ${{ matrix.service }}:test npm test\n      \n      - name: Push to registry\n        if: github.ref == 'refs/heads/main'\n        run: |\n          echo ${{ secrets.GITHUB_TOKEN }} | docker login ghcr.io -u ${{ github.actor }} --password-stdin\n          docker tag ${{ matrix.service }}:test ghcr.io/${{ github.repository }}/${{ matrix.service }}:${{ github.sha }}\n          docker push ghcr.io/${{ github.repository }}/${{ matrix.service }}:${{ github.sha }}\n\n  deploy:\n    needs: [detect-changes, build-and-test]\n    if: github.ref == 'refs/heads/main'\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        service: ${{ fromJson(needs.detect-changes.outputs.services) }}\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Update Kubernetes manifests\n        run: |\n          cd k8s/overlays/production\n          kustomize edit set image ${{ matrix.service }}=ghcr.io/${{ github.repository }}/${{ matrix.service }}:${{ github.sha }}\n      \n      - name: Commit and push\n        run: |\n          git config user.name github-actions\n          git config user.email github-actions@github.com\n          git add .\n          git commit -m \"Deploy ${{ matrix.service }}:${{ github.sha }}\"\n          git push\n```"}
{"prompt": "How do I configure auto-scaling in Kubernetes?", "response": "**Horizontal Pod Autoscaler (HPA):**\n```yaml\napiVersion: autoscaling/v2\nkind: HorizontalPodAutoscaler\nmetadata:\n  name: myapp-hpa\nspec:\n  scaleTargetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  minReplicas: 2\n  maxReplicas: 20\n  metrics:\n  - type: Resource\n    resource:\n      name: cpu\n      target:\n        type: Utilization\n        averageUtilization: 70\n  - type: Resource\n    resource:\n      name: memory\n      target:\n        type: Utilization\n        averageUtilization: 80\n  - type: Pods\n    pods:\n      metric:\n        name: http_requests_per_second\n      target:\n        type: AverageValue\n        averageValue: 1000\n  behavior:\n    scaleDown:\n      stabilizationWindowSeconds: 300\n      policies:\n      - type: Percent\n        value: 10\n        periodSeconds: 60\n    scaleUp:\n      stabilizationWindowSeconds: 0\n      policies:\n      - type: Percent\n        value: 100\n        periodSeconds: 15\n      - type: Pods\n        value: 4\n        periodSeconds: 15\n      selectPolicy: Max\n```\n\n**Vertical Pod Autoscaler (VPA):**\n```yaml\napiVersion: autoscaling.k8s.io/v1\nkind: VerticalPodAutoscaler\nmetadata:\n  name: myapp-vpa\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  updatePolicy:\n    updateMode: Auto  # Off, Initial, Recreate, Auto\n  resourcePolicy:\n    containerPolicies:\n    - containerName: '*'\n      minAllowed:\n        cpu: 100m\n        memory: 128Mi\n      maxAllowed:\n        cpu: 4\n        memory: 8Gi\n```\n\n**Cluster Autoscaler (node scaling):**\n```yaml\n# AWS EKS node group with autoscaling\napiVersion: eksctl.io/v1alpha5\nkind: ClusterConfig\nmetadata:\n  name: my-cluster\n  region: us-east-1\n\nmanagedNodeGroups:\n  - name: workers\n    instanceType: m5.large\n    minSize: 2\n    maxSize: 10\n    desiredCapacity: 3\n    labels:\n      role: worker\n    iam:\n      withAddonPolicies:\n        autoScaler: true\n```\n\n**KEDA for event-driven scaling:**\n```yaml\napiVersion: keda.sh/v1alpha1\nkind: ScaledObject\nmetadata:\n  name: myapp-scaler\nspec:\n  scaleTargetRef:\n    name: myapp\n  minReplicaCount: 0\n  maxReplicaCount: 100\n  triggers:\n  - type: aws-sqs-queue\n    metadata:\n      queueURL: https://sqs.us-east-1.amazonaws.com/123456789/myqueue\n      queueLength: \"10\"\n      awsRegion: us-east-1\n```"}
{"prompt": "How do I implement database migrations in a CI/CD pipeline?", "response": "**Database migration strategies:**\n\n**1. Flyway with Docker:**\n```dockerfile\n# Dockerfile.migrations\nFROM flyway/flyway:latest\nCOPY sql /flyway/sql\n```\n\n**sql/V1__create_users.sql:**\n```sql\nCREATE TABLE users (\n    id SERIAL PRIMARY KEY,\n    email VARCHAR(255) NOT NULL UNIQUE,\n    created_at TIMESTAMP DEFAULT NOW()\n);\n```\n\n**GitHub Actions:**\n```yaml\njobs:\n  migrate:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Run migrations\n        run: |\n          docker run --rm \\\n            -v $(pwd)/sql:/flyway/sql \\\n            flyway/flyway \\\n            -url=jdbc:postgresql://${{ secrets.DB_HOST }}:5432/mydb \\\n            -user=${{ secrets.DB_USER }} \\\n            -password=${{ secrets.DB_PASSWORD }} \\\n            migrate\n\n  deploy:\n    needs: migrate\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy application\n        run: # deploy after migrations succeed\n```\n\n**2. Kubernetes Job for migrations:**\n```yaml\napiVersion: batch/v1\nkind: Job\nmetadata:\n  name: db-migrate-${{ .Release.Revision }}\n  annotations:\n    helm.sh/hook: pre-upgrade,pre-install\n    helm.sh/hook-weight: \"-1\"\n    helm.sh/hook-delete-policy: hook-succeeded\nspec:\n  template:\n    spec:\n      restartPolicy: Never\n      containers:\n      - name: migrate\n        image: myapp:{{ .Values.image.tag }}\n        command: [\"npm\", \"run\", \"migrate\"]\n        env:\n        - name: DATABASE_URL\n          valueFrom:\n            secretKeyRef:\n              name: db-secrets\n              key: url\n```\n\n**3. Prisma migrations:**\n```yaml\n# GitHub Actions\n- name: Deploy migrations\n  run: npx prisma migrate deploy\n  env:\n    DATABASE_URL: ${{ secrets.DATABASE_URL }}\n```\n\n**4. Safe migration practices:**\n```sql\n-- Always backwards compatible\n-- Step 1: Add new column (nullable)\nALTER TABLE users ADD COLUMN name VARCHAR(255);\n\n-- Step 2: Deploy app that writes to both columns\n-- Step 3: Backfill data\nUPDATE users SET name = email WHERE name IS NULL;\n\n-- Step 4: Make column required (after app deployed)\nALTER TABLE users ALTER COLUMN name SET NOT NULL;\n\n-- Step 5: Remove old column (after app no longer uses it)\nALTER TABLE users DROP COLUMN old_column;\n```\n\n**Rollback strategy:**\n```bash\n# Flyway\nflyway undo\n\n# Prisma\nnpx prisma migrate resolve --rolled-back \"migration_name\"\n```"}
{"prompt": "How do I set up SSL/TLS certificates with Let's Encrypt?", "response": "**Multiple approaches for Let's Encrypt:**\n\n**1. Certbot standalone:**\n```bash\n# Install certbot\nsudo apt install certbot\n\n# Get certificate (stops web server temporarily)\nsudo certbot certonly --standalone -d example.com -d www.example.com\n\n# Auto-renewal cron\n0 0 1 * * /usr/bin/certbot renew --quiet\n```\n\n**2. Certbot with Nginx:**\n```bash\nsudo certbot --nginx -d example.com -d www.example.com\n\n# Nginx config after certbot\nserver {\n    listen 443 ssl http2;\n    server_name example.com;\n    \n    ssl_certificate /etc/letsencrypt/live/example.com/fullchain.pem;\n    ssl_certificate_key /etc/letsencrypt/live/example.com/privkey.pem;\n    include /etc/letsencrypt/options-ssl-nginx.conf;\n    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem;\n}\n```\n\n**3. Docker with Traefik:**\n```yaml\nversion: '3.8'\n\nservices:\n  traefik:\n    image: traefik:v2.10\n    command:\n      - \"--providers.docker=true\"\n      - \"--entrypoints.web.address=:80\"\n      - \"--entrypoints.websecure.address=:443\"\n      - \"--certificatesresolvers.letsencrypt.acme.httpchallenge=true\"\n      - \"--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web\"\n      - \"--certificatesresolvers.letsencrypt.acme.email=admin@example.com\"\n      - \"--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json\"\n    ports:\n      - \"80:80\"\n      - \"443:443\"\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n      - letsencrypt:/letsencrypt\n\n  app:\n    image: myapp\n    labels:\n      - \"traefik.enable=true\"\n      - \"traefik.http.routers.app.rule=Host(`example.com`)\"\n      - \"traefik.http.routers.app.tls.certresolver=letsencrypt\"\n      - \"traefik.http.routers.app.entrypoints=websecure\"\n      - \"traefik.http.routers.app-http.rule=Host(`example.com`)\"\n      - \"traefik.http.routers.app-http.entrypoints=web\"\n      - \"traefik.http.routers.app-http.middlewares=redirect-https\"\n      - \"traefik.http.middlewares.redirect-https.redirectscheme.scheme=https\"\n\nvolumes:\n  letsencrypt:\n```\n\n**4. Kubernetes with cert-manager:**\n```yaml\napiVersion: cert-manager.io/v1\nkind: ClusterIssuer\nmetadata:\n  name: letsencrypt-prod\nspec:\n  acme:\n    server: https://acme-v02.api.letsencrypt.org/directory\n    email: admin@example.com\n    privateKeySecretRef:\n      name: letsencrypt-prod\n    solvers:\n    - http01:\n        ingress:\n          class: nginx\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: myapp\n  annotations:\n    cert-manager.io/cluster-issuer: letsencrypt-prod\nspec:\n  tls:\n  - hosts:\n    - example.com\n    secretName: example-com-tls\n  rules:\n  - host: example.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: myapp\n            port:\n              number: 80\n```"}
{"prompt": "How do I implement a rollback strategy for deployments?", "response": "**Multiple rollback approaches:**\n\n**1. Kubernetes native rollback:**\n```bash\n# View rollout history\nkubectl rollout history deployment/myapp\n\n# Rollback to previous version\nkubectl rollout undo deployment/myapp\n\n# Rollback to specific revision\nkubectl rollout undo deployment/myapp --to-revision=3\n\n# Check rollout status\nkubectl rollout status deployment/myapp\n```\n\n**2. Helm rollback:**\n```bash\n# List releases\nhelm history myapp\n\n# Rollback\nhelm rollback myapp 3\n\n# Rollback with timeout\nhelm rollback myapp 3 --timeout 5m --wait\n```\n\n**3. GitOps rollback (ArgoCD):**\n```bash\n# Revert git commit\ngit revert HEAD\ngit push\n\n# Or use ArgoCD CLI\nargocd app rollback myapp\n```\n\n**4. AWS ECS rollback:**\n```bash\n# Get previous task definition\nPREV_TASK=$(aws ecs describe-services --cluster prod --services myapp \\\n  --query 'services[0].deployments[1].taskDefinition' --output text)\n\n# Update service to previous version\naws ecs update-service --cluster prod --service myapp --task-definition $PREV_TASK\n```\n\n**5. Terraform rollback:**\n```bash\n# Option 1: Revert git and re-apply\ngit revert HEAD\nterraform apply\n\n# Option 2: Target specific resource\nterraform apply -target=aws_ecs_service.myapp -var=\"image_tag=v1.2.2\"\n```\n\n**Automated rollback with health checks:**\n```yaml\n# Kubernetes deployment with rollback trigger\napiVersion: apps/v1\nkind: Deployment\nspec:\n  progressDeadlineSeconds: 600\n  strategy:\n    rollingUpdate:\n      maxSurge: 25%\n      maxUnavailable: 0\n  template:\n    spec:\n      containers:\n      - name: app\n        livenessProbe:\n          httpGet:\n            path: /health\n            port: 3000\n          initialDelaySeconds: 30\n          periodSeconds: 10\n          failureThreshold: 3\n        readinessProbe:\n          httpGet:\n            path: /ready\n            port: 3000\n          initialDelaySeconds: 5\n          periodSeconds: 5\n          failureThreshold: 3\n```\n\n**GitHub Actions with automatic rollback:**\n```yaml\n- name: Deploy\n  id: deploy\n  run: kubectl apply -f k8s/\n\n- name: Wait for rollout\n  id: rollout\n  run: |\n    if ! kubectl rollout status deployment/myapp --timeout=300s; then\n      echo \"Deployment failed, rolling back...\"\n      kubectl rollout undo deployment/myapp\n      exit 1\n    fi\n```"}
{"prompt": "How do I set up a VPN connection between cloud and on-premises?", "response": "**AWS Site-to-Site VPN with Terraform:**\n\n```hcl\n# Customer Gateway (on-premises router)\nresource \"aws_customer_gateway\" \"onprem\" {\n  bgp_asn    = 65000\n  ip_address = var.onprem_public_ip\n  type       = \"ipsec.1\"\n  \n  tags = {\n    Name = \"onprem-gateway\"\n  }\n}\n\n# Virtual Private Gateway\nresource \"aws_vpn_gateway\" \"main\" {\n  vpc_id = aws_vpc.main.id\n  \n  tags = {\n    Name = \"vpn-gateway\"\n  }\n}\n\n# VPN Connection\nresource \"aws_vpn_connection\" \"main\" {\n  vpn_gateway_id      = aws_vpn_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.onprem.id\n  type                = \"ipsec.1\"\n  static_routes_only  = false  # Use BGP\n  \n  tunnel1_ike_versions                 = [\"ikev2\"]\n  tunnel1_phase1_encryption_algorithms = [\"AES256\"]\n  tunnel1_phase1_integrity_algorithms  = [\"SHA2-256\"]\n  tunnel1_phase1_dh_group_numbers      = [14]\n  tunnel1_phase2_encryption_algorithms = [\"AES256\"]\n  tunnel1_phase2_integrity_algorithms  = [\"SHA2-256\"]\n  tunnel1_phase2_dh_group_numbers      = [14]\n  \n  tags = {\n    Name = \"onprem-vpn\"\n  }\n}\n\n# Route propagation\nresource \"aws_vpn_gateway_route_propagation\" \"main\" {\n  vpn_gateway_id = aws_vpn_gateway.main.id\n  route_table_id = aws_route_table.private.id\n}\n\n# Output tunnel configuration for on-prem router\noutput \"vpn_tunnel1_address\" {\n  value = aws_vpn_connection.main.tunnel1_address\n}\n\noutput \"vpn_tunnel1_preshared_key\" {\n  value     = aws_vpn_connection.main.tunnel1_preshared_key\n  sensitive = true\n}\n```\n\n**On-premises router config (example for strongSwan):**\n```\n# /etc/ipsec.conf\nconn aws-tunnel1\n    authby=secret\n    auto=start\n    left=%defaultroute\n    leftid=<on-prem-public-ip>\n    leftsubnet=10.0.0.0/16\n    right=<aws-tunnel1-address>\n    rightsubnet=172.31.0.0/16\n    ike=aes256-sha256-modp2048\n    esp=aes256-sha256-modp2048\n    keyexchange=ikev2\n    ikelifetime=8h\n    lifetime=1h\n    dpddelay=10s\n    dpdtimeout=30s\n    dpdaction=restart\n```\n\n**AWS Transit Gateway for multiple VPCs:**\n```hcl\nresource \"aws_ec2_transit_gateway\" \"main\" {\n  description = \"Main Transit Gateway\"\n  \n  default_route_table_association = \"enable\"\n  default_route_table_propagation = \"enable\"\n  dns_support                     = \"enable\"\n  vpn_ecmp_support               = \"enable\"\n}\n\nresource \"aws_ec2_transit_gateway_vpc_attachment\" \"vpc1\" {\n  subnet_ids         = aws_subnet.private[*].id\n  transit_gateway_id = aws_ec2_transit_gateway.main.id\n  vpc_id             = aws_vpc.main.id\n}\n\nresource \"aws_vpn_connection\" \"tgw\" {\n  transit_gateway_id  = aws_ec2_transit_gateway.main.id\n  customer_gateway_id = aws_customer_gateway.onprem.id\n  type                = \"ipsec.1\"\n}\n```"}
{"prompt": "How do I implement container image scanning in CI/CD?", "response": "**Container security scanning pipeline:**\n\n**1. Trivy scanning:**\n```yaml\n# GitHub Actions\nname: Container Security\n\non:\n  push:\n    branches: [main]\n  pull_request:\n\njobs:\n  scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Build image\n        run: docker build -t myapp:${{ github.sha }} .\n      \n      - name: Run Trivy vulnerability scanner\n        uses: aquasecurity/trivy-action@master\n        with:\n          image-ref: myapp:${{ github.sha }}\n          format: 'sarif'\n          output: 'trivy-results.sarif'\n          severity: 'CRITICAL,HIGH'\n          exit-code: '1'\n      \n      - name: Upload Trivy scan results\n        uses: github/codeql-action/upload-sarif@v2\n        if: always()\n        with:\n          sarif_file: 'trivy-results.sarif'\n```\n\n**2. Multi-scanner approach:**\n```yaml\njobs:\n  security-scan:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      \n      - name: Build image\n        run: docker build -t myapp:test .\n      \n      # Trivy - vulnerabilities\n      - name: Trivy scan\n        run: |\n          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n            aquasec/trivy image --exit-code 1 --severity HIGH,CRITICAL myapp:test\n      \n      # Dockle - best practices\n      - name: Dockle lint\n        run: |\n          docker run --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n            goodwithtech/dockle --exit-code 1 myapp:test\n      \n      # Grype - vulnerabilities (alternative)\n      - name: Grype scan\n        run: |\n          curl -sSfL https://raw.githubusercontent.com/anchore/grype/main/install.sh | sh -s -- -b /usr/local/bin\n          grype myapp:test --fail-on high\n      \n      # Hadolint - Dockerfile linting\n      - name: Hadolint\n        uses: hadolint/hadolint-action@v3.1.0\n        with:\n          dockerfile: Dockerfile\n          failure-threshold: error\n```\n\n**3. AWS ECR scanning:**\n```hcl\nresource \"aws_ecr_repository\" \"app\" {\n  name                 = \"myapp\"\n  image_tag_mutability = \"IMMUTABLE\"\n  \n  image_scanning_configuration {\n    scan_on_push = true\n  }\n}\n\n# Check scan results before deployment\nresource \"aws_codepipeline\" \"main\" {\n  stage {\n    name = \"SecurityScan\"\n    action {\n      name     = \"CheckVulnerabilities\"\n      category = \"Invoke\"\n      owner    = \"AWS\"\n      provider = \"Lambda\"\n      configuration = {\n        FunctionName = aws_lambda_function.check_ecr_scan.function_name\n      }\n    }\n  }\n}\n```\n\n**Lambda to check ECR scan:**\n```python\nimport boto3\n\ndef handler(event, context):\n    ecr = boto3.client('ecr')\n    \n    response = ecr.describe_image_scan_findings(\n        repositoryName='myapp',\n        imageId={'imageTag': event['tag']}\n    )\n    \n    findings = response['imageScanFindings']\n    critical = findings['findingSeverityCounts'].get('CRITICAL', 0)\n    high = findings['findingSeverityCounts'].get('HIGH', 0)\n    \n    if critical > 0 or high > 5:\n        raise Exception(f'Image has {critical} critical and {high} high vulnerabilities')\n    \n    return {'status': 'passed'}\n```"}
{"prompt": "How do I implement chaos engineering with Chaos Monkey?", "response": "**Chaos engineering tools and practices:**\n\n**1. Chaos Mesh for Kubernetes:**\n```yaml\n# Install Chaos Mesh\nhelm repo add chaos-mesh https://charts.chaos-mesh.org\nhelm install chaos-mesh chaos-mesh/chaos-mesh -n chaos-testing --create-namespace\n\n# Pod failure experiment\napiVersion: chaos-mesh.org/v1alpha1\nkind: PodChaos\nmetadata:\n  name: pod-failure\n  namespace: chaos-testing\nspec:\n  action: pod-failure\n  mode: one\n  selector:\n    namespaces:\n      - production\n    labelSelectors:\n      app: myapp\n  duration: '30s'\n  scheduler:\n    cron: '@every 2h'\n```\n\n**Network chaos:**\n```yaml\napiVersion: chaos-mesh.org/v1alpha1\nkind: NetworkChaos\nmetadata:\n  name: network-delay\nspec:\n  action: delay\n  mode: all\n  selector:\n    labelSelectors:\n      app: myapp\n  delay:\n    latency: '100ms'\n    jitter: '50ms'\n    correlation: '50'\n  duration: '5m'\n```\n\n**2. Litmus Chaos:**\n```yaml\napiVersion: litmuschaos.io/v1alpha1\nkind: ChaosEngine\nmetadata:\n  name: nginx-chaos\nspec:\n  appinfo:\n    appns: 'default'\n    applabel: 'app=nginx'\n    appkind: 'deployment'\n  chaosServiceAccount: litmus-admin\n  experiments:\n    - name: pod-delete\n      spec:\n        components:\n          env:\n            - name: TOTAL_CHAOS_DURATION\n              value: '30'\n            - name: CHAOS_INTERVAL\n              value: '10'\n            - name: FORCE\n              value: 'false'\n```\n\n**3. AWS Fault Injection Simulator:**\n```hcl\nresource \"aws_fis_experiment_template\" \"ec2_terminate\" {\n  description = \"Terminate random EC2 instance\"\n  role_arn    = aws_iam_role.fis.arn\n  \n  stop_condition {\n    source = \"none\"\n  }\n  \n  action {\n    name      = \"terminate-instances\"\n    action_id = \"aws:ec2:terminate-instances\"\n    \n    target {\n      key   = \"Instances\"\n      value = \"myapp-instances\"\n    }\n  }\n  \n  target {\n    name           = \"myapp-instances\"\n    resource_type  = \"aws:ec2:instance\"\n    selection_mode = \"COUNT(1)\"\n    \n    resource_tag {\n      key   = \"Application\"\n      value = \"myapp\"\n    }\n  }\n}\n```\n\n**4. Simple chaos script:**\n```bash\n#!/bin/bash\n# Simple chaos testing script\n\nNAMESPACE=\"production\"\nAPP=\"myapp\"\n\n# Get random pod\nPOD=$(kubectl get pods -n $NAMESPACE -l app=$APP -o jsonpath='{.items[*].metadata.name}' | tr ' ' '\\n' | shuf -n 1)\n\necho \"Killing pod: $POD\"\nkubectl delete pod $POD -n $NAMESPACE\n\n# Monitor recovery\necho \"Waiting for recovery...\"\nkubectl rollout status deployment/$APP -n $NAMESPACE --timeout=120s\n\nif [ $? -eq 0 ]; then\n  echo \"Recovery successful!\"\nelse\n  echo \"Recovery failed - alert!\"\n  # Send alert\nfi\n```\n\n**Game day runbook:**\n1. Notify team of chaos test\n2. Ensure monitoring dashboards are visible\n3. Run experiment with defined blast radius\n4. Observe system behavior and recovery\n5. Document findings\n6. Create tickets for improvements"}
{"prompt": "How do I set up a multi-region deployment?", "response": "**Multi-region architecture with AWS:**\n\n**Terraform multi-region setup:**\n```hcl\n# providers.tf\nprovider \"aws\" {\n  alias  = \"primary\"\n  region = \"us-east-1\"\n}\n\nprovider \"aws\" {\n  alias  = \"secondary\"\n  region = \"eu-west-1\"\n}\n\n# Global resources\nresource \"aws_route53_zone\" \"main\" {\n  name = \"example.com\"\n}\n\n# Primary region\nmodule \"primary\" {\n  source = \"./modules/region\"\n  providers = {\n    aws = aws.primary\n  }\n  \n  region      = \"us-east-1\"\n  environment = \"prod\"\n  is_primary  = true\n}\n\n# Secondary region\nmodule \"secondary\" {\n  source = \"./modules/region\"\n  providers = {\n    aws = aws.secondary\n  }\n  \n  region      = \"eu-west-1\"\n  environment = \"prod\"\n  is_primary  = false\n}\n\n# Global Accelerator for traffic routing\nresource \"aws_globalaccelerator_accelerator\" \"main\" {\n  name            = \"myapp-accelerator\"\n  ip_address_type = \"IPV4\"\n  enabled         = true\n}\n\nresource \"aws_globalaccelerator_listener\" \"main\" {\n  accelerator_arn = aws_globalaccelerator_accelerator.main.id\n  protocol        = \"TCP\"\n  \n  port_range {\n    from_port = 443\n    to_port   = 443\n  }\n}\n\nresource \"aws_globalaccelerator_endpoint_group\" \"primary\" {\n  listener_arn          = aws_globalaccelerator_listener.main.id\n  endpoint_group_region = \"us-east-1\"\n  \n  endpoint_configuration {\n    endpoint_id = module.primary.alb_arn\n    weight      = 100\n  }\n  \n  health_check_path             = \"/health\"\n  health_check_interval_seconds = 10\n  threshold_count               = 3\n}\n\nresource \"aws_globalaccelerator_endpoint_group\" \"secondary\" {\n  listener_arn          = aws_globalaccelerator_listener.main.id\n  endpoint_group_region = \"eu-west-1\"\n  \n  endpoint_configuration {\n    endpoint_id = module.secondary.alb_arn\n    weight      = 100\n  }\n}\n```\n\n**Route 53 failover routing:**\n```hcl\nresource \"aws_route53_health_check\" \"primary\" {\n  fqdn              = module.primary.alb_dns_name\n  port              = 443\n  type              = \"HTTPS\"\n  resource_path     = \"/health\"\n  failure_threshold = 3\n  request_interval  = 10\n}\n\nresource \"aws_route53_record\" \"primary\" {\n  zone_id = aws_route53_zone.main.zone_id\n  name    = \"api.example.com\"\n  type    = \"A\"\n  \n  failover_routing_policy {\n    type = \"PRIMARY\"\n  }\n  \n  set_identifier  = \"primary\"\n  health_check_id = aws_route53_health_check.primary.id\n  \n  alias {\n    name                   = module.primary.alb_dns_name\n    zone_id                = module.primary.alb_zone_id\n    evaluate_target_health = true\n  }\n}\n\nresource \"aws_route53_record\" \"secondary\" {\n  zone_id = aws_route53_zone.main.zone_id\n  name    = \"api.example.com\"\n  type    = \"A\"\n  \n  failover_routing_policy {\n    type = \"SECONDARY\"\n  }\n  \n  set_identifier = \"secondary\"\n  \n  alias {\n    name                   = module.secondary.alb_dns_name\n    zone_id                = module.secondary.alb_zone_id\n    evaluate_target_health = true\n  }\n}\n```\n\n**Multi-region database (Aurora Global):**\n```hcl\nresource \"aws_rds_global_cluster\" \"main\" {\n  global_cluster_identifier = \"myapp-global\"\n  engine                    = \"aurora-postgresql\"\n  engine_version            = \"15.4\"\n  database_name             = \"myapp\"\n}\n\nresource \"aws_rds_cluster\" \"primary\" {\n  provider                  = aws.primary\n  cluster_identifier        = \"myapp-primary\"\n  global_cluster_identifier = aws_rds_global_cluster.main.id\n  engine                    = aws_rds_global_cluster.main.engine\n  engine_version            = aws_rds_global_cluster.main.engine_version\n  master_username           = \"admin\"\n  master_password           = var.db_password\n}\n\nresource \"aws_rds_cluster\" \"secondary\" {\n  provider                  = aws.secondary\n  cluster_identifier        = \"myapp-secondary\"\n  global_cluster_identifier = aws_rds_global_cluster.main.id\n  engine                    = aws_rds_global_cluster.main.engine\n  engine_version            = aws_rds_global_cluster.main.engine_version\n  \n  depends_on = [aws_rds_cluster.primary]\n}\n```"}
{"prompt": "How do I configure log aggregation with Fluentd?", "response": "**Fluentd log aggregation setup:**\n\n**Kubernetes DaemonSet:**\n```yaml\napiVersion: v1\nkind: ConfigMap\nmetadata:\n  name: fluentd-config\n  namespace: logging\ndata:\n  fluent.conf: |\n    <source>\n      @type tail\n      path /var/log/containers/*.log\n      pos_file /var/log/fluentd-containers.log.pos\n      tag kubernetes.*\n      read_from_head true\n      <parse>\n        @type json\n        time_key time\n        time_format %Y-%m-%dT%H:%M:%S.%NZ\n      </parse>\n    </source>\n\n    <filter kubernetes.**>\n      @type kubernetes_metadata\n      @id filter_kube_metadata\n    </filter>\n\n    <filter kubernetes.**>\n      @type record_transformer\n      <record>\n        cluster_name \"${ENV['CLUSTER_NAME']}\"\n        environment \"${ENV['ENVIRONMENT']}\"\n      </record>\n    </filter>\n\n    # Route logs based on namespace\n    <match kubernetes.var.log.containers.**production**>\n      @type elasticsearch\n      host elasticsearch.logging.svc\n      port 9200\n      logstash_format true\n      logstash_prefix prod-logs\n      include_tag_key true\n      <buffer>\n        @type file\n        path /var/log/fluentd-buffers/prod\n        flush_mode interval\n        flush_interval 5s\n        retry_type exponential_backoff\n      </buffer>\n    </match>\n\n    <match kubernetes.**>\n      @type elasticsearch\n      host elasticsearch.logging.svc\n      port 9200\n      logstash_format true\n      logstash_prefix k8s-logs\n    </match>\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: logging\nspec:\n  selector:\n    matchLabels:\n      name: fluentd\n  template:\n    metadata:\n      labels:\n        name: fluentd\n    spec:\n      serviceAccountName: fluentd\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8\n        env:\n        - name: CLUSTER_NAME\n          value: \"production\"\n        - name: ENVIRONMENT\n          value: \"prod\"\n        resources:\n          limits:\n            memory: 500Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: containers\n          mountPath: /var/lib/docker/containers\n          readOnly: true\n        - name: config\n          mountPath: /fluentd/etc/fluent.conf\n          subPath: fluent.conf\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: containers\n        hostPath:\n          path: /var/lib/docker/containers\n      - name: config\n        configMap:\n          name: fluentd-config\n```\n\n**Send to multiple destinations:**\n```\n<match **>\n  @type copy\n  \n  <store>\n    @type elasticsearch\n    host elasticsearch.logging.svc\n    port 9200\n    logstash_format true\n  </store>\n  \n  <store>\n    @type s3\n    s3_bucket my-logs-bucket\n    s3_region us-east-1\n    path logs/%Y/%m/%d/\n    <buffer time>\n      @type file\n      path /var/log/fluentd-buffers/s3\n      timekey 3600\n      timekey_wait 10m\n    </buffer>\n  </store>\n  \n  <store>\n    @type cloudwatch_logs\n    region us-east-1\n    log_group_name /kubernetes/production\n    auto_create_stream true\n  </store>\n</match>\n```\n\n**Filter sensitive data:**\n```\n<filter **>\n  @type record_transformer\n  enable_ruby true\n  <record>\n    message ${record[\"message\"].gsub(/password=[^&\\s]+/, 'password=REDACTED')}\n  </record>\n</filter>\n```"}
{"prompt": "How do I implement service mesh with Istio?", "response": "**Istio service mesh setup:**\n\n**1. Install Istio:**\n```bash\ncurl -L https://istio.io/downloadIstio | sh -\ncd istio-*\nexport PATH=$PWD/bin:$PATH\n\n# Install with production profile\nistioctl install --set profile=production -y\n\n# Enable sidecar injection\nkubectl label namespace default istio-injection=enabled\n```\n\n**2. Traffic management (VirtualService):**\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp\nspec:\n  hosts:\n  - myapp\n  http:\n  - match:\n    - headers:\n        x-canary:\n          exact: \"true\"\n    route:\n    - destination:\n        host: myapp\n        subset: canary\n  - route:\n    - destination:\n        host: myapp\n        subset: stable\n      weight: 90\n    - destination:\n        host: myapp\n        subset: canary\n      weight: 10\n    retries:\n      attempts: 3\n      perTryTimeout: 2s\n    timeout: 10s\n---\napiVersion: networking.istio.io/v1beta1\nkind: DestinationRule\nmetadata:\n  name: myapp\nspec:\n  host: myapp\n  trafficPolicy:\n    connectionPool:\n      tcp:\n        maxConnections: 100\n      http:\n        h2UpgradePolicy: UPGRADE\n        http1MaxPendingRequests: 100\n        http2MaxRequests: 1000\n    loadBalancer:\n      simple: LEAST_CONN\n    outlierDetection:\n      consecutive5xxErrors: 5\n      interval: 30s\n      baseEjectionTime: 30s\n  subsets:\n  - name: stable\n    labels:\n      version: v1\n  - name: canary\n    labels:\n      version: v2\n```\n\n**3. mTLS and security:**\n```yaml\napiVersion: security.istio.io/v1beta1\nkind: PeerAuthentication\nmetadata:\n  name: default\n  namespace: istio-system\nspec:\n  mtls:\n    mode: STRICT\n---\napiVersion: security.istio.io/v1beta1\nkind: AuthorizationPolicy\nmetadata:\n  name: myapp-policy\nspec:\n  selector:\n    matchLabels:\n      app: myapp\n  rules:\n  - from:\n    - source:\n        principals: [\"cluster.local/ns/default/sa/frontend\"]\n    to:\n    - operation:\n        methods: [\"GET\", \"POST\"]\n        paths: [\"/api/*\"]\n```\n\n**4. Gateway for external traffic:**\n```yaml\napiVersion: networking.istio.io/v1beta1\nkind: Gateway\nmetadata:\n  name: main-gateway\nspec:\n  selector:\n    istio: ingressgateway\n  servers:\n  - port:\n      number: 443\n      name: https\n      protocol: HTTPS\n    tls:\n      mode: SIMPLE\n      credentialName: myapp-tls\n    hosts:\n    - \"api.example.com\"\n---\napiVersion: networking.istio.io/v1beta1\nkind: VirtualService\nmetadata:\n  name: myapp-external\nspec:\n  hosts:\n  - \"api.example.com\"\n  gateways:\n  - main-gateway\n  http:\n  - route:\n    - destination:\n        host: myapp\n        port:\n          number: 80\n```\n\n**5. Observability:**\n```bash\n# Install addons\nkubectl apply -f samples/addons/prometheus.yaml\nkubectl apply -f samples/addons/grafana.yaml\nkubectl apply -f samples/addons/jaeger.yaml\nkubectl apply -f samples/addons/kiali.yaml\n\n# Access dashboards\nistioctl dashboard kiali\nistioctl dashboard grafana\nistioctl dashboard jaeger\n```"}
{"prompt": "How do I set up disaster recovery for cloud infrastructure?", "response": "**Disaster recovery strategies:**\n\n**1. Backup strategy with Terraform:**\n```hcl\n# RDS automated backups\nresource \"aws_db_instance\" \"main\" {\n  identifier     = \"myapp-db\"\n  engine         = \"postgres\"\n  engine_version = \"15\"\n  instance_class = \"db.r6g.large\"\n  \n  backup_retention_period = 30\n  backup_window          = \"03:00-04:00\"\n  maintenance_window     = \"Mon:04:00-Mon:05:00\"\n  \n  copy_tags_to_snapshot = true\n  delete_automated_backups_on_delete = false\n  \n  # Cross-region snapshot copy\n  replicate_source_db = null  # Set for read replica in DR region\n}\n\n# S3 cross-region replication\nresource \"aws_s3_bucket\" \"primary\" {\n  bucket = \"myapp-data-primary\"\n}\n\nresource \"aws_s3_bucket_replication_configuration\" \"primary\" {\n  bucket = aws_s3_bucket.primary.id\n  role   = aws_iam_role.replication.arn\n  \n  rule {\n    id     = \"replicate-all\"\n    status = \"Enabled\"\n    \n    destination {\n      bucket        = aws_s3_bucket.dr.arn\n      storage_class = \"STANDARD_IA\"\n    }\n  }\n}\n```\n\n**2. Velero for Kubernetes backup:**\n```bash\n# Install Velero\nvelero install \\\n  --provider aws \\\n  --bucket velero-backups \\\n  --secret-file ./credentials-velero \\\n  --backup-location-config region=us-east-1 \\\n  --snapshot-location-config region=us-east-1\n\n# Schedule daily backups\nvelero schedule create daily-backup \\\n  --schedule=\"0 2 * * *\" \\\n  --include-namespaces production \\\n  --ttl 720h\n\n# Manual backup\nvelero backup create prod-backup --include-namespaces production\n\n# Restore\nvelero restore create --from-backup prod-backup\n```\n\n**Velero backup schedule:**\n```yaml\napiVersion: velero.io/v1\nkind: Schedule\nmetadata:\n  name: daily-production-backup\n  namespace: velero\nspec:\n  schedule: \"0 2 * * *\"\n  template:\n    includedNamespaces:\n    - production\n    - database\n    storageLocation: default\n    volumeSnapshotLocations:\n    - aws-primary\n    ttl: 720h0m0s\n    hooks:\n      resources:\n      - name: backup-hook\n        includedNamespaces:\n        - database\n        pre:\n        - exec:\n            container: postgres\n            command:\n            - /bin/sh\n            - -c\n            - pg_dump -U postgres mydb > /backup/dump.sql\n```\n\n**3. DR runbook automation:**\n```python\n#!/usr/bin/env python3\n\"\"\"Disaster Recovery Automation Script\"\"\"\n\nimport boto3\nimport subprocess\nimport time\n\nclass DisasterRecovery:\n    def __init__(self, primary_region, dr_region):\n        self.primary = primary_region\n        self.dr = dr_region\n        self.ec2_dr = boto3.client('ec2', region_name=dr_region)\n        self.rds_dr = boto3.client('rds', region_name=dr_region)\n        self.route53 = boto3.client('route53')\n    \n    def failover_database(self):\n        \"\"\"Promote RDS read replica to primary\"\"\"\n        print(\"Promoting DR database...\")\n        self.rds_dr.promote_read_replica(\n            DBInstanceIdentifier='myapp-db-dr'\n        )\n        \n        # Wait for promotion\n        waiter = self.rds_dr.get_waiter('db_instance_available')\n        waiter.wait(DBInstanceIdentifier='myapp-db-dr')\n        print(\"Database promoted successfully\")\n    \n    def update_dns(self):\n        \"\"\"Update Route53 to point to DR region\"\"\"\n        print(\"Updating DNS records...\")\n        self.route53.change_resource_record_sets(\n            HostedZoneId='ZONE_ID',\n            ChangeBatch={\n                'Changes': [{\n                    'Action': 'UPSERT',\n                    'ResourceRecordSet': {\n                        'Name': 'api.example.com',\n                        'Type': 'A',\n                        'AliasTarget': {\n                            'HostedZoneId': 'DR_ALB_ZONE_ID',\n                            'DNSName': 'dr-alb.us-west-2.elb.amazonaws.com',\n                            'EvaluateTargetHealth': True\n                        }\n                    }\n                }]\n            }\n        )\n    \n    def scale_dr_infrastructure(self):\n        \"\"\"Scale up DR infrastructure\"\"\"\n        subprocess.run([\n            'kubectl', 'scale', 'deployment', 'myapp',\n            '--replicas=10', '-n', 'production',\n            '--context', f'arn:aws:eks:{self.dr}:ACCOUNT:cluster/dr-cluster'\n        ])\n    \n    def execute_failover(self):\n        \"\"\"Full DR failover procedure\"\"\"\n        print(f\"Starting failover to {self.dr}\")\n        \n        self.failover_database()\n        self.scale_dr_infrastructure()\n        self.update_dns()\n        \n        print(\"Failover complete!\")\n\nif __name__ == '__main__':\n    dr = DisasterRecovery('us-east-1', 'us-west-2')\n    dr.execute_failover()\n```\n\n**RPO/RTO targets:**\n- Hot standby: RPO < 1 min, RTO < 15 min\n- Warm standby: RPO < 15 min, RTO < 1 hour\n- Pilot light: RPO < 1 hour, RTO < 4 hours\n- Backup/restore: RPO = last backup, RTO = hours"}
{"prompt": "How do I implement canary deployments?", "response": "**Canary deployment strategies:**\n\n**1. Kubernetes with Flagger (Istio):**\n```yaml\napiVersion: flagger.app/v1beta1\nkind: Canary\nmetadata:\n  name: myapp\nspec:\n  targetRef:\n    apiVersion: apps/v1\n    kind: Deployment\n    name: myapp\n  service:\n    port: 80\n    targetPort: 3000\n  analysis:\n    interval: 1m\n    threshold: 5\n    maxWeight: 50\n    stepWeight: 10\n    metrics:\n    - name: request-success-rate\n      thresholdRange:\n        min: 99\n      interval: 1m\n    - name: request-duration\n      thresholdRange:\n        max: 500\n      interval: 1m\n    webhooks:\n    - name: load-test\n      url: http://flagger-loadtester.test/\n      metadata:\n        cmd: \"hey -z 1m -q 10 -c 2 http://myapp-canary.production/\"\n    - name: acceptance-test\n      type: pre-rollout\n      url: http://flagger-loadtester.test/\n      metadata:\n        type: bash\n        cmd: \"curl -s http://myapp-canary.production/health | grep ok\"\n```\n\n**2. AWS ALB weighted routing:**\n```hcl\nresource \"aws_lb_listener_rule\" \"canary\" {\n  listener_arn = aws_lb_listener.main.arn\n  priority     = 100\n\n  action {\n    type = \"forward\"\n    forward {\n      target_group {\n        arn    = aws_lb_target_group.stable.arn\n        weight = 90\n      }\n      target_group {\n        arn    = aws_lb_target_group.canary.arn\n        weight = 10\n      }\n    }\n  }\n\n  condition {\n    path_pattern {\n      values = [\"/*\"]\n    }\n  }\n}\n```\n\n**3. Argo Rollouts:**\n```yaml\napiVersion: argoproj.io/v1alpha1\nkind: Rollout\nmetadata:\n  name: myapp\nspec:\n  replicas: 10\n  selector:\n    matchLabels:\n      app: myapp\n  template:\n    metadata:\n      labels:\n        app: myapp\n    spec:\n      containers:\n      - name: myapp\n        image: myapp:v2\n        ports:\n        - containerPort: 3000\n  strategy:\n    canary:\n      steps:\n      - setWeight: 5\n      - pause: {duration: 5m}\n      - setWeight: 20\n      - pause: {duration: 5m}\n      - setWeight: 50\n      - pause: {duration: 5m}\n      - setWeight: 80\n      - pause: {duration: 5m}\n      analysis:\n        templates:\n        - templateName: success-rate\n        startingStep: 2\n        args:\n        - name: service-name\n          value: myapp\n      trafficRouting:\n        nginx:\n          stableIngress: myapp-stable\n---\napiVersion: argoproj.io/v1alpha1\nkind: AnalysisTemplate\nmetadata:\n  name: success-rate\nspec:\n  args:\n  - name: service-name\n  metrics:\n  - name: success-rate\n    interval: 1m\n    successCondition: result[0] >= 0.99\n    failureLimit: 3\n    provider:\n      prometheus:\n        address: http://prometheus:9090\n        query: |\n          sum(rate(http_requests_total{service=\"{{args.service-name}}\",status=~\"2.*\"}[5m]))\n          /\n          sum(rate(http_requests_total{service=\"{{args.service-name}}\"}[5m]))\n```\n\n**4. GitHub Actions canary workflow:**\n```yaml\njobs:\n  canary-deploy:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Deploy canary (10%)\n        run: |\n          kubectl argo rollouts set weight myapp 10\n          sleep 300  # 5 minutes\n      \n      - name: Check metrics\n        id: metrics\n        run: |\n          ERROR_RATE=$(curl -s prometheus/api/v1/query?query=... | jq '.data.result[0].value[1]')\n          if (( $(echo \"$ERROR_RATE > 0.01\" | bc -l) )); then\n            echo \"High error rate: $ERROR_RATE\"\n            exit 1\n          fi\n      \n      - name: Promote to 50%\n        if: success()\n        run: kubectl argo rollouts set weight myapp 50\n      \n      - name: Rollback on failure\n        if: failure()\n        run: kubectl argo rollouts abort myapp\n```"}
