{"prompt": "How do I load and explore a dataset with pandas?", "response": "```python\nimport pandas as pd\nimport numpy as np\n\n# Load data\ndf = pd.read_csv('data.csv')\n\n# Basic exploration\nprint(df.shape)              # (rows, columns)\nprint(df.head())             # First 5 rows\nprint(df.info())             # Column types, non-null counts\nprint(df.describe())         # Statistical summary\n\n# Check for missing values\nprint(df.isnull().sum())\n\n# Check data types\nprint(df.dtypes)\n\n# Unique values per column\nfor col in df.columns:\n    print(f'{col}: {df[col].nunique()} unique values')\n```"}
{"prompt": "How do I handle missing values in a dataset?", "response": "```python\nimport pandas as pd\nimport numpy as np\n\n# Check missing values\nprint(df.isnull().sum())\nprint(df.isnull().sum() / len(df) * 100)  # Percentage\n\n# Drop rows with any missing values\ndf_clean = df.dropna()\n\n# Drop rows where specific columns are missing\ndf_clean = df.dropna(subset=['important_col'])\n\n# Fill with specific value\ndf['col'] = df['col'].fillna(0)\ndf['col'] = df['col'].fillna('Unknown')\n\n# Fill with mean/median/mode\ndf['numeric_col'] = df['numeric_col'].fillna(df['numeric_col'].mean())\ndf['numeric_col'] = df['numeric_col'].fillna(df['numeric_col'].median())\ndf['cat_col'] = df['cat_col'].fillna(df['cat_col'].mode()[0])\n\n# Forward/backward fill (time series)\ndf['col'] = df['col'].fillna(method='ffill')\ndf['col'] = df['col'].fillna(method='bfill')\n\n# Interpolation\ndf['col'] = df['col'].interpolate(method='linear')\n```"}
{"prompt": "How do I build a machine learning pipeline with scikit-learn?", "response": "```python\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\n# Define feature types\nnumeric_features = ['age', 'income', 'score']\ncategorical_features = ['gender', 'category', 'region']\n\n# Preprocessing pipelines\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessors\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Full pipeline with model\npipeline = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n])\n\n# Train and evaluate\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\npipeline.fit(X_train, y_train)\nprint(f'Accuracy: {pipeline.score(X_test, y_test):.3f}')\n\n# Cross-validation\nscores = cross_val_score(pipeline, X, y, cv=5)\nprint(f'CV Score: {scores.mean():.3f} (+/- {scores.std()*2:.3f})')\n```"}
{"prompt": "How do I perform feature engineering for machine learning?", "response": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import PolynomialFeatures\n\n# Date features\ndf['date'] = pd.to_datetime(df['date'])\ndf['year'] = df['date'].dt.year\ndf['month'] = df['date'].dt.month\ndf['day'] = df['date'].dt.day\ndf['dayofweek'] = df['date'].dt.dayofweek\ndf['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\ndf['quarter'] = df['date'].dt.quarter\n\n# Binning continuous variables\ndf['age_group'] = pd.cut(df['age'], bins=[0, 18, 35, 50, 65, 100],\n                         labels=['child', 'young', 'middle', 'senior', 'elderly'])\n\n# Log transform for skewed data\ndf['log_income'] = np.log1p(df['income'])\n\n# Polynomial features\npoly = PolynomialFeatures(degree=2, include_bias=False)\nX_poly = poly.fit_transform(df[['feature1', 'feature2']])\n\n# Interaction features\ndf['feature_interaction'] = df['feature1'] * df['feature2']\n\n# Aggregation features (for grouped data)\ndf['customer_avg_purchase'] = df.groupby('customer_id')['amount'].transform('mean')\ndf['customer_total_orders'] = df.groupby('customer_id')['order_id'].transform('count')\n\n# Text features\ndf['text_length'] = df['text'].str.len()\ndf['word_count'] = df['text'].str.split().str.len()\n\n# Target encoding (with care for leakage)\nfrom sklearn.model_selection import KFold\nmeans = df.groupby('category')['target'].mean()\ndf['category_encoded'] = df['category'].map(means)\n```"}
{"prompt": "How do I train a neural network with PyTorch?", "response": "```python\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, TensorDataset\n\n# Define model\nclass NeuralNet(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes):\n        super(NeuralNet, self).__init__()\n        self.fc1 = nn.Linear(input_size, hidden_size)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(0.3)\n        self.fc2 = nn.Linear(hidden_size, hidden_size)\n        self.fc3 = nn.Linear(hidden_size, num_classes)\n    \n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.relu(x)\n        x = self.fc3(x)\n        return x\n\n# Prepare data\nX_tensor = torch.FloatTensor(X_train)\ny_tensor = torch.LongTensor(y_train)\ndataset = TensorDataset(X_tensor, y_tensor)\nloader = DataLoader(dataset, batch_size=32, shuffle=True)\n\n# Initialize\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = NeuralNet(input_size=10, hidden_size=64, num_classes=3).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n\n# Training loop\nfor epoch in range(100):\n    model.train()\n    total_loss = 0\n    for batch_X, batch_y in loader:\n        batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n        \n        optimizer.zero_grad()\n        outputs = model(batch_X)\n        loss = criterion(outputs, batch_y)\n        loss.backward()\n        optimizer.step()\n        total_loss += loss.item()\n    \n    if (epoch + 1) % 10 == 0:\n        print(f'Epoch {epoch+1}, Loss: {total_loss/len(loader):.4f}')\n```"}
{"prompt": "How do I evaluate a classification model?", "response": "```python\nfrom sklearn.metrics import (\n    accuracy_score, precision_score, recall_score, f1_score,\n    confusion_matrix, classification_report, roc_auc_score,\n    roc_curve, precision_recall_curve, average_precision_score\n)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Predictions\ny_pred = model.predict(X_test)\ny_proba = model.predict_proba(X_test)[:, 1]  # For binary\n\n# Basic metrics\nprint(f'Accuracy: {accuracy_score(y_test, y_pred):.3f}')\nprint(f'Precision: {precision_score(y_test, y_pred):.3f}')\nprint(f'Recall: {recall_score(y_test, y_pred):.3f}')\nprint(f'F1 Score: {f1_score(y_test, y_pred):.3f}')\nprint(f'ROC-AUC: {roc_auc_score(y_test, y_proba):.3f}')\n\n# Classification report\nprint(classification_report(y_test, y_pred))\n\n# Confusion matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n# ROC Curve\nfpr, tpr, _ = roc_curve(y_test, y_proba)\nplt.figure(figsize=(8, 6))\nplt.plot(fpr, tpr, label=f'ROC (AUC = {roc_auc_score(y_test, y_proba):.3f})')\nplt.plot([0, 1], [0, 1], 'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.legend()\nplt.show()\n```"}
{"prompt": "How do I perform hyperparameter tuning?", "response": "```python\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import randint, uniform\nimport optuna\n\n# Grid Search\nparam_grid = {\n    'n_estimators': [100, 200, 300],\n    'max_depth': [5, 10, 15, None],\n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4]\n}\n\ngrid_search = GridSearchCV(\n    RandomForestClassifier(random_state=42),\n    param_grid, cv=5, scoring='f1', n_jobs=-1, verbose=1\n)\ngrid_search.fit(X_train, y_train)\nprint(f'Best params: {grid_search.best_params_}')\nprint(f'Best score: {grid_search.best_score_:.3f}')\n\n# Randomized Search (faster for large param spaces)\nparam_dist = {\n    'n_estimators': randint(100, 500),\n    'max_depth': randint(5, 30),\n    'min_samples_split': randint(2, 20),\n    'learning_rate': uniform(0.01, 0.3)\n}\n\nrandom_search = RandomizedSearchCV(\n    model, param_dist, n_iter=100, cv=5, scoring='f1', n_jobs=-1\n)\nrandom_search.fit(X_train, y_train)\n\n# Optuna (Bayesian optimization)\ndef objective(trial):\n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n        'max_depth': trial.suggest_int('max_depth', 5, 30),\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True)\n    }\n    model = XGBClassifier(**params, random_state=42)\n    score = cross_val_score(model, X_train, y_train, cv=5, scoring='f1').mean()\n    return score\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=100)\nprint(f'Best params: {study.best_params}')\n```"}
{"prompt": "How do I handle imbalanced datasets?", "response": "```python\nfrom imblearn.over_sampling import SMOTE, ADASYN, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom imblearn.combine import SMOTETomek\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Check class distribution\nprint(y_train.value_counts())\nprint(y_train.value_counts(normalize=True))\n\n# Oversampling with SMOTE\nsmote = SMOTE(random_state=42)\nX_resampled, y_resampled = smote.fit_resample(X_train, y_train)\n\n# Undersampling\nrus = RandomUnderSampler(random_state=42)\nX_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n\n# Combined approach\nsmt = SMOTETomek(random_state=42)\nX_resampled, y_resampled = smt.fit_resample(X_train, y_train)\n\n# Pipeline with resampling\npipeline = ImbPipeline([\n    ('smote', SMOTE(random_state=42)),\n    ('classifier', RandomForestClassifier(random_state=42))\n])\npipeline.fit(X_train, y_train)\n\n# Class weights (built-in for many models)\nmodel = RandomForestClassifier(class_weight='balanced', random_state=42)\nmodel.fit(X_train, y_train)\n\n# Custom class weights\nfrom sklearn.utils.class_weight import compute_class_weight\nweights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\nclass_weights = dict(zip(np.unique(y_train), weights))\nmodel = RandomForestClassifier(class_weight=class_weights)\n```"}
{"prompt": "How do I create visualizations for data analysis?", "response": "```python\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\n# Set style\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (10, 6)\n\n# Distribution plots\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\nsns.histplot(df['numeric_col'], kde=True, ax=axes[0])\nsns.boxplot(x='category', y='value', data=df, ax=axes[1])\nplt.tight_layout()\nplt.show()\n\n# Correlation heatmap\nplt.figure(figsize=(12, 8))\nsns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0)\nplt.title('Correlation Matrix')\nplt.show()\n\n# Pairplot for relationships\nsns.pairplot(df, hue='target', diag_kind='kde')\nplt.show()\n\n# Time series\nplt.figure(figsize=(14, 6))\nplt.plot(df['date'], df['value'], label='Actual')\nplt.plot(df['date'], df['rolling_mean'], label='7-day MA')\nplt.fill_between(df['date'], df['lower'], df['upper'], alpha=0.3)\nplt.legend()\nplt.title('Time Series with Confidence Interval')\nplt.show()\n\n# Interactive with Plotly\nfig = px.scatter(df, x='feature1', y='feature2', color='category',\n                 size='value', hover_data=['name'])\nfig.show()\n\n# Feature importance\nimportances = model.feature_importances_\nfeature_imp = pd.DataFrame({'feature': features, 'importance': importances})\nfeature_imp = feature_imp.sort_values('importance', ascending=False).head(20)\nsns.barplot(x='importance', y='feature', data=feature_imp)\nplt.title('Top 20 Feature Importances')\nplt.show()\n```"}
{"prompt": "How do I perform time series forecasting?", "response": "```python\nimport pandas as pd\nimport numpy as np\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom prophet import Prophet\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# Prepare time series data\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date').sort_index()\n\n# Train/test split (temporal)\ntrain = df[:'2023-12-31']\ntest = df['2024-01-01':]\n\n# ARIMA\nmodel = ARIMA(train['value'], order=(2, 1, 2))\nresults = model.fit()\nforecast = results.forecast(steps=len(test))\n\n# SARIMA (with seasonality)\nmodel = SARIMAX(train['value'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))\nresults = model.fit()\nforecast = results.forecast(steps=len(test))\n\n# Prophet\ndf_prophet = df.reset_index().rename(columns={'date': 'ds', 'value': 'y'})\nmodel = Prophet(yearly_seasonality=True, weekly_seasonality=True)\nmodel.fit(df_prophet[df_prophet['ds'] <= '2023-12-31'])\nfuture = model.make_future_dataframe(periods=len(test))\nforecast = model.predict(future)\n\n# Evaluation\nmae = mean_absolute_error(test['value'], forecast)\nrmse = np.sqrt(mean_squared_error(test['value'], forecast))\nmape = np.mean(np.abs((test['value'] - forecast) / test['value'])) * 100\nprint(f'MAE: {mae:.2f}, RMSE: {rmse:.2f}, MAPE: {mape:.2f}%')\n\n# Feature engineering for ML approach\ndf['lag_1'] = df['value'].shift(1)\ndf['lag_7'] = df['value'].shift(7)\ndf['rolling_mean_7'] = df['value'].rolling(7).mean()\ndf['rolling_std_7'] = df['value'].rolling(7).std()\n```"}
{"prompt": "How do I deploy a machine learning model with MLflow?", "response": "```python\nimport mlflow\nimport mlflow.sklearn\nfrom mlflow.tracking import MlflowClient\n\n# Set tracking URI\nmlflow.set_tracking_uri('http://localhost:5000')\nmlflow.set_experiment('my_experiment')\n\n# Log experiment\nwith mlflow.start_run(run_name='rf_baseline'):\n    # Log parameters\n    mlflow.log_param('n_estimators', 100)\n    mlflow.log_param('max_depth', 10)\n    \n    # Train model\n    model = RandomForestClassifier(n_estimators=100, max_depth=10)\n    model.fit(X_train, y_train)\n    \n    # Log metrics\n    y_pred = model.predict(X_test)\n    mlflow.log_metric('accuracy', accuracy_score(y_test, y_pred))\n    mlflow.log_metric('f1', f1_score(y_test, y_pred))\n    \n    # Log model\n    mlflow.sklearn.log_model(model, 'model')\n    \n    # Log artifacts\n    mlflow.log_artifact('feature_importance.png')\n\n# Register model\nresult = mlflow.register_model(\n    'runs:/run_id/model',\n    'my_model'\n)\n\n# Transition to production\nclient = MlflowClient()\nclient.transition_model_version_stage(\n    name='my_model',\n    version=1,\n    stage='Production'\n)\n\n# Load and serve\nmodel = mlflow.sklearn.load_model('models:/my_model/Production')\n\n# Serve with REST API\n# mlflow models serve -m models:/my_model/Production -p 5001\n```"}
{"prompt": "How do I perform feature selection?", "response": "```python\nfrom sklearn.feature_selection import (\n    SelectKBest, f_classif, mutual_info_classif,\n    RFE, SelectFromModel, VarianceThreshold\n)\nfrom sklearn.ensemble import RandomForestClassifier\nimport numpy as np\n\n# Remove low variance features\nselector = VarianceThreshold(threshold=0.01)\nX_selected = selector.fit_transform(X)\nprint(f'Features kept: {selector.get_support().sum()}')\n\n# Univariate selection (ANOVA F-test)\nselector = SelectKBest(f_classif, k=20)\nX_selected = selector.fit_transform(X, y)\nscores = pd.DataFrame({'feature': features, 'score': selector.scores_})\nprint(scores.sort_values('score', ascending=False).head(20))\n\n# Mutual information\nselector = SelectKBest(mutual_info_classif, k=20)\nX_selected = selector.fit_transform(X, y)\n\n# Recursive Feature Elimination\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\nrfe = RFE(model, n_features_to_select=20, step=5)\nrfe.fit(X, y)\nselected_features = [f for f, s in zip(features, rfe.support_) if s]\nprint(f'Selected features: {selected_features}')\n\n# Model-based selection\nselector = SelectFromModel(RandomForestClassifier(n_estimators=100), threshold='median')\nX_selected = selector.fit_transform(X, y)\n\n# Correlation-based removal\ncorr_matrix = df.corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\nto_drop = [col for col in upper.columns if any(upper[col] > 0.95)]\ndf_reduced = df.drop(columns=to_drop)\n```"}
{"prompt": "How do I perform dimensionality reduction with PCA?", "response": "```python\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Scale data first (important for PCA)\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Fit PCA\npca = PCA(n_components=0.95)  # Keep 95% variance\nX_pca = pca.fit_transform(X_scaled)\nprint(f'Components: {pca.n_components_}')\nprint(f'Explained variance: {pca.explained_variance_ratio_.sum():.3f}')\n\n# Scree plot\npca_full = PCA()\npca_full.fit(X_scaled)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         pca_full.explained_variance_ratio_, 'bo-')\nplt.xlabel('Component')\nplt.ylabel('Explained Variance Ratio')\nplt.title('Scree Plot')\n\nplt.subplot(1, 2, 2)\nplt.plot(range(1, len(pca_full.explained_variance_ratio_) + 1),\n         np.cumsum(pca_full.explained_variance_ratio_), 'ro-')\nplt.axhline(y=0.95, color='k', linestyle='--')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance')\nplt.title('Cumulative Variance')\nplt.tight_layout()\nplt.show()\n\n# 2D visualization\npca_2d = PCA(n_components=2)\nX_2d = pca_2d.fit_transform(X_scaled)\n\nplt.figure(figsize=(10, 8))\nscatter = plt.scatter(X_2d[:, 0], X_2d[:, 1], c=y, cmap='viridis', alpha=0.6)\nplt.colorbar(scatter)\nplt.xlabel('PC1')\nplt.ylabel('PC2')\nplt.title('PCA Visualization')\nplt.show()\n\n# Feature loadings\nloadings = pd.DataFrame(\n    pca.components_.T,\n    columns=[f'PC{i+1}' for i in range(pca.n_components_)],\n    index=features\n)\nprint(loadings.head(10))\n```"}
{"prompt": "How do I perform clustering analysis?", "response": "```python\nfrom sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\nfrom sklearn.metrics import silhouette_score, calinski_harabasz_score\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\n# Scale data\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# Elbow method for K-Means\ninertias = []\nsilhouettes = []\nK_range = range(2, 11)\n\nfor k in K_range:\n    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n    kmeans.fit(X_scaled)\n    inertias.append(kmeans.inertia_)\n    silhouettes.append(silhouette_score(X_scaled, kmeans.labels_))\n\nplt.figure(figsize=(12, 5))\nplt.subplot(1, 2, 1)\nplt.plot(K_range, inertias, 'bo-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Inertia')\nplt.title('Elbow Method')\n\nplt.subplot(1, 2, 2)\nplt.plot(K_range, silhouettes, 'go-')\nplt.xlabel('Number of Clusters')\nplt.ylabel('Silhouette Score')\nplt.title('Silhouette Analysis')\nplt.tight_layout()\nplt.show()\n\n# Final K-Means\nkmeans = KMeans(n_clusters=4, random_state=42, n_init=10)\nclusters = kmeans.fit_predict(X_scaled)\n\n# DBSCAN (density-based)\ndbscan = DBSCAN(eps=0.5, min_samples=5)\nclusters = dbscan.fit_predict(X_scaled)\nprint(f'Clusters found: {len(set(clusters)) - (1 if -1 in clusters else 0)}')\nprint(f'Noise points: {(clusters == -1).sum()}')\n\n# Hierarchical clustering\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nZ = linkage(X_scaled, method='ward')\nplt.figure(figsize=(12, 6))\ndendrogram(Z, truncate_mode='level', p=5)\nplt.title('Hierarchical Clustering Dendrogram')\nplt.show()\n\n# Cluster profiling\ndf['cluster'] = clusters\ncluster_profile = df.groupby('cluster').mean()\nprint(cluster_profile)\n```"}
{"prompt": "How do I build an XGBoost model?", "response": "```python\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Prepare data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Create DMatrix (XGBoost's optimized data structure)\ndtrain = xgb.DMatrix(X_train, label=y_train)\ndtest = xgb.DMatrix(X_test, label=y_test)\n\n# Parameters\nparams = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'max_depth': 6,\n    'learning_rate': 0.1,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'min_child_weight': 1,\n    'reg_alpha': 0.1,\n    'reg_lambda': 1.0,\n    'seed': 42\n}\n\n# Train with early stopping\nevals = [(dtrain, 'train'), (dtest, 'eval')]\nmodel = xgb.train(\n    params, dtrain,\n    num_boost_round=1000,\n    evals=evals,\n    early_stopping_rounds=50,\n    verbose_eval=100\n)\n\n# Predictions\ny_proba = model.predict(dtest)\ny_pred = (y_proba > 0.5).astype(int)\nprint(classification_report(y_test, y_pred))\n\n# Feature importance\nxgb.plot_importance(model, max_num_features=20, importance_type='gain')\nplt.show()\n\n# Scikit-learn API\nmodel = xgb.XGBClassifier(\n    n_estimators=100,\n    max_depth=6,\n    learning_rate=0.1,\n    use_label_encoder=False,\n    eval_metric='logloss'\n)\nmodel.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n```"}
{"prompt": "How do I implement cross-validation properly?", "response": "```python\nfrom sklearn.model_selection import (\n    KFold, StratifiedKFold, TimeSeriesSplit, LeaveOneOut,\n    cross_val_score, cross_validate\n)\nimport numpy as np\n\n# Standard K-Fold\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=kfold, scoring='accuracy')\nprint(f'Accuracy: {scores.mean():.3f} (+/- {scores.std()*2:.3f})')\n\n# Stratified K-Fold (maintains class distribution)\nskfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nscores = cross_val_score(model, X, y, cv=skfold, scoring='f1')\n\n# Time Series Split (for temporal data)\ntscv = TimeSeriesSplit(n_splits=5)\nfor train_idx, test_idx in tscv.split(X):\n    X_train, X_test = X[train_idx], X[test_idx]\n    y_train, y_test = y[train_idx], y[test_idx]\n    model.fit(X_train, y_train)\n    print(f'Test size: {len(test_idx)}, Score: {model.score(X_test, y_test):.3f}')\n\n# Multiple metrics\nscoring = ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']\nresults = cross_validate(model, X, y, cv=5, scoring=scoring, return_train_score=True)\n\nfor metric in scoring:\n    train_score = results[f'train_{metric}'].mean()\n    test_score = results[f'test_{metric}'].mean()\n    print(f'{metric}: Train={train_score:.3f}, Test={test_score:.3f}')\n\n# Nested cross-validation (for hyperparameter tuning)\nfrom sklearn.model_selection import GridSearchCV\n\nouter_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ninner_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n\nparam_grid = {'max_depth': [3, 5, 7], 'n_estimators': [50, 100, 200]}\ngrid_search = GridSearchCV(model, param_grid, cv=inner_cv, scoring='f1')\nnested_scores = cross_val_score(grid_search, X, y, cv=outer_cv, scoring='f1')\nprint(f'Nested CV Score: {nested_scores.mean():.3f} (+/- {nested_scores.std()*2:.3f})')\n```"}
{"prompt": "How do I build a text classification model with NLP?", "response": "```python\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\n\n# Text preprocessing\ndef preprocess_text(text):\n    # Lowercase\n    text = text.lower()\n    # Remove special characters\n    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n    # Tokenize\n    tokens = text.split()\n    # Remove stopwords\n    stop_words = set(stopwords.words('english'))\n    tokens = [t for t in tokens if t not in stop_words]\n    # Lemmatize\n    lemmatizer = WordNetLemmatizer()\n    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n    return ' '.join(tokens)\n\ndf['clean_text'] = df['text'].apply(preprocess_text)\n\n# TF-IDF Pipeline\npipeline = Pipeline([\n    ('tfidf', TfidfVectorizer(max_features=10000, ngram_range=(1, 2))),\n    ('classifier', LogisticRegression(max_iter=1000))\n])\n\npipeline.fit(X_train['clean_text'], y_train)\ny_pred = pipeline.predict(X_test['clean_text'])\nprint(classification_report(y_test, y_pred))\n\n# With transformers (BERT)\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom transformers import Trainer, TrainingArguments\nimport torch\n\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n\n# Tokenize\nencodings = tokenizer(texts, truncation=True, padding=True, max_length=512)\n\n# Fine-tune\ntraining_args = TrainingArguments(\n    output_dir='./results',\n    num_train_epochs=3,\n    per_device_train_batch_size=16,\n    evaluation_strategy='epoch'\n)\ntrainer = Trainer(model=model, args=training_args, train_dataset=train_dataset)\ntrainer.train()\n```"}
{"prompt": "How do I handle outliers in data?", "response": "```python\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\n\n# Detection methods\n\n# 1. IQR method\nQ1 = df['col'].quantile(0.25)\nQ3 = df['col'].quantile(0.75)\nIQR = Q3 - Q1\nlower_bound = Q1 - 1.5 * IQR\nupper_bound = Q3 + 1.5 * IQR\noutliers = df[(df['col'] < lower_bound) | (df['col'] > upper_bound)]\nprint(f'Outliers: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)')\n\n# 2. Z-score method\nz_scores = np.abs(stats.zscore(df['col']))\noutliers = df[z_scores > 3]\n\n# 3. Modified Z-score (robust to outliers)\nmedian = df['col'].median()\nmad = np.median(np.abs(df['col'] - median))\nmodified_z = 0.6745 * (df['col'] - median) / mad\noutliers = df[np.abs(modified_z) > 3.5]\n\n# Treatment methods\n\n# 1. Remove outliers\ndf_clean = df[(df['col'] >= lower_bound) & (df['col'] <= upper_bound)]\n\n# 2. Cap/Winsorize\ndf['col_capped'] = df['col'].clip(lower=lower_bound, upper=upper_bound)\n\n# 3. Transform\ndf['col_log'] = np.log1p(df['col'])\n\n# 4. Impute with median\ndf.loc[z_scores > 3, 'col'] = df['col'].median()\n\n# 5. Isolation Forest (for multivariate)\nfrom sklearn.ensemble import IsolationForest\niso = IsolationForest(contamination=0.1, random_state=42)\ndf['outlier'] = iso.fit_predict(df[numeric_cols])\ndf_clean = df[df['outlier'] == 1]\n\n# Visualization\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(1, 2, figsize=(12, 5))\ndf['col'].plot.box(ax=axes[0], title='Box Plot')\ndf['col'].plot.hist(bins=50, ax=axes[1], title='Distribution')\nplt.show()\n```"}
{"prompt": "How do I save and load machine learning models?", "response": "```python\nimport joblib\nimport pickle\nimport json\n\n# Joblib (recommended for sklearn)\njoblib.dump(model, 'model.joblib')\nmodel = joblib.load('model.joblib')\n\n# Pickle\nwith open('model.pkl', 'wb') as f:\n    pickle.dump(model, f)\n\nwith open('model.pkl', 'rb') as f:\n    model = pickle.load(f)\n\n# Save entire pipeline with preprocessing\njoblib.dump({\n    'model': model,\n    'scaler': scaler,\n    'feature_names': feature_names,\n    'label_encoder': label_encoder\n}, 'pipeline.joblib')\n\n# PyTorch\ntorch.save(model.state_dict(), 'model.pth')\nmodel.load_state_dict(torch.load('model.pth'))\n\n# Save full model\ntorch.save(model, 'model_full.pth')\nmodel = torch.load('model_full.pth')\n\n# TensorFlow/Keras\nmodel.save('model.h5')\nmodel = tf.keras.models.load_model('model.h5')\n\n# SavedModel format\nmodel.save('saved_model/')\nmodel = tf.keras.models.load_model('saved_model/')\n\n# ONNX (cross-platform)\nimport onnx\nfrom skl2onnx import convert_sklearn\nfrom skl2onnx.common.data_types import FloatTensorType\n\ninitial_type = [('float_input', FloatTensorType([None, X.shape[1]]))]\nonnx_model = convert_sklearn(model, initial_types=initial_type)\nonnx.save_model(onnx_model, 'model.onnx')\n\n# Save model metadata\nmetadata = {\n    'model_type': 'RandomForestClassifier',\n    'features': feature_names,\n    'target': 'label',\n    'metrics': {'accuracy': 0.95, 'f1': 0.93},\n    'trained_date': '2024-01-15'\n}\nwith open('model_metadata.json', 'w') as f:\n    json.dump(metadata, f, indent=2)\n```"}
{"prompt": "How do I perform A/B testing analysis?", "response": "```python\nimport numpy as np\nfrom scipy import stats\nimport pandas as pd\n\n# Sample data\ncontrol = df[df['group'] == 'control']['conversion']\ntreatment = df[df['group'] == 'treatment']['conversion']\n\n# Basic statistics\nprint(f'Control: n={len(control)}, conversion={control.mean():.4f}')\nprint(f'Treatment: n={len(treatment)}, conversion={treatment.mean():.4f}')\nprint(f'Lift: {(treatment.mean() - control.mean()) / control.mean() * 100:.2f}%')\n\n# Chi-square test for proportions\ncontingency = pd.crosstab(df['group'], df['converted'])\nchi2, p_value, dof, expected = stats.chi2_contingency(contingency)\nprint(f'Chi-square: {chi2:.4f}, p-value: {p_value:.4f}')\n\n# Z-test for proportions\nfrom statsmodels.stats.proportion import proportions_ztest\n\nsuccesses = np.array([treatment.sum(), control.sum()])\nnobs = np.array([len(treatment), len(control)])\n\nz_stat, p_value = proportions_ztest(successes, nobs, alternative='larger')\nprint(f'Z-statistic: {z_stat:.4f}, p-value: {p_value:.4f}')\n\n# T-test for continuous metrics\nt_stat, p_value = stats.ttest_ind(treatment, control)\nprint(f'T-statistic: {t_stat:.4f}, p-value: {p_value:.4f}')\n\n# Confidence interval for difference\nfrom statsmodels.stats.proportion import confint_proportions_2indep\n\ndiff = treatment.mean() - control.mean()\nci_low, ci_high = confint_proportions_2indep(\n    treatment.sum(), len(treatment),\n    control.sum(), len(control),\n    method='wald'\n)\nprint(f'Difference: {diff:.4f} (95% CI: [{ci_low:.4f}, {ci_high:.4f}])')\n\n# Sample size calculation\nfrom statsmodels.stats.power import TTestIndPower\n\neffect_size = 0.2  # Cohen's d\nalpha = 0.05\npower = 0.8\n\nanalysis = TTestIndPower()\nsample_size = analysis.solve_power(effect_size, alpha=alpha, power=power)\nprint(f'Required sample size per group: {int(np.ceil(sample_size))}')\n```"}
{"prompt": "How do I implement model explainability with SHAP?", "response": "```python\nimport shap\nimport matplotlib.pyplot as plt\n\n# Initialize explainer\nexplainer = shap.TreeExplainer(model)  # For tree-based models\n# explainer = shap.KernelExplainer(model.predict, X_train[:100])  # For any model\n\n# Calculate SHAP values\nshap_values = explainer.shap_values(X_test)\n\n# Summary plot (feature importance)\nplt.figure(figsize=(10, 8))\nshap.summary_plot(shap_values, X_test, feature_names=feature_names)\nplt.show()\n\n# Bar plot (mean absolute SHAP values)\nshap.summary_plot(shap_values, X_test, plot_type='bar', feature_names=feature_names)\nplt.show()\n\n# Single prediction explanation\nidx = 0\nshap.force_plot(explainer.expected_value, shap_values[idx], X_test.iloc[idx],\n                feature_names=feature_names, matplotlib=True)\nplt.show()\n\n# Waterfall plot\nshap.waterfall_plot(shap.Explanation(\n    values=shap_values[idx],\n    base_values=explainer.expected_value,\n    data=X_test.iloc[idx],\n    feature_names=feature_names\n))\n\n# Dependence plot (feature interaction)\nshap.dependence_plot('feature_name', shap_values, X_test, \n                     interaction_index='auto', feature_names=feature_names)\n\n# SHAP for deep learning\nfrom shap import DeepExplainer\nexplainer = shap.DeepExplainer(model, X_train[:100])\nshap_values = explainer.shap_values(X_test[:10])\n\n# Global feature importance\nimportance = np.abs(shap_values).mean(axis=0)\nfeature_importance = pd.DataFrame({\n    'feature': feature_names,\n    'importance': importance\n}).sort_values('importance', ascending=False)\nprint(feature_importance.head(10))\n```"}
{"prompt": "How do I build a recommendation system?", "response": "```python\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\n\n# Content-based filtering\ndef content_based_recommendations(item_id, item_features, top_n=10):\n    # Compute similarity matrix\n    similarity_matrix = cosine_similarity(item_features)\n    \n    # Get similar items\n    item_idx = item_id_to_idx[item_id]\n    similar_scores = list(enumerate(similarity_matrix[item_idx]))\n    similar_scores = sorted(similar_scores, key=lambda x: x[1], reverse=True)\n    \n    # Return top N (excluding self)\n    similar_items = [idx_to_item_id[i] for i, score in similar_scores[1:top_n+1]]\n    return similar_items\n\n# Collaborative filtering (user-item matrix)\nuser_item_matrix = ratings.pivot(index='user_id', columns='item_id', values='rating')\nuser_item_matrix = user_item_matrix.fillna(0)\nsparse_matrix = csr_matrix(user_item_matrix.values)\n\n# KNN-based\nmodel_knn = NearestNeighbors(metric='cosine', algorithm='brute')\nmodel_knn.fit(sparse_matrix)\n\ndef collaborative_recommendations(user_id, top_n=10):\n    user_idx = user_id_to_idx[user_id]\n    distances, indices = model_knn.kneighbors(\n        user_item_matrix.iloc[user_idx].values.reshape(1, -1), \n        n_neighbors=10\n    )\n    \n    # Get items from similar users\n    similar_users = indices.flatten()[1:]\n    recommendations = []\n    \n    for similar_user in similar_users:\n        user_items = user_item_matrix.iloc[similar_user]\n        liked_items = user_items[user_items > 3].index.tolist()\n        recommendations.extend(liked_items)\n    \n    # Remove already rated items\n    user_rated = user_item_matrix.iloc[user_idx]\n    recommendations = [item for item in recommendations if user_rated[item] == 0]\n    \n    return list(set(recommendations))[:top_n]\n\n# Matrix Factorization (SVD)\nfrom surprise import SVD, Dataset, Reader\nfrom surprise.model_selection import cross_validate\n\nreader = Reader(rating_scale=(1, 5))\ndata = Dataset.load_from_df(ratings[['user_id', 'item_id', 'rating']], reader)\n\nsvd = SVD(n_factors=100, n_epochs=20, lr_all=0.005, reg_all=0.02)\ncross_validate(svd, data, measures=['RMSE', 'MAE'], cv=5, verbose=True)\n\ntrainset = data.build_full_trainset()\nsvd.fit(trainset)\n\n# Predict\nprediction = svd.predict(user_id=1, item_id=100)\nprint(f'Predicted rating: {prediction.est:.2f}')\n```"}
{"prompt": "How do I perform data augmentation for images?", "response": "```python\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport albumentations as A\nfrom PIL import Image\nimport numpy as np\n\n# Keras ImageDataGenerator\ndatagen = ImageDataGenerator(\n    rotation_range=20,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    fill_mode='nearest'\n)\n\n# Generate augmented images\nfor batch in datagen.flow(x_train, y_train, batch_size=32):\n    # Use augmented batch\n    break\n\n# Albumentations (more advanced)\ntransform = A.Compose([\n    A.RandomRotate90(),\n    A.Flip(),\n    A.Transpose(),\n    A.OneOf([\n        A.GaussNoise(),\n        A.GaussianBlur(blur_limit=3),\n        A.MotionBlur(blur_limit=3),\n    ], p=0.3),\n    A.OneOf([\n        A.OpticalDistortion(),\n        A.GridDistortion(),\n        A.PiecewiseAffine(),\n    ], p=0.3),\n    A.OneOf([\n        A.CLAHE(),\n        A.Sharpen(),\n        A.Emboss(),\n        A.RandomBrightnessContrast(),\n    ], p=0.3),\n    A.HueSaturationValue(p=0.3),\n    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n])\n\n# Apply transformation\nimage = np.array(Image.open('image.jpg'))\naugmented = transform(image=image)\naugmented_image = augmented['image']\n\n# PyTorch transforms\nfrom torchvision import transforms\n\ntrain_transform = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n    transforms.RandomRotation(15),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntest_transform = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n```"}
{"prompt": "How do I create a data pipeline with Apache Spark?", "response": "```python\nfrom pyspark.sql import SparkSession\nfrom pyspark.sql.functions import col, when, lit, udf, explode, split\nfrom pyspark.sql.types import StringType, IntegerType, StructType, StructField\nfrom pyspark.ml.feature import VectorAssembler, StandardScaler, StringIndexer\nfrom pyspark.ml.classification import RandomForestClassifier\nfrom pyspark.ml import Pipeline\n\n# Initialize Spark\nspark = SparkSession.builder \\\n    .appName('DataPipeline') \\\n    .config('spark.sql.adaptive.enabled', 'true') \\\n    .getOrCreate()\n\n# Read data\ndf = spark.read.csv('data.csv', header=True, inferSchema=True)\ndf = spark.read.parquet('data.parquet')\ndf = spark.read.json('data.json')\n\n# Basic operations\ndf.show(5)\ndf.printSchema()\ndf.describe().show()\n\n# Transformations\ndf_clean = df \\\n    .filter(col('age') > 0) \\\n    .withColumn('age_group', when(col('age') < 30, 'young')\n                              .when(col('age') < 50, 'middle')\n                              .otherwise('senior')) \\\n    .withColumn('income_log', log(col('income') + 1)) \\\n    .drop('unnecessary_column') \\\n    .dropna(subset=['important_col']) \\\n    .fillna({'category': 'unknown', 'value': 0})\n\n# Aggregations\nstats = df.groupBy('category').agg(\n    count('*').alias('count'),\n    avg('value').alias('avg_value'),\n    sum('amount').alias('total_amount')\n)\n\n# ML Pipeline\nstring_indexer = StringIndexer(inputCol='category', outputCol='category_idx')\nassembler = VectorAssembler(inputCols=['feature1', 'feature2'], outputCol='features')\nscaler = StandardScaler(inputCols='features', outputCol='scaled_features')\nrf = RandomForestClassifier(featuresCol='scaled_features', labelCol='label')\n\npipeline = Pipeline(stages=[string_indexer, assembler, scaler, rf])\nmodel = pipeline.fit(train_df)\npredictions = model.transform(test_df)\n\n# Write output\ndf_clean.write.parquet('output.parquet', mode='overwrite')\ndf_clean.write.partitionBy('date').parquet('partitioned_output/')\n\nspark.stop()\n```"}
{"prompt": "How do I implement model monitoring in production?", "response": "```python\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom evidently.report import Report\nfrom evidently.metric_preset import DataDriftPreset, TargetDriftPreset\nimport mlflow\n\n# Track prediction distributions\nclass ModelMonitor:\n    def __init__(self, reference_data, model_name):\n        self.reference = reference_data\n        self.model_name = model_name\n        self.predictions = []\n        self.actuals = []\n        \n    def log_prediction(self, features, prediction, actual=None):\n        self.predictions.append({\n            'timestamp': pd.Timestamp.now(),\n            'features': features,\n            'prediction': prediction,\n            'actual': actual\n        })\n    \n    def check_data_drift(self, current_data, threshold=0.05):\n        \"\"\"Kolmogorov-Smirnov test for feature drift\"\"\"\n        drift_detected = {}\n        for col in self.reference.columns:\n            if self.reference[col].dtype in ['int64', 'float64']:\n                stat, p_value = stats.ks_2samp(\n                    self.reference[col], \n                    current_data[col]\n                )\n                drift_detected[col] = {\n                    'statistic': stat,\n                    'p_value': p_value,\n                    'drift': p_value < threshold\n                }\n        return drift_detected\n    \n    def check_prediction_drift(self, window_size=1000):\n        \"\"\"Monitor prediction distribution changes\"\"\"\n        recent = pd.DataFrame(self.predictions[-window_size:])\n        historical = pd.DataFrame(self.predictions[:-window_size])\n        \n        stat, p_value = stats.ks_2samp(\n            historical['prediction'],\n            recent['prediction']\n        )\n        return {'statistic': stat, 'p_value': p_value}\n    \n    def calculate_metrics(self):\n        \"\"\"Calculate performance metrics on labeled data\"\"\"\n        labeled = [p for p in self.predictions if p['actual'] is not None]\n        if not labeled:\n            return None\n        \n        df = pd.DataFrame(labeled)\n        from sklearn.metrics import accuracy_score, f1_score\n        return {\n            'accuracy': accuracy_score(df['actual'], df['prediction']),\n            'f1': f1_score(df['actual'], df['prediction'], average='weighted')\n        }\n\n# Evidently for drift reports\nreport = Report(metrics=[DataDriftPreset(), TargetDriftPreset()])\nreport.run(reference_data=reference_df, current_data=current_df)\nreport.save_html('drift_report.html')\n\n# Alert on drift\ndef send_alert(message):\n    # Slack/Email/PagerDuty integration\n    print(f'ALERT: {message}')\n\nmonitor = ModelMonitor(reference_df, 'my_model')\ndrift = monitor.check_data_drift(current_df)\nif any(d['drift'] for d in drift.values()):\n    send_alert('Data drift detected!')\n```"}
