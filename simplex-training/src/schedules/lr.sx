// Learnable Learning Rate Schedule
//
// Learning rate schedule with meta-gradient support. All parameters
// are dual numbers, allowing the schedule to be optimized via gradients.

use simplex_std::dual;

/// Learnable LR schedule with meta-gradient support
pub struct LearnableLRSchedule {
    // Core parameters (all dual for gradient tracking)
    pub initial_lr: dual,        // Starting learning rate
    pub decay_rate: dual,        // Exponential decay rate
    pub warmup_steps: dual,      // Warmup duration
    pub min_lr: dual,            // Floor learning rate

    // Oscillation for escaping plateaus
    pub oscillation_amp: dual,   // Amplitude of oscillation
    pub oscillation_freq: dual,  // Frequency of oscillation

    // Plateau detection
    pub plateau_threshold: dual, // Stagnation threshold
    pub plateau_boost: dual,     // LR boost when stuck
}

impl LearnableLRSchedule {
    /// Create schedule with default initial values
    pub fn new() -> Self {
        LearnableLRSchedule {
            initial_lr: dual::variable(2e-4),
            decay_rate: dual::variable(0.01),
            warmup_steps: dual::variable(100.0),
            min_lr: dual::constant(1e-6),
            oscillation_amp: dual::variable(0.0),
            oscillation_freq: dual::variable(0.1),
            plateau_threshold: dual::variable(50.0),
            plateau_boost: dual::variable(0.1),
        }
    }

    /// Compute learning rate at given step
    pub fn learning_rate(&self, step: dual, loss_history: &[dual]) -> dual {
        // Warmup phase
        let warmup_factor = (step / self.warmup_steps).min(dual::constant(1.0));

        // Decay phase
        let decay = (-self.decay_rate * step).exp();

        // Oscillation (helps escape local minima)
        let osc = dual::constant(1.0) +
                  self.oscillation_amp * (self.oscillation_freq * step).sin();

        // Plateau boost (increase LR when stuck)
        let stagnation = self.compute_stagnation(loss_history);
        let plateau_factor = dual::constant(1.0) +
            self.plateau_boost * ((stagnation - self.plateau_threshold) /
                                  dual::constant(10.0)).sigmoid();

        // Combined LR
        let lr = self.initial_lr * warmup_factor * decay * osc * plateau_factor;
        lr.max(self.min_lr)
    }

    /// Extract gradient for meta-update
    pub fn gradient(&self) -> LRGradient {
        LRGradient {
            d_initial_lr: self.initial_lr.der,
            d_decay_rate: self.decay_rate.der,
            d_warmup_steps: self.warmup_steps.der,
            d_oscillation_amp: self.oscillation_amp.der,
            d_oscillation_freq: self.oscillation_freq.der,
            d_plateau_threshold: self.plateau_threshold.der,
            d_plateau_boost: self.plateau_boost.der,
        }
    }

    /// Apply meta-gradient update
    pub fn update(&mut self, grad: LRGradient, learning_rate: f64) {
        self.initial_lr = dual::variable(
            (self.initial_lr.val - learning_rate * grad.d_initial_lr).max(1e-6)
        );
        self.decay_rate = dual::variable(
            (self.decay_rate.val - learning_rate * grad.d_decay_rate).max(0.0)
        );
        self.warmup_steps = dual::variable(
            (self.warmup_steps.val - learning_rate * grad.d_warmup_steps).max(0.0)
        );
        self.oscillation_amp = dual::variable(
            (self.oscillation_amp.val - learning_rate * grad.d_oscillation_amp).clamp(0.0, 0.5)
        );
        self.oscillation_freq = dual::variable(
            (self.oscillation_freq.val - learning_rate * grad.d_oscillation_freq).max(0.0)
        );
        self.plateau_threshold = dual::variable(
            (self.plateau_threshold.val - learning_rate * grad.d_plateau_threshold).max(1.0)
        );
        self.plateau_boost = dual::variable(
            (self.plateau_boost.val - learning_rate * grad.d_plateau_boost).clamp(0.0, 1.0)
        );
    }

    // Internal: compute stagnation from loss history
    fn compute_stagnation(&self, loss_history: &[dual]) -> dual {
        if loss_history.len() < 2 {
            return dual::constant(0.0);
        }

        // Count steps since significant improvement
        let threshold = dual::constant(0.001);
        var stagnation = dual::constant(0.0);

        for i in 1..loss_history.len() {
            let improvement = loss_history[i - 1] - loss_history[i];
            if improvement.val < threshold.val {
                stagnation = stagnation + dual::constant(1.0);
            } else {
                stagnation = dual::constant(0.0);
            }
        }

        stagnation
    }
}

/// Gradient of LR schedule parameters
#[derive(Clone, Default)]
pub struct LRGradient {
    pub d_initial_lr: f64,
    pub d_decay_rate: f64,
    pub d_warmup_steps: f64,
    pub d_oscillation_amp: f64,
    pub d_oscillation_freq: f64,
    pub d_plateau_threshold: f64,
    pub d_plateau_boost: f64,
}
