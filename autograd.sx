// Simplex Autograd Runtime
// Eager-mode automatic differentiation for Neural IR
//
// This module provides:
// - GradValue: Values with implicit gradient tracking
// - GradTape: Eager-mode computation graph recording
// - Backward pass: Reverse-mode automatic differentiation
// - Per-gate gradient accumulation
//
// Copyright (c) 2025-2026 Rod Higgins
// Licensed under AGPL-3.0 - see LICENSE file
// https://github.com/senuamedia/simplex-lang

// Operation types for the computation graph
enum GradOp {
    // Constants and inputs (no gradient contribution)
    Constant,
    Input,

    // Arithmetic operations
    Add,
    Sub,
    Mul,
    Div,
    Neg,

    // Comparison operations (relaxed via sigmoid in training mode)
    Gt,     // >  becomes sigmoid((a - b) * temperature)
    Lt,     // <  becomes sigmoid((b - a) * temperature)
    Ge,     // >= becomes sigmoid((a - b + epsilon) * temperature)
    Le,     // <= becomes sigmoid((b - a + epsilon) * temperature)

    // Activation functions
    Sigmoid,
    Tanh,
    Relu,

    // Neural gate specific
    GumbelSoftmax,  // Differentiable categorical sampling
    SoftThreshold   // Soft threshold for belief confidence
}

// GradValue: A value with gradient tracking
// Layout:
//   value(0): f64      - the forward value
//   grad(1): f64       - accumulated gradient (updated during backward)
//   op(2): i64         - GradOp enum value
//   inputs(3): i64     - ptr to vec of input GradValue ptrs (for backprop)
//   gate_id(4): i64    - ID of the neural gate this value belongs to (0 = none)
//   tape(5): i64       - ptr to the GradTape that tracks this value
//
fn grad_value_new(value: f64, op: i64, inputs: i64, gate_id: i64, tape: i64) -> i64 {
    let gv: i64 = malloc(48);
    store_f64(gv, 0, value);
    store_f64(gv, 1, 0.0);      // Initial gradient is 0
    store_i64(gv, 2, op);
    store_ptr(gv, 3, inputs);
    store_i64(gv, 4, gate_id);
    store_ptr(gv, 5, tape);

    // Register with tape for later backprop
    if tape != 0 {
        grad_tape_record(tape, gv);
    }

    gv
}

fn grad_value_get(gv: i64) -> f64 {
    load_f64(gv, 0)
}

fn grad_value_grad(gv: i64) -> f64 {
    load_f64(gv, 1)
}

fn grad_value_set_grad(gv: i64, grad: f64) -> i64 {
    store_f64(gv, 1, grad);
    0
}

fn grad_value_add_grad(gv: i64, grad: f64) -> i64 {
    let current: f64 = load_f64(gv, 1);
    store_f64(gv, 1, current + grad);
    0
}

fn grad_value_op(gv: i64) -> i64 {
    load_i64(gv, 2)
}

fn grad_value_inputs(gv: i64) -> i64 {
    load_ptr(gv, 3)
}

fn grad_value_gate_id(gv: i64) -> i64 {
    load_i64(gv, 4)
}

fn grad_value_tape(gv: i64) -> i64 {
    load_ptr(gv, 5)
}

// Create a constant (no gradient tracking needed)
fn grad_const(value: f64) -> i64 {
    grad_value_new(value, GradOp::Constant, 0, 0, 0)
}

// Create an input value with gradient tracking
fn grad_input(value: f64, tape: i64) -> i64 {
    grad_value_new(value, GradOp::Input, 0, 0, tape)
}

// Create an input value bound to a neural gate
fn grad_input_gate(value: f64, gate_id: i64, tape: i64) -> i64 {
    grad_value_new(value, GradOp::Input, 0, gate_id, tape)
}

// GradTape: Eager-mode computation graph recorder
// Layout:
//   nodes(0): i64      - vec of GradValue ptrs in topological order
//   training(1): i64   - 1 if in training mode, 0 for inference
//   temperature(2): f64 - temperature for Gumbel-softmax annealing
//   gate_grads(3): i64 - map from gate_id -> accumulated gradient
//
fn grad_tape_new() -> i64 {
    let tape: i64 = malloc(32);
    store_ptr(tape, 0, vec_new());
    store_i64(tape, 1, 1);          // Training mode by default
    store_f64(tape, 2, 1.0);        // Initial temperature = 1.0
    store_ptr(tape, 3, map_new());  // Empty gate gradients map
    tape
}

fn grad_tape_nodes(tape: i64) -> i64 {
    load_ptr(tape, 0)
}

fn grad_tape_is_training(tape: i64) -> i64 {
    load_i64(tape, 1)
}

fn grad_tape_set_training(tape: i64, training: i64) -> i64 {
    store_i64(tape, 1, training);
    0
}

fn grad_tape_temperature(tape: i64) -> f64 {
    load_f64(tape, 2)
}

fn grad_tape_set_temperature(tape: i64, temp: f64) -> i64 {
    store_f64(tape, 2, temp);
    0
}

fn grad_tape_gate_grads(tape: i64) -> i64 {
    load_ptr(tape, 3)
}

// Record a node in the tape
fn grad_tape_record(tape: i64, gv: i64) -> i64 {
    let nodes: i64 = grad_tape_nodes(tape);
    vec_push(nodes, gv);
    0
}

// Clear the tape (for next forward pass)
fn grad_tape_clear(tape: i64) -> i64 {
    let nodes: i64 = grad_tape_nodes(tape);
    vec_clear(nodes);
    let gate_grads: i64 = grad_tape_gate_grads(tape);
    map_clear(gate_grads);
    0
}

// ============================================================================
// Differentiable Operations
// ============================================================================

// Addition: d(a+b)/da = 1, d(a+b)/db = 1
fn grad_add(a: i64, b: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);
    let result: f64 = va + vb;

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Add, inputs, 0, tape)
}

// Subtraction: d(a-b)/da = 1, d(a-b)/db = -1
fn grad_sub(a: i64, b: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);
    let result: f64 = va - vb;

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Sub, inputs, 0, tape)
}

// Multiplication: d(a*b)/da = b, d(a*b)/db = a
fn grad_mul(a: i64, b: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);
    let result: f64 = va * vb;

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Mul, inputs, 0, tape)
}

// Division: d(a/b)/da = 1/b, d(a/b)/db = -a/b^2
fn grad_div(a: i64, b: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);
    let result: f64 = va / vb;

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Div, inputs, 0, tape)
}

// Negation: d(-a)/da = -1
fn grad_neg(a: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let result: f64 = 0.0 - va;

    let inputs: i64 = vec_new();
    vec_push(inputs, a);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Neg, inputs, 0, tape)
}

// Sigmoid: sigma(x) = 1 / (1 + exp(-x))
// Derivative: sigma(x) * (1 - sigma(x))
fn sigmoid(x: f64) -> f64 {
    1.0 / (1.0 + exp(0.0 - x))
}

fn grad_sigmoid(a: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let result: f64 = sigmoid(va);

    let inputs: i64 = vec_new();
    vec_push(inputs, a);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Sigmoid, inputs, 0, tape)
}

// Tanh: tanh(x) = (exp(x) - exp(-x)) / (exp(x) + exp(-x))
// Derivative: 1 - tanh(x)^2
fn grad_tanh(a: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let result: f64 = tanh(va);

    let inputs: i64 = vec_new();
    vec_push(inputs, a);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Tanh, inputs, 0, tape)
}

// ReLU: max(0, x)
// Derivative: 1 if x > 0, else 0
fn grad_relu(a: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let result: f64 = va;
    if va < 0.0 {
        result = 0.0;
    }

    let inputs: i64 = vec_new();
    vec_push(inputs, a);

    let tape: i64 = grad_value_tape(a);
    grad_value_new(result, GradOp::Relu, inputs, 0, tape)
}

// ============================================================================
// Differentiable Comparisons (for Neural Gates)
// ============================================================================

// These comparison operations use sigmoid relaxation in training mode
// to make them differentiable. In inference mode, they snap to discrete values.

// Greater than: a > b
// Training: sigmoid((a - b) * temperature)
// Inference: a > b ? 1.0 : 0.0
fn grad_gt(a: i64, b: i64, tape: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);

    let result: f64 = 0.0;
    if grad_tape_is_training(tape) == 1 {
        // Training mode: soft/differentiable
        let temp: f64 = grad_tape_temperature(tape);
        result = sigmoid((va - vb) * temp);
    } else {
        // Inference mode: hard/discrete
        if va > vb {
            result = 1.0;
        }
    }

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    grad_value_new(result, GradOp::Gt, inputs, 0, tape)
}

// Less than: a < b
// Training: sigmoid((b - a) * temperature)
fn grad_lt(a: i64, b: i64, tape: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);

    let result: f64 = 0.0;
    if grad_tape_is_training(tape) == 1 {
        let temp: f64 = grad_tape_temperature(tape);
        result = sigmoid((vb - va) * temp);
    } else {
        if va < vb {
            result = 1.0;
        }
    }

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    grad_value_new(result, GradOp::Lt, inputs, 0, tape)
}

// Greater than or equal: a >= b
fn grad_ge(a: i64, b: i64, tape: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);

    let result: f64 = 0.0;
    let epsilon: f64 = 0.0001;  // Small offset for >= vs >
    if grad_tape_is_training(tape) == 1 {
        let temp: f64 = grad_tape_temperature(tape);
        result = sigmoid((va - vb + epsilon) * temp);
    } else {
        if va >= vb {
            result = 1.0;
        }
    }

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    grad_value_new(result, GradOp::Ge, inputs, 0, tape)
}

// Less than or equal: a <= b
fn grad_le(a: i64, b: i64, tape: i64) -> i64 {
    let va: f64 = grad_value_get(a);
    let vb: f64 = grad_value_get(b);

    let result: f64 = 0.0;
    let epsilon: f64 = 0.0001;
    if grad_tape_is_training(tape) == 1 {
        let temp: f64 = grad_tape_temperature(tape);
        result = sigmoid((vb - va + epsilon) * temp);
    } else {
        if va <= vb {
            result = 1.0;
        }
    }

    let inputs: i64 = vec_new();
    vec_push(inputs, a);
    vec_push(inputs, b);

    grad_value_new(result, GradOp::Le, inputs, 0, tape)
}

// ============================================================================
// Gumbel-Softmax for Categorical Choices
// ============================================================================

// Gumbel-Softmax: Differentiable approximation to categorical sampling
// Used for neural gates that select from N options
//
// Given logits [l1, l2, ..., ln] and temperature tau:
// 1. Sample Gumbel noise: g_i = -log(-log(uniform(0,1)))
// 2. Compute: softmax((l_i + g_i) / tau)
//
// As tau -> 0, this approaches a one-hot categorical sample
// During training, tau is high (soft), during inference tau -> 0 (hard)

// Sample from Gumbel(0, 1) distribution
fn sample_gumbel() -> f64 {
    let u: f64 = random_uniform();  // uniform(0, 1)
    // Clamp to avoid log(0)
    if u < 0.0001 {
        u = 0.0001;
    }
    if u > 0.9999 {
        u = 0.9999;
    }
    0.0 - log(0.0 - log(u))
}

// Gumbel-Softmax for a vector of logits
// Returns a vector of soft probabilities (sums to ~1)
// logits: vec of f64 (raw scores for each category)
// temperature: softness parameter (lower = harder)
fn gumbel_softmax(logits: i64, temperature: f64, tape: i64) -> i64 {
    let n: i64 = vec_len(logits);
    let result: i64 = vec_new();

    // Compute perturbed logits
    let perturbed: i64 = vec_new();
    let i: i64 = 0;
    while i < n {
        let logit: f64 = vec_get_f64(logits, i);
        let gumbel: f64 = sample_gumbel();
        let perturbed_logit: f64 = (logit + gumbel) / temperature;
        vec_push_f64(perturbed, perturbed_logit);
        i = i + 1;
    }

    // Compute softmax over perturbed logits
    // First find max for numerical stability
    let max_val: f64 = vec_get_f64(perturbed, 0);
    i = 1;
    while i < n {
        let v: f64 = vec_get_f64(perturbed, i);
        if v > max_val {
            max_val = v;
        }
        i = i + 1;
    }

    // Compute exp(x - max) and sum
    let exp_sum: f64 = 0.0;
    let exps: i64 = vec_new();
    i = 0;
    while i < n {
        let v: f64 = vec_get_f64(perturbed, i);
        let e: f64 = exp(v - max_val);
        vec_push_f64(exps, e);
        exp_sum = exp_sum + e;
        i = i + 1;
    }

    // Normalize
    i = 0;
    while i < n {
        let e: f64 = vec_get_f64(exps, i);
        let prob: f64 = e / exp_sum;
        vec_push_f64(result, prob);
        i = i + 1;
    }

    result
}

// Hard categorical from Gumbel-Softmax (straight-through estimator)
// During forward: returns argmax (one-hot)
// During backward: uses soft gradients
fn gumbel_softmax_hard(logits: i64, temperature: f64, tape: i64) -> i64 {
    // Get soft probabilities
    let soft: i64 = gumbel_softmax(logits, temperature, tape);
    let n: i64 = vec_len(soft);

    if grad_tape_is_training(tape) == 0 {
        // Inference mode: return hard one-hot
        let argmax: i64 = 0;
        let max_val: f64 = vec_get_f64(soft, 0);
        let i: i64 = 1;
        while i < n {
            let v: f64 = vec_get_f64(soft, i);
            if v > max_val {
                max_val = v;
                argmax = i;
            }
            i = i + 1;
        }

        let result: i64 = vec_new();
        i = 0;
        while i < n {
            if i == argmax {
                vec_push_f64(result, 1.0);
            } else {
                vec_push_f64(result, 0.0);
            }
            i = i + 1;
        }
        return result;
    }

    // Training mode: return soft probabilities for gradient flow
    soft
}

// ============================================================================
// Backward Pass (Reverse-Mode Autodiff)
// ============================================================================

// Compute gradients for a single node based on its operation
fn backward_node(gv: i64) -> i64 {
    let op: i64 = grad_value_op(gv);
    let inputs: i64 = grad_value_inputs(gv);
    let grad_out: f64 = grad_value_grad(gv);  // Gradient flowing into this node

    if op == GradOp::Constant {
        // No gradient for constants
        return 0;
    }

    if op == GradOp::Input {
        // Gradient accumulates in input nodes (updated by gate gradient aggregation)
        return 0;
    }

    if op == GradOp::Add {
        // d(a+b)/da = 1, d(a+b)/db = 1
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        grad_value_add_grad(a, grad_out);
        grad_value_add_grad(b, grad_out);
        return 0;
    }

    if op == GradOp::Sub {
        // d(a-b)/da = 1, d(a-b)/db = -1
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        grad_value_add_grad(a, grad_out);
        grad_value_add_grad(b, 0.0 - grad_out);
        return 0;
    }

    if op == GradOp::Mul {
        // d(a*b)/da = b, d(a*b)/db = a
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let va: f64 = grad_value_get(a);
        let vb: f64 = grad_value_get(b);
        grad_value_add_grad(a, grad_out * vb);
        grad_value_add_grad(b, grad_out * va);
        return 0;
    }

    if op == GradOp::Div {
        // d(a/b)/da = 1/b, d(a/b)/db = -a/b^2
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let va: f64 = grad_value_get(a);
        let vb: f64 = grad_value_get(b);
        grad_value_add_grad(a, grad_out / vb);
        grad_value_add_grad(b, (0.0 - grad_out * va) / (vb * vb));
        return 0;
    }

    if op == GradOp::Neg {
        // d(-a)/da = -1
        let a: i64 = vec_get(inputs, 0);
        grad_value_add_grad(a, 0.0 - grad_out);
        return 0;
    }

    if op == GradOp::Sigmoid {
        // d(sigmoid(x))/dx = sigmoid(x) * (1 - sigmoid(x))
        let a: i64 = vec_get(inputs, 0);
        let out: f64 = grad_value_get(gv);  // sigmoid(x) was stored as output
        let local_grad: f64 = out * (1.0 - out);
        grad_value_add_grad(a, grad_out * local_grad);
        return 0;
    }

    if op == GradOp::Tanh {
        // d(tanh(x))/dx = 1 - tanh(x)^2
        let a: i64 = vec_get(inputs, 0);
        let out: f64 = grad_value_get(gv);
        let local_grad: f64 = 1.0 - out * out;
        grad_value_add_grad(a, grad_out * local_grad);
        return 0;
    }

    if op == GradOp::Relu {
        // d(relu(x))/dx = 1 if x > 0, else 0
        let a: i64 = vec_get(inputs, 0);
        let va: f64 = grad_value_get(a);
        let local_grad: f64 = 0.0;
        if va > 0.0 {
            local_grad = 1.0;
        }
        grad_value_add_grad(a, grad_out * local_grad);
        return 0;
    }

    // Comparison operations use sigmoid gradient
    if op == GradOp::Gt {
        // grad of sigmoid((a-b) * temp) w.r.t. a and b
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let out: f64 = grad_value_get(gv);
        let tape: i64 = grad_value_tape(gv);
        let temp: f64 = grad_tape_temperature(tape);
        let sig_grad: f64 = out * (1.0 - out) * temp;
        grad_value_add_grad(a, grad_out * sig_grad);
        grad_value_add_grad(b, 0.0 - grad_out * sig_grad);
        return 0;
    }

    if op == GradOp::Lt {
        // grad of sigmoid((b-a) * temp) w.r.t. a and b
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let out: f64 = grad_value_get(gv);
        let tape: i64 = grad_value_tape(gv);
        let temp: f64 = grad_tape_temperature(tape);
        let sig_grad: f64 = out * (1.0 - out) * temp;
        grad_value_add_grad(a, 0.0 - grad_out * sig_grad);
        grad_value_add_grad(b, grad_out * sig_grad);
        return 0;
    }

    if op == GradOp::Ge {
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let out: f64 = grad_value_get(gv);
        let tape: i64 = grad_value_tape(gv);
        let temp: f64 = grad_tape_temperature(tape);
        let sig_grad: f64 = out * (1.0 - out) * temp;
        grad_value_add_grad(a, grad_out * sig_grad);
        grad_value_add_grad(b, 0.0 - grad_out * sig_grad);
        return 0;
    }

    if op == GradOp::Le {
        let a: i64 = vec_get(inputs, 0);
        let b: i64 = vec_get(inputs, 1);
        let out: f64 = grad_value_get(gv);
        let tape: i64 = grad_value_tape(gv);
        let temp: f64 = grad_tape_temperature(tape);
        let sig_grad: f64 = out * (1.0 - out) * temp;
        grad_value_add_grad(a, 0.0 - grad_out * sig_grad);
        grad_value_add_grad(b, grad_out * sig_grad);
        return 0;
    }

    0
}

// Backward pass: compute gradients for all nodes in the tape
// loss: the output GradValue to differentiate
fn backward(loss: i64) -> i64 {
    let tape: i64 = grad_value_tape(loss);
    if tape == 0 {
        return 0;  // No tape = no gradients
    }

    // Seed the loss gradient with 1.0
    grad_value_set_grad(loss, 1.0);

    // Iterate through tape in reverse order (reverse-mode autodiff)
    let nodes: i64 = grad_tape_nodes(tape);
    let n: i64 = vec_len(nodes);
    let i: i64 = n - 1;

    while i >= 0 {
        let gv: i64 = vec_get(nodes, i);
        backward_node(gv);
        i = i - 1;
    }

    // Aggregate gradients by gate_id for per-gate gradient accumulation
    aggregate_gate_gradients(tape);

    0
}

// Aggregate gradients by neural gate ID
fn aggregate_gate_gradients(tape: i64) -> i64 {
    let nodes: i64 = grad_tape_nodes(tape);
    let gate_grads: i64 = grad_tape_gate_grads(tape);
    let n: i64 = vec_len(nodes);
    let i: i64 = 0;

    while i < n {
        let gv: i64 = vec_get(nodes, i);
        let gate_id: i64 = grad_value_gate_id(gv);

        if gate_id != 0 {
            let grad: f64 = grad_value_grad(gv);
            let existing: f64 = map_get_f64_or(gate_grads, gate_id, 0.0);
            map_set_f64(gate_grads, gate_id, existing + grad);
        }

        i = i + 1;
    }

    0
}

// Get the accumulated gradient for a specific gate
fn get_gate_gradient(tape: i64, gate_id: i64) -> f64 {
    let gate_grads: i64 = grad_tape_gate_grads(tape);
    map_get_f64_or(gate_grads, gate_id, 0.0)
}

// ============================================================================
// Temperature Annealing
// ============================================================================

// Exponential annealing: temp = temp_init * decay^step
fn anneal_exponential(tape: i64, decay: f64) -> i64 {
    let current: f64 = grad_tape_temperature(tape);
    let new_temp: f64 = current * decay;
    // Clamp to minimum temperature
    if new_temp < 0.01 {
        new_temp = 0.01;
    }
    grad_tape_set_temperature(tape, new_temp);
    0
}

// Linear annealing: temp = max(temp_min, temp - delta)
fn anneal_linear(tape: i64, delta: f64, min_temp: f64) -> i64 {
    let current: f64 = grad_tape_temperature(tape);
    let new_temp: f64 = current - delta;
    if new_temp < min_temp {
        new_temp = min_temp;
    }
    grad_tape_set_temperature(tape, new_temp);
    0
}

// ============================================================================
// Neural Gate Runtime
// ============================================================================

// NeuralGate: Runtime representation of a neural gate
// Layout:
//   id(0): i64          - unique gate identifier
//   threshold(1): f64   - learnable threshold parameter
//   gradient(2): f64    - accumulated gradient for threshold
//   anima_id(3): i64    - bound anima ID (for belief sync)
//   belief_key(4): i64  - belief key in anima (string ptr)
//
fn neural_gate_new(id: i64, threshold: f64, anima_id: i64, belief_key: i64) -> i64 {
    let gate: i64 = malloc(40);
    store_i64(gate, 0, id);
    store_f64(gate, 1, threshold);
    store_f64(gate, 2, 0.0);  // Initial gradient
    store_i64(gate, 3, anima_id);
    store_ptr(gate, 4, belief_key);
    gate
}

fn neural_gate_id(gate: i64) -> i64 {
    load_i64(gate, 0)
}

fn neural_gate_threshold(gate: i64) -> f64 {
    load_f64(gate, 1)
}

fn neural_gate_set_threshold(gate: i64, threshold: f64) -> i64 {
    store_f64(gate, 1, threshold);
    0
}

fn neural_gate_gradient(gate: i64) -> f64 {
    load_f64(gate, 2)
}

fn neural_gate_add_gradient(gate: i64, grad: f64) -> i64 {
    let current: f64 = load_f64(gate, 2);
    store_f64(gate, 2, current + grad);
    0
}

fn neural_gate_zero_grad(gate: i64) -> i64 {
    store_f64(gate, 2, 0.0);
    0
}

fn neural_gate_anima_id(gate: i64) -> i64 {
    load_i64(gate, 3)
}

fn neural_gate_belief_key(gate: i64) -> i64 {
    load_ptr(gate, 4)
}

// Apply gradient update to gate threshold (SGD step)
fn neural_gate_update(gate: i64, learning_rate: f64) -> i64 {
    let threshold: f64 = neural_gate_threshold(gate);
    let grad: f64 = neural_gate_gradient(gate);
    let new_threshold: f64 = threshold - learning_rate * grad;

    // Clamp threshold to valid range [0, 1] for belief compatibility
    if new_threshold < 0.0 {
        new_threshold = 0.0;
    }
    if new_threshold > 1.0 {
        new_threshold = 1.0;
    }

    neural_gate_set_threshold(gate, new_threshold);
    neural_gate_zero_grad(gate);
    0
}

// Evaluate a threshold gate in training or inference mode
// Returns soft probability in training, hard bool in inference
fn neural_gate_eval(gate: i64, value: f64, tape: i64) -> i64 {
    let threshold: f64 = neural_gate_threshold(gate);
    let gate_id: i64 = neural_gate_id(gate);

    // Create tracked values
    let v: i64 = grad_input_gate(value, gate_id, tape);
    let t: i64 = grad_input_gate(threshold, gate_id, tape);

    // Compare: value > threshold
    grad_gt(v, t, tape)
}

// ============================================================================
// Anima Belief Integration
// ============================================================================

// Anima integration allows neural gate thresholds to be stored as beliefs
// and updated through gradient descent. This unifies the cognitive architecture
// with the neural IR system.

// AnimaBeliefBinding: Links a neural gate to an anima belief
// Layout:
//   gate(0): i64       - ptr to the neural gate
//   anima(1): i64      - ptr to the anima memory
//   belief_key(2): i64 - string key for the belief in the anima
//   min_conf(3): f64   - minimum confidence threshold for this gate
//
fn anima_belief_binding_new(gate: i64, anima: i64, belief_key: i64, min_conf: f64) -> i64 {
    let binding: i64 = malloc(32);
    store_ptr(binding, 0, gate);
    store_ptr(binding, 1, anima);
    store_ptr(binding, 2, belief_key);
    store_f64(binding, 3, min_conf);
    binding
}

fn binding_gate(binding: i64) -> i64 { load_ptr(binding, 0) }
fn binding_anima(binding: i64) -> i64 { load_ptr(binding, 1) }
fn binding_belief_key(binding: i64) -> i64 { load_ptr(binding, 2) }
fn binding_min_conf(binding: i64) -> f64 { load_f64(binding, 3) }

// Sync gate threshold to anima belief
// This updates the belief confidence in the anima to match the gate threshold
fn sync_gate_to_belief(binding: i64) -> i64 {
    let gate: i64 = binding_gate(binding);
    let anima: i64 = binding_anima(binding);
    let key: i64 = binding_belief_key(binding);
    let threshold: f64 = neural_gate_threshold(gate);

    // Scale threshold (0.0-1.0) to confidence (0-100)
    let confidence: i64 = f64_to_i64(threshold * 100.0);

    // Update belief in anima
    // The anima's belief system uses the same confidence scale
    anima_update_belief_confidence(anima, key, confidence);

    0
}

// Sync anima belief to gate threshold
// This updates the gate threshold from the current belief confidence
fn sync_belief_to_gate(binding: i64) -> i64 {
    let gate: i64 = binding_gate(binding);
    let anima: i64 = binding_anima(binding);
    let key: i64 = binding_belief_key(binding);

    // Get belief confidence from anima (0-100 scale)
    let confidence: i64 = anima_get_belief_confidence(anima, key);

    // Scale to threshold (0.0-1.0)
    let threshold: f64 = i64_to_f64(confidence) / 100.0;

    neural_gate_set_threshold(gate, threshold);

    0
}

// Apply gradient update to gate and sync to anima belief
fn neural_gate_update_with_belief(binding: i64, learning_rate: f64) -> i64 {
    let gate: i64 = binding_gate(binding);
    let anima: i64 = binding_anima(binding);
    let key: i64 = binding_belief_key(binding);
    let min_conf: f64 = binding_min_conf(binding);

    // Get current threshold and gradient
    let threshold: f64 = neural_gate_threshold(gate);
    let grad: f64 = neural_gate_gradient(gate);

    // Compute new threshold via SGD
    let new_threshold: f64 = threshold - learning_rate * grad;

    // Clamp to valid range [min_conf, 1.0]
    // min_conf ensures the gate respects the anima's belief threshold requirements
    if new_threshold < min_conf {
        new_threshold = min_conf;
    }
    if new_threshold > 1.0 {
        new_threshold = 1.0;
    }

    // Update gate
    neural_gate_set_threshold(gate, new_threshold);
    neural_gate_zero_grad(gate);

    // Sync to anima belief
    let confidence: i64 = f64_to_i64(new_threshold * 100.0);
    anima_update_belief_confidence(anima, key, confidence);

    // If confidence changes significantly, log to anima's episodic memory
    let old_conf: i64 = f64_to_i64(threshold * 100.0);
    let conf_change: i64 = confidence - old_conf;
    if conf_change > 5 {
        // Significant positive change - belief strengthened
        anima_remember_belief_update(anima, key, old_conf, confidence, 1);
    }
    if conf_change < -5 {
        // Significant negative change - belief weakened
        anima_remember_belief_update(anima, key, old_conf, confidence, 0);
    }

    0
}

// ============================================================================
// Anima Runtime Interface (to be implemented in runtime)
// ============================================================================

// These functions interface with the anima runtime system
// They are declared here and implemented in the anima module

// Update a belief's confidence in the anima
fn anima_update_belief_confidence(anima: i64, key: i64, confidence: i64) -> i64 {
    // Call into anima runtime
    // This will update the belief and potentially trigger belief revision
    anima_belief_set_confidence(anima, key, confidence)
}

// Get a belief's confidence from the anima
fn anima_get_belief_confidence(anima: i64, key: i64) -> i64 {
    anima_belief_get_confidence(anima, key)
}

// Record a belief update in episodic memory
fn anima_remember_belief_update(anima: i64, key: i64, old_conf: i64, new_conf: i64, strengthened: i64) -> i64 {
    // Create memory of belief change
    // This allows the anima to learn from how its beliefs evolve
    anima_episodic_add_belief_change(anima, key, old_conf, new_conf, strengthened)
}

// External runtime functions (declared, implemented in runtime)
fn anima_belief_set_confidence(anima: i64, key: i64, confidence: i64) -> i64 { 0 }
fn anima_belief_get_confidence(anima: i64, key: i64) -> i64 { 50 }  // Default 50%
fn anima_episodic_add_belief_change(anima: i64, key: i64, old_conf: i64, new_conf: i64, dir: i64) -> i64 { 0 }

// Helper functions
fn f64_to_i64(x: f64) -> i64 {
    // Truncate to integer
    let result: i64 = 0;
    if x >= 0.0 {
        result = x;  // implicit conversion
    } else {
        result = 0 - (0.0 - x);
    }
    result
}

fn i64_to_f64(x: i64) -> f64 {
    let result: f64 = x;  // implicit conversion
    result
}

// ============================================================================
// Training Loop Helpers
// ============================================================================

// Training configuration
// Layout:
//   learning_rate(0): f64
//   initial_temp(1): f64
//   min_temp(2): f64
//   temp_decay(3): f64
//   epochs(4): i64
//
fn training_config_new(lr: f64, init_temp: f64, min_temp: f64, decay: f64, epochs: i64) -> i64 {
    let config: i64 = malloc(40);
    store_f64(config, 0, lr);
    store_f64(config, 1, init_temp);
    store_f64(config, 2, min_temp);
    store_f64(config, 3, decay);
    store_i64(config, 4, epochs);
    config
}

fn config_learning_rate(config: i64) -> f64 { load_f64(config, 0) }
fn config_initial_temp(config: i64) -> f64 { load_f64(config, 1) }
fn config_min_temp(config: i64) -> f64 { load_f64(config, 2) }
fn config_temp_decay(config: i64) -> f64 { load_f64(config, 3) }
fn config_epochs(config: i64) -> i64 { load_i64(config, 4) }

// Run a training step on all gates with anima bindings
fn training_step(tape: i64, bindings: i64, config: i64) -> i64 {
    let lr: f64 = config_learning_rate(config);
    let n: i64 = vec_len(bindings);
    let i: i64 = 0;

    while i < n {
        let binding: i64 = vec_get(bindings, i);
        neural_gate_update_with_belief(binding, lr);
        i = i + 1;
    }

    // Anneal temperature
    let decay: f64 = config_temp_decay(config);
    let min_temp: f64 = config_min_temp(config);
    anneal_linear(tape, 1.0 - decay, min_temp);

    0
}

// Initialize training: set tape to training mode and initial temperature
fn training_init(tape: i64, config: i64) -> i64 {
    grad_tape_set_training(tape, 1);
    let init_temp: f64 = config_initial_temp(config);
    grad_tape_set_temperature(tape, init_temp);
    0
}

// Finalize training: set tape to inference mode
fn training_finalize(tape: i64) -> i64 {
    grad_tape_set_training(tape, 0);
    0
}
