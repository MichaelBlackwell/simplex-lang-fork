// Data loader for batching and preprocessing training data

use simplex_std::vec::Vec;
use simplex_std::string::String;
use simplex_std::option::Option;
use super::generator::{TrainingExample, Rng};

/// Configuration for data loader
#[derive(Clone)]
pub struct DataLoaderConfig {
    /// Batch size
    pub batch_size: usize,

    /// Whether to shuffle data
    pub shuffle: bool,

    /// Maximum sequence length (truncate longer)
    pub max_length: Option<usize>,

    /// Whether to drop incomplete last batch
    pub drop_last: bool,

    /// Number of worker threads (0 = single threaded)
    pub num_workers: usize,

    /// Prefetch batches
    pub prefetch: usize,
}

impl Default for DataLoaderConfig {
    fn default() -> Self {
        DataLoaderConfig {
            batch_size: 32,
            shuffle: true,
            max_length: Some(512),
            drop_last: false,
            num_workers: 0,
            prefetch: 2,
        }
    }
}

/// A batch of training examples
#[derive(Clone)]
pub struct Batch {
    /// Examples in the batch
    pub examples: Vec<TrainingExample>,

    /// Batch index
    pub batch_idx: usize,

    /// Whether this is the last batch
    pub is_last: bool,
}

impl Batch {
    /// Number of examples in batch
    pub fn len(&self) -> usize {
        self.examples.len()
    }

    /// Check if empty
    pub fn is_empty(&self) -> bool {
        self.examples.is_empty()
    }

    /// Get prompts
    pub fn prompts(&self) -> Vec<&str> {
        self.examples.iter().map(|e| e.prompt.as_str()).collect()
    }

    /// Get responses
    pub fn responses(&self) -> Vec<&str> {
        self.examples.iter().map(|e| e.response.as_str()).collect()
    }

    /// Average length of examples
    pub fn avg_length(&self) -> f64 {
        if self.examples.is_empty() {
            0.0
        } else {
            let total: usize = self.examples.iter().map(|e| e.total_length()).sum();
            total as f64 / self.examples.len() as f64
        }
    }

    /// Convert responses to target tensor for loss computation
    ///
    /// Creates a tensor representation of target outputs suitable for
    /// computing cross-entropy loss against model predictions.
    pub fn target_tensor(&self) -> DualTensor {
        use simplex_std::dual;
        use simplex_learning::tensor::DualTensor;

        let responses = self.responses();
        let batch_size = responses.len();

        // Find max response length for padding
        let seq_len = responses.iter().map(|r| r.len()).max().unwrap_or(1);
        let hidden_dim = 512; // Match model hidden dimension

        // Create target tensor: for each character, create one-hot over vocab
        // Simplified: use character codes as soft targets
        let mut target_data = Vec::with_capacity(batch_size * seq_len * hidden_dim);

        for response in &responses {
            for (_, ch) in response.chars().take(seq_len).enumerate() {
                // Create soft target (real impl would use proper vocabulary)
                let char_id = (ch as u32) as usize;
                for j in 0..hidden_dim {
                    // Soft one-hot: high at char_id % hidden_dim, low elsewhere
                    let target_idx = char_id % hidden_dim;
                    let val = if j == target_idx { 1.0 } else { 0.0 };
                    target_data.push(dual::constant(val));
                }
            }
            // Pad to seq_len
            let pad_chars = seq_len.saturating_sub(response.len());
            for _ in 0..(pad_chars * hidden_dim) {
                target_data.push(dual::constant(0.0));
            }
        }

        DualTensor::from_vec(target_data, &[batch_size, seq_len, hidden_dim])
    }
}

/// Data loader for iterating over training data
pub struct DataLoader {
    /// All examples
    examples: Vec<TrainingExample>,

    /// Configuration
    config: DataLoaderConfig,

    /// Current position
    position: usize,

    /// Current epoch
    epoch: usize,

    /// Random number generator for shuffling
    rng: Rng,

    /// Indices for iteration (shuffled if enabled)
    indices: Vec<usize>,
}

impl DataLoader {
    /// Create new data loader
    pub fn new(examples: Vec<TrainingExample>, config: DataLoaderConfig) -> Self {
        let n = examples.len();
        let indices: Vec<usize> = (0..n).collect();

        let mut loader = DataLoader {
            examples,
            config,
            position: 0,
            epoch: 0,
            rng: Rng::new(42),
            indices,
        };

        if loader.config.shuffle {
            loader.shuffle();
        }

        loader
    }

    /// Create with default config
    pub fn from_examples(examples: Vec<TrainingExample>) -> Self {
        Self::new(examples, DataLoaderConfig::default())
    }

    /// Shuffle indices
    fn shuffle(&mut self) {
        self.rng.shuffle(&mut self.indices);
    }

    /// Get next batch
    pub fn next_batch(&mut self) -> Option<Batch> {
        if self.position >= self.examples.len() {
            return None;
        }

        let batch_size = self.config.batch_size;
        let remaining = self.examples.len() - self.position;

        // Check if we should drop incomplete batch
        if remaining < batch_size && self.config.drop_last {
            return None;
        }

        let actual_size = batch_size.min(remaining);
        let mut batch_examples = Vec::with_capacity(actual_size);

        for i in 0..actual_size {
            let idx = self.indices[self.position + i];
            let mut example = self.examples[idx].clone();

            // Apply max length truncation if configured
            if let Some(max_len) = self.config.max_length {
                if example.prompt.len() > max_len {
                    example.prompt = example.prompt[..max_len].to_string();
                }
                if example.response.len() > max_len {
                    example.response = example.response[..max_len].to_string();
                }
            }

            batch_examples.push(example);
        }

        let batch_idx = self.position / batch_size;
        let is_last = self.position + actual_size >= self.examples.len();

        self.position += actual_size;

        Some(Batch {
            examples: batch_examples,
            batch_idx,
            is_last,
        })
    }

    /// Reset to beginning (start new epoch)
    pub fn reset(&mut self) {
        self.position = 0;
        self.epoch += 1;

        if self.config.shuffle {
            self.shuffle();
        }
    }

    /// Get number of batches
    pub fn num_batches(&self) -> usize {
        if self.config.drop_last {
            self.examples.len() / self.config.batch_size
        } else {
            (self.examples.len() + self.config.batch_size - 1) / self.config.batch_size
        }
    }

    /// Get total examples
    pub fn len(&self) -> usize {
        self.examples.len()
    }

    /// Check if empty
    pub fn is_empty(&self) -> bool {
        self.examples.is_empty()
    }

    /// Get current epoch
    pub fn epoch(&self) -> usize {
        self.epoch
    }

    /// Set random seed
    pub fn seed(&mut self, seed: u64) {
        self.rng.seed(seed);
    }

    /// Iterate over all batches
    pub fn iter(&mut self) -> DataLoaderIterator {
        self.reset();
        DataLoaderIterator { loader: self }
    }

    /// Get a clone of all examples
    pub fn examples(&self) -> Vec<TrainingExample> {
        self.examples.clone()
    }

    /// Get a clone of the configuration
    pub fn config(&self) -> DataLoaderConfig {
        self.config.clone()
    }

    /// Create a clone of this DataLoader for iteration
    pub fn clone_for_iteration(&self) -> DataLoader {
        DataLoader::new(self.examples.clone(), self.config.clone())
    }
}

/// Iterator over batches
pub struct DataLoaderIterator<'a> {
    loader: &'a mut DataLoader,
}

impl<'a> Iterator for DataLoaderIterator<'a> {
    type Item = Batch;

    fn next(&mut self) -> Option<Self::Item> {
        self.loader.next_batch()
    }
}

/// Collate function for combining examples into batch tensors
pub struct Collator {
    /// Padding token ID
    pad_token_id: usize,

    /// Maximum length (None = use longest in batch)
    max_length: Option<usize>,

    /// Whether to pad to fixed length
    pad_to_max: bool,
}

impl Collator {
    /// Create new collator
    pub fn new(pad_token_id: usize) -> Self {
        Collator {
            pad_token_id,
            max_length: None,
            pad_to_max: false,
        }
    }

    /// Set maximum length
    pub fn with_max_length(mut self, length: usize) -> Self {
        self.max_length = Some(length);
        self
    }

    /// Enable padding to max length
    pub fn pad_to_max(mut self) -> Self {
        self.pad_to_max = true;
        self
    }

    /// Find maximum length in sequences
    pub fn find_max_length(&self, lengths: &[usize]) -> usize {
        let batch_max = lengths.iter().max().copied().unwrap_or(0);

        match self.max_length {
            Some(configured_max) => batch_max.min(configured_max),
            None => batch_max,
        }
    }
}

/// Split dataset into train/val/test
pub struct DatasetSplit {
    /// Training examples
    pub train: Vec<TrainingExample>,

    /// Validation examples
    pub val: Vec<TrainingExample>,

    /// Test examples
    pub test: Vec<TrainingExample>,
}

impl DatasetSplit {
    /// Split dataset with given ratios (train, val, test)
    pub fn split(examples: Vec<TrainingExample>, train_ratio: f64, val_ratio: f64) -> Self {
        let n = examples.len();
        let train_end = (n as f64 * train_ratio) as usize;
        let val_end = train_end + (n as f64 * val_ratio) as usize;

        let mut examples = examples;
        // Shuffle before splitting
        let mut rng = Rng::new(42);
        rng.shuffle(&mut examples);

        DatasetSplit {
            train: examples[..train_end].to_vec(),
            val: examples[train_end..val_end].to_vec(),
            test: examples[val_end..].to_vec(),
        }
    }

    /// Split 80/10/10
    pub fn standard_split(examples: Vec<TrainingExample>) -> Self {
        Self::split(examples, 0.8, 0.1)
    }

    /// Split 90/5/5
    pub fn large_train_split(examples: Vec<TrainingExample>) -> Self {
        Self::split(examples, 0.9, 0.05)
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    fn make_examples(n: usize) -> Vec<TrainingExample> {
        (0..n)
            .map(|i| TrainingExample::new(&format!("prompt {}", i), &format!("response {}", i)))
            .collect()
    }

    #[test]
    fn test_data_loader_creation() {
        let examples = make_examples(100);
        let loader = DataLoader::from_examples(examples);

        assert_eq!(loader.len(), 100);
        assert!(!loader.is_empty());
    }

    #[test]
    fn test_batching() {
        let examples = make_examples(100);
        let config = DataLoaderConfig {
            batch_size: 32,
            shuffle: false,
            ..Default::default()
        };
        let mut loader = DataLoader::new(examples, config);

        let batch = loader.next_batch().unwrap();
        assert_eq!(batch.len(), 32);
        assert_eq!(batch.batch_idx, 0);
        assert!(!batch.is_last);

        let batch2 = loader.next_batch().unwrap();
        assert_eq!(batch2.batch_idx, 1);
    }

    #[test]
    fn test_num_batches() {
        let examples = make_examples(100);
        let config = DataLoaderConfig {
            batch_size: 32,
            drop_last: false,
            ..Default::default()
        };
        let loader = DataLoader::new(examples, config);

        assert_eq!(loader.num_batches(), 4); // 32 + 32 + 32 + 4
    }

    #[test]
    fn test_drop_last() {
        let examples = make_examples(100);
        let config = DataLoaderConfig {
            batch_size: 32,
            drop_last: true,
            ..Default::default()
        };
        let loader = DataLoader::new(examples, config);

        assert_eq!(loader.num_batches(), 3); // 32 + 32 + 32, drop 4
    }

    #[test]
    fn test_split() {
        let examples = make_examples(100);
        let split = DatasetSplit::standard_split(examples);

        assert_eq!(split.train.len(), 80);
        assert_eq!(split.val.len(), 10);
        assert_eq!(split.test.len(), 10);
    }

    #[test]
    fn test_batch_properties() {
        let examples = make_examples(10);
        let batch = Batch {
            examples,
            batch_idx: 0,
            is_last: false,
        };

        assert_eq!(batch.len(), 10);
        assert!(!batch.is_empty());
        assert!(batch.avg_length() > 0.0);
    }
}
