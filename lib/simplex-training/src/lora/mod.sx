// LoRA (Low-Rank Adaptation) module
//
// Implements efficient fine-tuning of large models by training only
// low-rank adapter matrices while keeping base model weights frozen.
//
// # Key Components
//
// - `layer`: LoRA adapter layer
// - `config`: Configuration for LoRA training
// - `adapter`: Full model adapter wrapper
//
// # Reference
// "LoRA: Low-Rank Adaptation of Large Language Models"
// https://arxiv.org/abs/2106.09685

pub mod layer;
pub mod config;
pub mod adapter;

pub use layer::LoRALayer;
pub use config::LoRAConfig;
pub use adapter::{LoRAModel, LoRAAdapter};
