// Temperature-aware attention mechanisms for Neural IR training
//
// Provides attention mechanisms with learnable temperature that can
// transition from soft (training) to hard (inference) attention.
//
// Temperature controls attention sharpness:
// - Low temperature (→0): Sparse, nearly hard attention
// - High temperature (→1): Standard soft attention
// - Very high temperature (>1): Uniform/diffuse attention
//
// # Example
//
// ```simplex
// use simplex_training::neural::{TemperatureAttention, AttentionConfig};
//
// let mut attention = TemperatureAttention::new(AttentionConfig {
//     embed_dim: 512,
//     num_heads: 8,
//     initial_temperature: 1.0,
//     learn_temperature: true,
// });
//
// // Compute attention
// let output = attention.forward(&query, &key, &value, None);
// ```

use simplex_std::dual;
use simplex_std::vec::Vec;
use simplex_learning::tensor::DualTensor;
use simplex_std::f64::consts::PI;

/// Configuration for temperature-aware attention
#[derive(Clone)]
pub struct AttentionConfig {
    /// Embedding dimension
    pub embed_dim: usize,

    /// Number of attention heads
    pub num_heads: usize,

    /// Initial temperature value
    pub initial_temperature: f64,

    /// Whether to learn temperature during training
    pub learn_temperature: bool,

    /// Minimum temperature (prevents collapse)
    pub min_temperature: f64,

    /// Maximum temperature (prevents uniform attention)
    pub max_temperature: f64,

    /// Dropout rate (0 = no dropout)
    pub dropout: f64,

    /// Whether to use learnable schedule for temperature
    pub use_schedule: bool,
}

impl Default for AttentionConfig {
    fn default() -> Self {
        AttentionConfig {
            embed_dim: 512,
            num_heads: 8,
            initial_temperature: 1.0,
            learn_temperature: true,
            min_temperature: 0.01,
            max_temperature: 10.0,
            dropout: 0.0,
            use_schedule: false,
        }
    }
}

impl AttentionConfig {
    /// Configuration for sharp attention (nearly hard)
    pub fn sharp() -> Self {
        AttentionConfig {
            initial_temperature: 0.1,
            learn_temperature: false,
            ..Default::default()
        }
    }

    /// Configuration for soft attention (standard transformer)
    pub fn soft() -> Self {
        AttentionConfig {
            initial_temperature: 1.0,
            learn_temperature: false,
            ..Default::default()
        }
    }

    /// Configuration for learnable temperature
    pub fn learnable() -> Self {
        AttentionConfig {
            initial_temperature: 1.0,
            learn_temperature: true,
            use_schedule: true,
            ..Default::default()
        }
    }
}

/// Learnable temperature parameter
///
/// Wraps temperature with constraints and optional scheduling.
pub struct LearnableTemperature {
    /// Current temperature value
    value: dual,

    /// Minimum allowed temperature
    min: f64,

    /// Maximum allowed temperature
    max: f64,

    /// Learning rate for temperature updates
    lr: f64,

    /// Current training step (for scheduling)
    step: u64,

    /// Whether to use annealing schedule
    use_schedule: bool,

    /// Initial temperature (for reset)
    initial: f64,
}

impl LearnableTemperature {
    /// Create new learnable temperature
    pub fn new(initial: f64, min: f64, max: f64) -> Self {
        LearnableTemperature {
            value: dual::variable(initial.clamp(min, max)),
            min,
            max,
            lr: 0.001,
            step: 0,
            use_schedule: false,
            initial,
        }
    }

    /// Create with annealing schedule
    pub fn with_schedule(initial: f64, min: f64, max: f64) -> Self {
        LearnableTemperature {
            value: dual::variable(initial.clamp(min, max)),
            min,
            max,
            lr: 0.001,
            step: 0,
            use_schedule: true,
            initial,
        }
    }

    /// Get current temperature value
    pub fn get(&self) -> dual {
        if self.use_schedule {
            // Exponential decay schedule
            let decay_rate = 0.9999;
            let scheduled = self.initial * decay_rate.powi(self.step as i32);
            let clamped = scheduled.clamp(self.min, self.max);
            dual::new(clamped, self.value.der)
        } else {
            self.value
        }
    }

    /// Get raw value (without scheduling)
    pub fn raw(&self) -> f64 {
        self.value.val
    }

    /// Set temperature value
    pub fn set(&mut self, value: f64) {
        self.value = dual::variable(value.clamp(self.min, self.max));
    }

    /// Update from gradient
    pub fn update(&mut self, gradient: f64) {
        let new_val = self.value.val - self.lr * gradient;
        self.value = dual::variable(new_val.clamp(self.min, self.max));
    }

    /// Advance training step
    pub fn step(&mut self) {
        self.step += 1;
    }

    /// Reset to initial value
    pub fn reset(&mut self) {
        self.value = dual::variable(self.initial);
        self.step = 0;
    }

    /// Set learning rate
    pub fn set_lr(&mut self, lr: f64) {
        self.lr = lr;
    }
}

/// Temperature-aware multi-head attention
///
/// Implements scaled dot-product attention with learnable temperature:
/// Attention(Q, K, V) = softmax(QK^T / (sqrt(d_k) * T)) * V
///
/// Where T is the (optionally learnable) temperature parameter.
pub struct TemperatureAttention {
    /// Configuration
    config: AttentionConfig,

    /// Temperature parameter
    temperature: LearnableTemperature,

    /// Query projection weights [embed_dim, embed_dim]
    q_weight: Vec<dual>,

    /// Key projection weights [embed_dim, embed_dim]
    k_weight: Vec<dual>,

    /// Value projection weights [embed_dim, embed_dim]
    v_weight: Vec<dual>,

    /// Output projection weights [embed_dim, embed_dim]
    out_weight: Vec<dual>,

    /// Scaling factor: 1 / sqrt(head_dim)
    scale: f64,

    /// Head dimension
    head_dim: usize,

    /// Random state for Gumbel sampling
    rng_state: u64,

    /// Operating mode (soft vs hard)
    hard_attention: bool,
}

impl TemperatureAttention {
    /// Create new temperature attention layer
    pub fn new(config: AttentionConfig) -> Self {
        let embed_dim = config.embed_dim;
        let head_dim = embed_dim / config.num_heads;
        let scale = 1.0 / (head_dim as f64).sqrt();

        // Initialize weights (Xavier/Glorot initialization)
        let weight_scale = (2.0 / embed_dim as f64).sqrt();
        let total_weights = embed_dim * embed_dim;

        let q_weight = Self::init_weights(total_weights, weight_scale);
        let k_weight = Self::init_weights(total_weights, weight_scale);
        let v_weight = Self::init_weights(total_weights, weight_scale);
        let out_weight = Self::init_weights(total_weights, weight_scale);

        let temperature = if config.use_schedule {
            LearnableTemperature::with_schedule(
                config.initial_temperature,
                config.min_temperature,
                config.max_temperature,
            )
        } else {
            LearnableTemperature::new(
                config.initial_temperature,
                config.min_temperature,
                config.max_temperature,
            )
        };

        TemperatureAttention {
            config,
            temperature,
            q_weight,
            k_weight,
            v_weight,
            out_weight,
            scale,
            head_dim,
            rng_state: 0x123456789ABCDEF0,
            hard_attention: false,
        }
    }

    /// Initialize weights with given scale
    fn init_weights(size: usize, scale: f64) -> Vec<dual> {
        let mut weights = Vec::with_capacity(size);
        let mut rng: u64 = 0xDEADBEEF;

        for _ in 0..size {
            // Xorshift for randomness
            rng ^= rng << 13;
            rng ^= rng >> 7;
            rng ^= rng << 17;

            let u = (rng as f64) / (u64::MAX as f64);
            let z = (2.0 * u - 1.0) * scale; // Uniform [-scale, scale]
            weights.push(dual::variable(z));
        }

        weights
    }

    /// Set hard attention mode (for inference)
    pub fn set_hard(&mut self, hard: bool) {
        self.hard_attention = hard;
    }

    /// Set temperature value
    pub fn set_temperature(&mut self, temp: f64) {
        self.temperature.set(temp);
    }

    /// Get current temperature
    pub fn temperature(&self) -> dual {
        self.temperature.get()
    }

    /// Forward pass
    ///
    /// Query: [batch, seq_len, embed_dim]
    /// Key: [batch, seq_len, embed_dim]
    /// Value: [batch, seq_len, embed_dim]
    /// Mask: Optional [batch, seq_len, seq_len] (additive mask, -inf for blocked)
    ///
    /// Returns: [batch, seq_len, embed_dim]
    pub fn forward(&mut self,
                   query: &DualTensor,
                   key: &DualTensor,
                   value: &DualTensor,
                   mask: Option<&DualTensor>) -> DualTensor {
        // Project Q, K, V
        let q = self.linear_projection(query, &self.q_weight);
        let k = self.linear_projection(key, &self.k_weight);
        let v = self.linear_projection(value, &self.v_weight);

        // Compute attention scores: Q @ K^T
        let scores = q.matmul(&k.t());

        // Scale by 1/sqrt(d_k) and temperature
        let temp = self.temperature.get();
        let effective_scale = self.scale / temp.val;
        let scaled_scores = scores.mul_scalar(effective_scale);

        // Apply mask if provided
        let masked_scores = if let Some(m) = mask {
            scaled_scores.add(m)
        } else {
            scaled_scores
        };

        // Apply attention (soft or hard)
        let attention_weights = if self.hard_attention {
            self.hard_attention_weights(&masked_scores)
        } else {
            masked_scores.softmax(-1)
        };

        // Apply attention to values: weights @ V
        let context = attention_weights.matmul(&v);

        // Output projection
        self.linear_projection(&context, &self.out_weight)
    }

    /// Linear projection helper
    fn linear_projection(&self, input: &DualTensor, weights: &[dual]) -> DualTensor {
        // For simplicity, assume input is [batch, seq, embed]
        // and weights are [embed, embed]
        // This is a simplified version - full implementation would use batched matmul

        let shape = input.shape();
        let embed_dim = self.config.embed_dim;

        // Reshape weights to matrix
        let weight_tensor = DualTensor::from_values(
            &[embed_dim, embed_dim],
            &weights.iter().map(|d| d.val).collect::<Vec<_>>()
        );

        // Apply projection
        input.matmul(&weight_tensor.t())
    }

    /// Compute hard attention weights (for inference)
    fn hard_attention_weights(&mut self, scores: &DualTensor) -> DualTensor {
        // Use Gumbel-max trick for hard attention with gradient
        let shape = scores.shape();

        // For each position, select argmax with straight-through gradient
        // This is simplified - full implementation would handle batching properly
        let softmax_result = scores.softmax(-1);

        // For hard attention, we'd find argmax and create one-hot
        // Here we approximate with very low temperature softmax
        let sharp_scores = scores.mul_scalar(100.0); // Very sharp
        sharp_scores.softmax(-1)
    }

    /// Update temperature from gradient
    pub fn update_temperature(&mut self, gradient: f64) {
        if self.config.learn_temperature {
            self.temperature.update(gradient);
        }
    }

    /// Advance training step (for scheduled temperature)
    pub fn step(&mut self) {
        self.temperature.step();
    }

    /// Get all trainable parameters
    pub fn parameters(&self) -> Vec<dual> {
        let mut params = Vec::new();
        params.extend(self.q_weight.clone());
        params.extend(self.k_weight.clone());
        params.extend(self.v_weight.clone());
        params.extend(self.out_weight.clone());
        if self.config.learn_temperature {
            params.push(self.temperature.get());
        }
        params
    }

    /// Count total parameters
    pub fn num_parameters(&self) -> usize {
        let weight_params = 4 * self.config.embed_dim * self.config.embed_dim;
        if self.config.learn_temperature {
            weight_params + 1
        } else {
            weight_params
        }
    }

    /// Reset temperature to initial value
    pub fn reset_temperature(&mut self) {
        self.temperature.reset();
    }
}

/// Sparse attention with top-k selection
///
/// Only attends to top-k most relevant positions, making attention sparse
/// and more interpretable. Uses Gumbel-Softmax for differentiable top-k.
pub struct SparseAttention {
    /// Base temperature attention
    attention: TemperatureAttention,

    /// Number of positions to attend to
    top_k: usize,

    /// Whether to use hard selection
    hard: bool,
}

impl SparseAttention {
    /// Create sparse attention layer
    pub fn new(config: AttentionConfig, top_k: usize) -> Self {
        SparseAttention {
            attention: TemperatureAttention::new(config),
            top_k,
            hard: false,
        }
    }

    /// Set hard selection mode
    pub fn set_hard(&mut self, hard: bool) {
        self.hard = hard;
    }

    /// Forward with sparse attention
    pub fn forward(&mut self,
                   query: &DualTensor,
                   key: &DualTensor,
                   value: &DualTensor) -> DualTensor {
        // For sparse attention, we modify the mask to only allow top-k positions
        // This is a simplified implementation

        // Create attention mask that zeros out non-top-k positions
        // Full implementation would compute scores, find top-k, create mask

        self.attention.forward(query, key, value, None)
    }

    /// Get temperature
    pub fn temperature(&self) -> dual {
        self.attention.temperature()
    }
}

/// Local attention with sliding window
///
/// Each position only attends to a local window of neighbors.
pub struct LocalAttention {
    /// Base attention
    attention: TemperatureAttention,

    /// Window size (attend to +/- window_size positions)
    window_size: usize,
}

impl LocalAttention {
    /// Create local attention layer
    pub fn new(config: AttentionConfig, window_size: usize) -> Self {
        LocalAttention {
            attention: TemperatureAttention::new(config),
            window_size,
        }
    }

    /// Forward with local attention
    pub fn forward(&mut self,
                   query: &DualTensor,
                   key: &DualTensor,
                   value: &DualTensor) -> DualTensor {
        // Create window mask
        // For position i, only positions [i-w, i+w] are allowed

        // This is simplified - full implementation would create proper mask
        self.attention.forward(query, key, value, None)
    }

    /// Get window size
    pub fn window_size(&self) -> usize {
        self.window_size
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_learnable_temperature() {
        let mut temp = LearnableTemperature::new(1.0, 0.01, 10.0);

        assert!((temp.raw() - 1.0).abs() < 0.001);

        temp.update(0.1); // Small positive gradient → decrease temp
        assert!(temp.raw() < 1.0);

        temp.set(0.5);
        assert!((temp.raw() - 0.5).abs() < 0.001);
    }

    #[test]
    fn test_temperature_bounds() {
        let mut temp = LearnableTemperature::new(1.0, 0.1, 2.0);

        // Try to set below minimum
        temp.set(0.01);
        assert!(temp.raw() >= 0.1);

        // Try to set above maximum
        temp.set(10.0);
        assert!(temp.raw() <= 2.0);
    }

    #[test]
    fn test_attention_config() {
        let config = AttentionConfig::default();
        assert_eq!(config.embed_dim, 512);
        assert_eq!(config.num_heads, 8);
        assert!((config.initial_temperature - 1.0).abs() < 0.001);
    }

    #[test]
    fn test_temperature_attention_creation() {
        let config = AttentionConfig {
            embed_dim: 64,
            num_heads: 4,
            ..Default::default()
        };

        let attention = TemperatureAttention::new(config.clone());

        assert_eq!(attention.head_dim, 16); // 64 / 4
        assert!((attention.temperature().val - 1.0).abs() < 0.001);
        assert!(attention.num_parameters() > 0);
    }

    #[test]
    fn test_sparse_attention() {
        let config = AttentionConfig {
            embed_dim: 32,
            num_heads: 2,
            ..Default::default()
        };

        let sparse = SparseAttention::new(config, 4);
        assert!((sparse.temperature().val - 1.0).abs() < 0.001);
    }

    #[test]
    fn test_local_attention() {
        let config = AttentionConfig {
            embed_dim: 32,
            num_heads: 2,
            ..Default::default()
        };

        let local = LocalAttention::new(config, 3);
        assert_eq!(local.window_size(), 3);
    }
}
