// simplex-training - Self-Learning Training Pipeline
//
// Implements the training infrastructure for Simplex models with
// self-learning schedules that discover optimal hyperparameters
// through meta-gradient optimization.
//
// # Key Components
//
// - `schedules`: Learnable LR, distillation, pruning, quantization schedules
// - `trainer`: Meta-training framework for specialists
// - `compress`: Model compression pipeline (prune, quantize, distill)
// - `export`: GGUF export for deployment
//
// # Example
//
// ```simplex
// use simplex_training::{MetaTrainer, MetaConfig, SpecialistConfig};
//
// // Create meta-trainer with learnable schedules
// let trainer = MetaTrainer::new(MetaConfig::default())
//     .with_learnable_lr()
//     .with_learnable_distillation()
//     .with_learnable_pruning();
//
// // Train all specialists
// let specialists = trainer.train_all(&base_model, &configs);
// ```
//
// # Architecture
//
// Training follows a meta-learning approach where schedule parameters
// are treated as learnable variables. Each training epoch produces
// gradients that update not just model weights, but also the schedules
// that govern learning rate, distillation temperature, pruning rate,
// and quantization precision.

use simplex_std::String;

pub mod schedules;
pub mod trainer;
pub mod compress;
pub mod export;
pub mod neural;
pub mod layers;
pub mod lora;
pub mod data;
pub mod pipeline;
pub mod research;

// Re-exports for convenience
pub use schedules::{
    LearnableLRSchedule,
    LearnableDistillation,
    LearnablePruning,
    LearnableQuantization,
    LearnedSchedules,
};

pub use trainer::{
    MetaTrainer,
    MetaConfig,
    MetaLoss,
    SpecialistTrainer,
    SpecialistConfig,
    StepResult,
    TrainingHistory,
    Checkpoint,
    EarlyStopping,
};

pub use compress::{
    CompressionPipeline,
    CompressionConfig,
    CompressionResult,
    PruningPipeline,
    PruningConfig,
    PruningResult,
    QuantizationPipeline,
    QuantConfig,
    QuantResult,
};

pub use export::{
    GgufExporter,
    GgufConfig,
    GgufTensor,
    GgufMetadata,
    ExportConfig,
    ExportFormat,
    ExportTensor,
    ExportResult,
    TensorDtype,
};

pub use neural::{
    DualGate,
    GateMode,
    GumbelSoftmax,
    StraightThroughEstimator,
    SoftLogic,
    SoftLogicGate,
    TemperatureAttention,
    LearnableTemperature,
};

pub use layers::{
    Linear,
    LinearConfig,
    MultiHeadAttention,
    Embedding,
    PositionalEmbedding,
    RotaryEmbedding,
    LayerNorm,
    RMSNorm,
};

pub use lora::{
    LoRALayer,
    LoRAConfig,
    LoRAModel,
    LoRAAdapter,
};

pub use data::{
    DataGenerator,
    TrainingExample,
    DataLoader,
    DataLoaderConfig,
    Batch,
    DocumentGenerator,
    CodeGenerator,
    SentimentGenerator,
    ReasoningGenerator,
    NeuralIRGenerator,
    ClassificationGenerator,
};

pub use research::{
    ResearchSpecialist,
    ResearchConfig,
    ResearchResult,
    DataValidator,
    ValidationResult,
    DatasetAnnealer,
    AnnealedExample,
    DataRefiner,
    RefinerConfig,
    RefinementReport,
    SourceRegistry,
    SourceType,
    refine_training_data,
    quick_refine,
};

// =============================================================================
// Core Types
// =============================================================================

/// Model size variants
#[derive(Clone, Copy)]
pub enum ModelSize {
    /// 70 billion parameters (teacher)
    B70,
    /// 8 billion parameters (large student)
    B8,
    /// 3 billion parameters (medium student)
    B3,
    /// 1 billion parameters (small student)
    B1,
    /// 500 million parameters (tiny student)
    M500,
    /// Small model (~500M-1B)
    Small,
    /// Medium model (~3B)
    Medium,
    /// Large model (~8B)
    Large,
}

impl ModelSize {
    /// Approximate parameter count
    pub fn params(&self) -> i64 {
        match self {
            ModelSize::B70 => 70_000_000_000,
            ModelSize::B8 => 8_000_000_000,
            ModelSize::B3 => 3_000_000_000,
            ModelSize::B1 => 1_000_000_000,
            ModelSize::M500 => 500_000_000,
            ModelSize::Small => 500_000_000,
            ModelSize::Medium => 3_000_000_000,
            ModelSize::Large => 8_000_000_000,
        }
    }

    /// Memory footprint in bytes (fp16)
    pub fn memory_bytes(&self) -> i64 {
        self.params() * 2  // 2 bytes per fp16 parameter
    }

    /// Get name as string
    pub fn name(&self) -> &str {
        match self {
            ModelSize::B70 => "70B",
            ModelSize::B8 => "8B",
            ModelSize::B3 => "3B",
            ModelSize::B1 => "1B",
            ModelSize::M500 => "500M",
            ModelSize::Small => "small",
            ModelSize::Medium => "medium",
            ModelSize::Large => "large",
        }
    }
}

/// Training configuration
#[derive(Clone)]
pub struct TrainingConfig {
    /// Batch size
    pub batch_size: i64,
    /// Number of epochs
    pub epochs: i64,
    /// Maximum learning rate
    pub max_lr: f64,
    /// Weight decay
    pub weight_decay: f64,
    /// Gradient accumulation steps
    pub gradient_accumulation: i64,
    /// Mixed precision training
    pub mixed_precision: bool,
    /// Checkpoint interval (steps)
    pub checkpoint_interval: i64,
}

impl TrainingConfig {
    /// Default configuration for 8B models
    pub fn default_8b() -> TrainingConfig {
        TrainingConfig {
            batch_size: 8,
            epochs: 3,
            max_lr: 2e-4,
            weight_decay: 0.01,
            gradient_accumulation: 4,
            mixed_precision: true,
            checkpoint_interval: 1000,
        }
    }

    /// Default configuration for 3B models
    pub fn default_3b() -> TrainingConfig {
        TrainingConfig {
            batch_size: 16,
            epochs: 3,
            max_lr: 3e-4,
            weight_decay: 0.01,
            gradient_accumulation: 2,
            mixed_precision: true,
            checkpoint_interval: 500,
        }
    }

    /// Default configuration for 1B models
    pub fn default_1b() -> TrainingConfig {
        TrainingConfig {
            batch_size: 32,
            epochs: 5,
            max_lr: 5e-4,
            weight_decay: 0.01,
            gradient_accumulation: 1,
            mixed_precision: true,
            checkpoint_interval: 250,
        }
    }
}

impl Default for TrainingConfig {
    fn default() -> TrainingConfig {
        TrainingConfig::default_8b()
    }
}

/// Specialist domain
#[derive(Clone)]
pub enum SpecialistDomain {
    /// Code generation and understanding
    Code,
    /// Mathematical reasoning
    Math,
    /// General reasoning and logic
    Reasoning,
    /// Document processing
    Document,
    /// Data extraction
    Extraction,
    /// Classification tasks
    Classification,
    /// Summarization
    Summarization,
    /// Translation
    Translation,
    /// Custom domain
    Custom(String),
}

impl SpecialistDomain {
    /// Get domain name as string
    pub fn name(&self) -> String {
        match self {
            SpecialistDomain::Code => "code".to_string(),
            SpecialistDomain::Math => "math".to_string(),
            SpecialistDomain::Reasoning => "reasoning".to_string(),
            SpecialistDomain::Document => "document".to_string(),
            SpecialistDomain::Extraction => "extraction".to_string(),
            SpecialistDomain::Classification => "classification".to_string(),
            SpecialistDomain::Summarization => "summarization".to_string(),
            SpecialistDomain::Translation => "translation".to_string(),
            SpecialistDomain::Custom(s) => s.clone(),
        }
    }
}
