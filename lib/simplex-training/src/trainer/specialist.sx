// simplex-training::trainer::specialist - Domain-Specific Specialist Training
//
// Implements training for specialist models:
// - Domain-focused fine-tuning
// - Knowledge distillation from teacher
// - Progressive compression (pruning + quantization)
// - Multi-task curriculum learning

use simplex_std::dual::dual;
use simplex_std::{Clone, Default, Vec};
use crate::schedules::LearnedSchedules;
use crate::{ModelSize, SpecialistDomain};
use super::{StepResult, TrainingHistory, Checkpoint, EarlyStopping};

/// Configuration for specialist training
#[derive(Clone)]
pub struct SpecialistConfig {
    /// Target model size
    pub target_size: ModelSize,
    /// Specialist domain
    pub domain: SpecialistDomain,
    /// Total training steps
    pub total_steps: u64,
    /// Batch size
    pub batch_size: u64,
    /// Gradient accumulation steps
    pub grad_accum: u64,
    /// Validation frequency (steps)
    pub val_every: u64,
    /// Checkpoint frequency (steps)
    pub checkpoint_every: u64,
    /// Enable distillation
    pub use_distillation: bool,
    /// Enable progressive compression
    pub use_compression: bool,
    /// Gradient clipping
    pub grad_clip: f64,
    /// Early stopping patience
    pub patience: u64,
}

impl SpecialistConfig {
    pub fn new(domain: SpecialistDomain, target_size: ModelSize) -> SpecialistConfig {
        SpecialistConfig {
            target_size,
            domain,
            total_steps: 10000,
            batch_size: 32,
            grad_accum: 1,
            val_every: 100,
            checkpoint_every: 1000,
            use_distillation: true,
            use_compression: true,
            grad_clip: 1.0,
            patience: 20,
        }
    }

    /// Preset for fast iteration
    pub fn fast(domain: SpecialistDomain) -> SpecialistConfig {
        var config = SpecialistConfig::new(domain, ModelSize::Small);
        config.total_steps = 1000;
        config.val_every = 50;
        config
    }

    /// Preset for quality training
    pub fn quality(domain: SpecialistDomain) -> SpecialistConfig {
        var config = SpecialistConfig::new(domain, ModelSize::Medium);
        config.total_steps = 50000;
        config.batch_size = 64;
        config.grad_accum = 4;
        config
    }

    /// Preset for production models
    pub fn production(domain: SpecialistDomain) -> SpecialistConfig {
        var config = SpecialistConfig::new(domain, ModelSize::Large);
        config.total_steps = 100000;
        config.batch_size = 128;
        config.grad_accum = 8;
        config.use_distillation = true;
        config.use_compression = true;
        config
    }
}

impl Default for SpecialistConfig {
    fn default() -> SpecialistConfig {
        SpecialistConfig::new(SpecialistDomain::Code, ModelSize::Medium)
    }
}

/// Phase of specialist training
#[derive(Clone, Copy, PartialEq)]
pub enum TrainingPhase {
    /// Initial warmup phase
    Warmup,
    /// Main training phase
    Training,
    /// Distillation from teacher
    Distillation,
    /// Compression (pruning/quantization)
    Compression,
    /// Final fine-tuning
    FineTuning,
}

impl TrainingPhase {
    pub fn name(&self) -> &str {
        match self {
            TrainingPhase::Warmup => "warmup",
            TrainingPhase::Training => "training",
            TrainingPhase::Distillation => "distillation",
            TrainingPhase::Compression => "compression",
            TrainingPhase::FineTuning => "fine_tuning",
        }
    }
}

/// Specialist trainer state
#[derive(Clone)]
pub struct SpecialistState {
    /// Current step
    pub step: u64,
    /// Current epoch
    pub epoch: u64,
    /// Current phase
    pub phase: TrainingPhase,
    /// Training history
    pub history: TrainingHistory,
    /// Best validation loss
    pub best_val_loss: f64,
    /// Best checkpoint step
    pub best_step: u64,
    /// Accumulated gradients (for gradient accumulation)
    pub accum_count: u64,
}

impl SpecialistState {
    pub fn new() -> SpecialistState {
        SpecialistState {
            step: 0,
            epoch: 0,
            phase: TrainingPhase::Warmup,
            history: TrainingHistory::new(),
            best_val_loss: f64::MAX,
            best_step: 0,
            accum_count: 0,
        }
    }

    pub fn update_best(&mut self, val_loss: f64) -> bool {
        if val_loss < self.best_val_loss {
            self.best_val_loss = val_loss;
            self.best_step = self.step;
            true
        } else {
            false
        }
    }

    pub fn should_validate(&self, config: &SpecialistConfig) -> bool {
        self.step > 0 && self.step % config.val_every == 0
    }

    pub fn should_checkpoint(&self, config: &SpecialistConfig) -> bool {
        self.step > 0 && self.step % config.checkpoint_every == 0
    }
}

impl Default for SpecialistState {
    fn default() -> SpecialistState {
        SpecialistState::new()
    }
}

/// Specialist trainer
pub struct SpecialistTrainer {
    /// Configuration
    pub config: SpecialistConfig,
    /// Training state
    pub state: SpecialistState,
    /// Learned schedules
    pub schedules: LearnedSchedules,
    /// Early stopping
    pub early_stopping: EarlyStopping,
}

impl SpecialistTrainer {
    /// Create new specialist trainer
    pub fn new(config: SpecialistConfig) -> SpecialistTrainer {
        let patience = config.patience;
        SpecialistTrainer {
            config,
            state: SpecialistState::new(),
            schedules: LearnedSchedules::new(),
            early_stopping: EarlyStopping::new(patience, 1e-5),
        }
    }

    /// Get current phase based on training progress
    fn compute_phase(&self) -> TrainingPhase {
        let progress = self.state.step as f64 / self.config.total_steps as f64;

        if progress < 0.05 {
            TrainingPhase::Warmup
        } else if progress < 0.4 {
            TrainingPhase::Training
        } else if progress < 0.6 && self.config.use_distillation {
            TrainingPhase::Distillation
        } else if progress < 0.9 && self.config.use_compression {
            TrainingPhase::Compression
        } else {
            TrainingPhase::FineTuning
        }
    }

    /// Perform single training step
    pub fn step<F>(&mut self, mut forward_backward: F) -> StepResult
    where
        F: FnMut(f64, TrainingPhase) -> (dual, f64),  // (lr, phase) -> (loss, grad_norm)
    {
        // Update phase
        self.state.phase = self.compute_phase();

        // Get learning rate
        let step_dual = dual::constant(self.state.step as f64);
        let lr = self.schedules.learning_rate(step_dual,
            self.state.history.losses.as_slice());

        // Forward/backward pass
        let (loss, grad_norm) = forward_backward(lr.val, self.state.phase);

        // Create result
        let result = StepResult::new(loss, self.state.step, lr.val)
            .with_grad_norm(grad_norm);

        // Record history
        self.state.history.record(&result);
        self.state.step = self.state.step + 1;

        result
    }

    /// Perform validation
    pub fn validate<F>(&mut self, mut val_fn: F) -> f64
    where
        F: FnMut() -> f64,
    {
        let val_loss = val_fn();

        // Update best
        if self.state.update_best(val_loss) {
            // New best - would save checkpoint here
        }

        // Early stopping check
        self.early_stopping.check(val_loss);

        val_loss
    }

    /// Check if should stop training
    pub fn should_stop(&self) -> bool {
        self.state.step >= self.config.total_steps ||
        (self.state.step > self.state.best_step + self.config.patience)
    }

    /// Get distillation parameters for current step
    pub fn distillation_params(&self) -> DistillationParams {
        let step = dual::constant(self.state.step as f64);
        DistillationParams {
            temperature: self.schedules.distill_temperature(step).val,
            alpha: self.schedules.distill.alpha(step).val,
            enabled: self.state.phase == TrainingPhase::Distillation ||
                     self.config.use_distillation,
        }
    }

    /// Get compression parameters for current step
    pub fn compression_params(&self) -> CompressionParams {
        let step = dual::constant(self.state.step as f64);
        CompressionParams {
            sparsity: self.schedules.sparsity(step).val,
            bits: self.schedules.bits(step).val,
            enabled: self.state.phase == TrainingPhase::Compression ||
                     self.config.use_compression,
        }
    }

    /// Train loop (simplified)
    pub fn train<F, V>(&mut self, mut train_step: F, mut val_fn: V)
    where
        F: FnMut(f64, TrainingPhase) -> (dual, f64),
        V: FnMut() -> f64,
    {
        while !self.should_stop() {
            // Training step
            let result = self.step(&mut train_step);

            // Validation
            if self.state.should_validate(&self.config) {
                self.validate(&mut val_fn);
            }

            // Could add checkpoint saving here
        }
    }

    /// Get current progress (0.0 to 1.0)
    pub fn progress(&self) -> f64 {
        self.state.step as f64 / self.config.total_steps as f64
    }

    /// Get training summary
    pub fn summary(&self) -> TrainingSummary {
        TrainingSummary {
            total_steps: self.state.step,
            final_train_loss: self.state.history.moving_average(10),
            best_val_loss: self.state.best_val_loss,
            best_step: self.state.best_step,
            domain: self.config.domain,
            target_size: self.config.target_size,
        }
    }
}

impl Default for SpecialistTrainer {
    fn default() -> SpecialistTrainer {
        SpecialistTrainer::new(SpecialistConfig::default())
    }
}

/// Parameters for distillation at current step
#[derive(Clone)]
pub struct DistillationParams {
    /// Temperature for soft targets
    pub temperature: f64,
    /// Soft/hard mixing ratio
    pub alpha: f64,
    /// Whether distillation is enabled
    pub enabled: bool,
}

/// Parameters for compression at current step
#[derive(Clone)]
pub struct CompressionParams {
    /// Target sparsity
    pub sparsity: f64,
    /// Target bit-width
    pub bits: f64,
    /// Whether compression is enabled
    pub enabled: bool,
}

/// Training summary
#[derive(Clone)]
pub struct TrainingSummary {
    /// Total steps completed
    pub total_steps: u64,
    /// Final training loss (moving average)
    pub final_train_loss: f64,
    /// Best validation loss achieved
    pub best_val_loss: f64,
    /// Step of best validation
    pub best_step: u64,
    /// Specialist domain
    pub domain: SpecialistDomain,
    /// Target model size
    pub target_size: ModelSize,
}
