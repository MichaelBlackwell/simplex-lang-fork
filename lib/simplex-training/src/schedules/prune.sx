// simplex-training::schedules::prune - Learnable Pruning Schedule
//
// Implements differentiable pruning with:
// - Learnable sparsity schedule
// - Importance scoring (magnitude, gradient, activation)
// - Layer-wise sensitivity
// - Recovery phases

use simplex_std::dual::dual;
use simplex_std::{Clone, Default, Vec};

/// Gradient for pruning parameters
#[derive(Clone)]
pub struct PruneGradient {
    pub d_initial_sparsity: f64,
    pub d_final_sparsity: f64,
    pub d_pruning_rate: f64,
    pub d_magnitude_weight: f64,
    pub d_gradient_weight: f64,
    pub d_recovery_threshold: f64,
}

impl PruneGradient {
    pub fn scale(&mut self, factor: f64) {
        self.d_initial_sparsity = self.d_initial_sparsity * factor;
        self.d_final_sparsity = self.d_final_sparsity * factor;
        self.d_pruning_rate = self.d_pruning_rate * factor;
        self.d_magnitude_weight = self.d_magnitude_weight * factor;
        self.d_gradient_weight = self.d_gradient_weight * factor;
        self.d_recovery_threshold = self.d_recovery_threshold * factor;
    }

    pub fn norm(&self) -> f64 {
        (self.d_initial_sparsity * self.d_initial_sparsity +
         self.d_final_sparsity * self.d_final_sparsity +
         self.d_pruning_rate * self.d_pruning_rate +
         self.d_magnitude_weight * self.d_magnitude_weight +
         self.d_gradient_weight * self.d_gradient_weight +
         self.d_recovery_threshold * self.d_recovery_threshold).sqrt()
    }
}

impl Default for PruneGradient {
    fn default() -> PruneGradient {
        PruneGradient {
            d_initial_sparsity: 0.0,
            d_final_sparsity: 0.0,
            d_pruning_rate: 0.0,
            d_magnitude_weight: 0.0,
            d_gradient_weight: 0.0,
            d_recovery_threshold: 0.0,
        }
    }
}

/// Learnable pruning schedule
#[derive(Clone)]
pub struct LearnablePruning {
    /// Starting sparsity (usually 0)
    pub initial_sparsity: dual,
    /// Target final sparsity
    pub final_sparsity: dual,
    /// Rate of pruning progression
    pub pruning_rate: dual,
    /// Weight for magnitude importance
    pub magnitude_weight: dual,
    /// Weight for gradient importance
    pub gradient_weight: dual,
    /// Loss increase threshold for recovery
    pub recovery_threshold: dual,
}

impl LearnablePruning {
    /// Create with default values
    pub fn new() -> LearnablePruning {
        LearnablePruning {
            initial_sparsity: dual::constant(0.0),
            final_sparsity: dual::variable(0.5),  // 50% pruning target
            pruning_rate: dual::variable(0.0001),
            magnitude_weight: dual::variable(1.0),
            gradient_weight: dual::variable(0.1),
            recovery_threshold: dual::variable(0.1),
        }
    }

    /// Get target sparsity at step
    pub fn target_sparsity(&self, step: dual) -> dual {
        let progress = dual::constant(1.0) - (dual::zero() - self.pruning_rate * step).exp();
        self.initial_sparsity + (self.final_sparsity - self.initial_sparsity) * progress
    }

    /// Compute importance score for weights (differentiable)
    ///
    /// Combines magnitude and gradient information
    pub fn importance_score(
        &self,
        weight: dual,
        gradient: f64
    ) -> dual {
        let mag_term = self.magnitude_weight * weight.abs();
        let grad_term = self.gradient_weight * dual::constant(gradient.abs());
        mag_term + grad_term
    }

    /// Compute soft pruning mask (differentiable)
    ///
    /// Returns mask in [0, 1] where 0 = pruned, 1 = kept
    pub fn compute_mask(
        &self,
        weights: &[dual],
        gradients: &[f64],
        step: dual
    ) -> Vec<dual> {
        // Get target sparsity
        let target = self.target_sparsity(step);

        // Compute importance scores
        var scores = Vec::new();
        for i in 0..weights.len() {
            let grad = if i < gradients.len() { gradients[i] } else { 0.0 };
            scores.push(self.importance_score(weights[i], grad));
        }

        // Find threshold for target sparsity
        var sorted_scores: Vec<f64> = Vec::new();
        for s in scores.iter() {
            sorted_scores.push(s.val);
        }
        sorted_scores.sort();

        let threshold_idx = ((target.val * (sorted_scores.len() as f64)) as usize)
            .min(sorted_scores.len() - 1);
        let threshold = sorted_scores[threshold_idx];

        // Compute soft mask (sigmoid around threshold)
        var mask = Vec::new();
        for s in scores.iter() {
            let diff = *s - dual::constant(threshold);
            let soft_mask = (diff * dual::constant(10.0)).sigmoid();  // Sharpness = 10
            mask.push(soft_mask);
        }

        mask
    }

    /// Check if should pause pruning (recovery phase)
    pub fn should_recover(&self, loss_increase: f64) -> bool {
        loss_increase > self.recovery_threshold.val
    }

    /// Extract gradients
    pub fn gradient(&self) -> PruneGradient {
        PruneGradient {
            d_initial_sparsity: self.initial_sparsity.der,
            d_final_sparsity: self.final_sparsity.der,
            d_pruning_rate: self.pruning_rate.der,
            d_magnitude_weight: self.magnitude_weight.der,
            d_gradient_weight: self.gradient_weight.der,
            d_recovery_threshold: self.recovery_threshold.der,
        }
    }

    /// Update parameters
    pub fn update(&mut self, grad: &PruneGradient, lr: f64) {
        self.final_sparsity = dual::new(
            self.final_sparsity.val - lr * grad.d_final_sparsity,
            1.0
        );
        self.pruning_rate = dual::new(
            self.pruning_rate.val - lr * grad.d_pruning_rate,
            1.0
        );
        self.magnitude_weight = dual::new(
            self.magnitude_weight.val - lr * grad.d_magnitude_weight,
            1.0
        );
        self.gradient_weight = dual::new(
            self.gradient_weight.val - lr * grad.d_gradient_weight,
            1.0
        );
        self.recovery_threshold = dual::new(
            self.recovery_threshold.val - lr * grad.d_recovery_threshold,
            1.0
        );

        // Clamp to valid ranges
        if self.final_sparsity.val < 0.0 {
            self.final_sparsity = dual::new(0.0, self.final_sparsity.der);
        }
        if self.final_sparsity.val > 0.9 {
            self.final_sparsity = dual::new(0.9, self.final_sparsity.der);
        }
        if self.magnitude_weight.val < 0.0 {
            self.magnitude_weight = dual::new(0.0, self.magnitude_weight.der);
        }
        if self.gradient_weight.val < 0.0 {
            self.gradient_weight = dual::new(0.0, self.gradient_weight.der);
        }
    }

    /// L2 norm for regularization
    pub fn l2_norm(&self) -> dual {
        self.final_sparsity * self.final_sparsity +
        self.pruning_rate * self.pruning_rate
    }
}

impl Default for LearnablePruning {
    fn default() -> LearnablePruning {
        LearnablePruning::new()
    }
}

// Sort helper
trait SortVec {
    fn sort(&mut self);
}

impl SortVec for Vec<f64> {
    fn sort(&mut self) {
        // Simple bubble sort for now
        let n = self.len();
        for i in 0..n {
            for j in 0..(n - i - 1) {
                if self[j] > self[j + 1] {
                    let tmp = self[j];
                    self[j] = self[j + 1];
                    self[j + 1] = tmp;
                }
            }
        }
    }
}
