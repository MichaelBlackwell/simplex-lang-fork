// simplex-training::schedules::lr - Learnable Learning Rate Schedule
//
// Implements a differentiable learning rate schedule with:
// - Warmup phase
// - Exponential/cosine decay
// - Plateau detection and boosting
// - Oscillation for escaping local minima
//
// All parameters are dual numbers for meta-gradient computation.

use simplex_std::dual::dual;
use simplex_std::{Clone, Default, Vec};

/// Gradient for LR schedule parameters
#[derive(Clone)]
pub struct LRGradient {
    pub d_initial_lr: f64,
    pub d_decay_rate: f64,
    pub d_warmup_steps: f64,
    pub d_min_lr: f64,
    pub d_oscillation_amp: f64,
    pub d_oscillation_freq: f64,
    pub d_plateau_threshold: f64,
    pub d_plateau_boost: f64,
}

impl LRGradient {
    pub fn scale(&mut self, factor: f64) {
        self.d_initial_lr = self.d_initial_lr * factor;
        self.d_decay_rate = self.d_decay_rate * factor;
        self.d_warmup_steps = self.d_warmup_steps * factor;
        self.d_min_lr = self.d_min_lr * factor;
        self.d_oscillation_amp = self.d_oscillation_amp * factor;
        self.d_oscillation_freq = self.d_oscillation_freq * factor;
        self.d_plateau_threshold = self.d_plateau_threshold * factor;
        self.d_plateau_boost = self.d_plateau_boost * factor;
    }

    pub fn norm(&self) -> f64 {
        (self.d_initial_lr * self.d_initial_lr +
         self.d_decay_rate * self.d_decay_rate +
         self.d_warmup_steps * self.d_warmup_steps +
         self.d_min_lr * self.d_min_lr +
         self.d_oscillation_amp * self.d_oscillation_amp +
         self.d_oscillation_freq * self.d_oscillation_freq +
         self.d_plateau_threshold * self.d_plateau_threshold +
         self.d_plateau_boost * self.d_plateau_boost).sqrt()
    }
}

impl Default for LRGradient {
    fn default() -> LRGradient {
        LRGradient {
            d_initial_lr: 0.0,
            d_decay_rate: 0.0,
            d_warmup_steps: 0.0,
            d_min_lr: 0.0,
            d_oscillation_amp: 0.0,
            d_oscillation_freq: 0.0,
            d_plateau_threshold: 0.0,
            d_plateau_boost: 0.0,
        }
    }
}

/// Learnable LR schedule with meta-gradient support
#[derive(Clone)]
pub struct LearnableLRSchedule {
    /// Starting learning rate
    pub initial_lr: dual,
    /// Exponential decay rate
    pub decay_rate: dual,
    /// Warmup duration (steps)
    pub warmup_steps: dual,
    /// Floor learning rate
    pub min_lr: dual,
    /// Oscillation amplitude (for escaping plateaus)
    pub oscillation_amp: dual,
    /// Oscillation frequency
    pub oscillation_freq: dual,
    /// Loss stagnation threshold
    pub plateau_threshold: dual,
    /// LR boost when stuck on plateau
    pub plateau_boost: dual,
}

impl LearnableLRSchedule {
    /// Create with default values suitable for most training
    pub fn new() -> LearnableLRSchedule {
        LearnableLRSchedule {
            initial_lr: dual::variable(2e-4),
            decay_rate: dual::variable(0.0001),
            warmup_steps: dual::variable(100.0),
            min_lr: dual::constant(1e-6),
            oscillation_amp: dual::variable(0.0),
            oscillation_freq: dual::variable(0.01),
            plateau_threshold: dual::variable(0.001),
            plateau_boost: dual::variable(1.5),
        }
    }

    /// Compute learning rate at given step
    ///
    /// LR = initial_lr * warmup_factor * decay * oscillation * plateau_boost
    pub fn learning_rate(&self, step: dual, loss_history: &[f64]) -> dual {
        // Warmup: ramp from 0 to 1 over warmup_steps
        let warmup_progress = step / self.warmup_steps;
        let warmup_factor = warmup_progress.min(dual::constant(1.0));

        // Decay: exponential decay
        let decay = (dual::zero() - self.decay_rate * step).exp();

        // Oscillation: sinusoidal perturbation
        let osc = dual::constant(1.0) +
                  self.oscillation_amp * (self.oscillation_freq * step).sin();

        // Plateau detection: boost if loss stagnated
        let plateau_factor = if loss_history.len() >= 10 {
            let recent_variance = compute_variance(&loss_history[loss_history.len()-10..]);
            if recent_variance < self.plateau_threshold.val {
                self.plateau_boost
            } else {
                dual::constant(1.0)
            }
        } else {
            dual::constant(1.0)
        };

        // Combined LR
        let lr = self.initial_lr * warmup_factor * decay * osc * plateau_factor;
        lr.max(self.min_lr)
    }

    /// Extract gradients from parameters
    pub fn gradient(&self) -> LRGradient {
        LRGradient {
            d_initial_lr: self.initial_lr.der,
            d_decay_rate: self.decay_rate.der,
            d_warmup_steps: self.warmup_steps.der,
            d_min_lr: self.min_lr.der,
            d_oscillation_amp: self.oscillation_amp.der,
            d_oscillation_freq: self.oscillation_freq.der,
            d_plateau_threshold: self.plateau_threshold.der,
            d_plateau_boost: self.plateau_boost.der,
        }
    }

    /// Update parameters with gradient descent
    pub fn update(&mut self, grad: &LRGradient, lr: f64) {
        self.initial_lr = dual::new(
            self.initial_lr.val - lr * grad.d_initial_lr,
            1.0
        );
        self.decay_rate = dual::new(
            self.decay_rate.val - lr * grad.d_decay_rate,
            1.0
        );
        self.warmup_steps = dual::new(
            self.warmup_steps.val - lr * grad.d_warmup_steps,
            1.0
        );
        self.oscillation_amp = dual::new(
            self.oscillation_amp.val - lr * grad.d_oscillation_amp,
            1.0
        );
        self.oscillation_freq = dual::new(
            self.oscillation_freq.val - lr * grad.d_oscillation_freq,
            1.0
        );
        self.plateau_threshold = dual::new(
            self.plateau_threshold.val - lr * grad.d_plateau_threshold,
            1.0
        );
        self.plateau_boost = dual::new(
            self.plateau_boost.val - lr * grad.d_plateau_boost,
            1.0
        );

        // Clamp to valid ranges
        self.clamp_parameters();
    }

    fn clamp_parameters(&mut self) {
        if self.initial_lr.val < 1e-7 {
            self.initial_lr = dual::new(1e-7, self.initial_lr.der);
        }
        if self.initial_lr.val > 1e-2 {
            self.initial_lr = dual::new(1e-2, self.initial_lr.der);
        }
        if self.decay_rate.val < 0.0 {
            self.decay_rate = dual::new(0.0, self.decay_rate.der);
        }
        if self.warmup_steps.val < 1.0 {
            self.warmup_steps = dual::new(1.0, self.warmup_steps.der);
        }
        if self.plateau_boost.val < 1.0 {
            self.plateau_boost = dual::new(1.0, self.plateau_boost.der);
        }
    }

    /// L2 norm for regularization
    pub fn l2_norm(&self) -> dual {
        self.initial_lr * self.initial_lr +
        self.decay_rate * self.decay_rate +
        self.oscillation_amp * self.oscillation_amp
    }
}

impl Default for LearnableLRSchedule {
    fn default() -> LearnableLRSchedule {
        LearnableLRSchedule::new()
    }
}

/// Compute variance of a slice
fn compute_variance(values: &[f64]) -> f64 {
    if values.len() == 0 {
        return 0.0;
    }
    var sum = 0.0;
    for v in values {
        sum = sum + v;
    }
    let mean = sum / (values.len() as f64);

    var var_sum = 0.0;
    for v in values {
        let diff = v - mean;
        var_sum = var_sum + diff * diff;
    }
    var_sum / (values.len() as f64)
}
