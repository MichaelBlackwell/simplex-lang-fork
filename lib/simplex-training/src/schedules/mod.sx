// simplex-training::schedules - Learnable Training Schedules
//
// Implements differentiable schedules for:
// - Learning rate (LR) with warmup, decay, plateau detection
// - Knowledge distillation temperature
// - Pruning magnitude and timing
// - Quantization precision
//
// All schedules use dual numbers for meta-gradient computation.

pub mod lr;
pub mod distill;
pub mod prune;
pub mod quant;

pub use lr::LearnableLRSchedule;
pub use distill::LearnableDistillation;
pub use prune::LearnablePruning;
pub use quant::LearnableQuantization;

use simplex_std::dual::dual;

// =============================================================================
// Unified Learned Schedules
// =============================================================================

/// All learnable schedules combined
#[derive(Clone)]
pub struct LearnedSchedules {
    /// Learning rate schedule
    pub lr: LearnableLRSchedule,
    /// Distillation temperature schedule
    pub distill: LearnableDistillation,
    /// Pruning schedule
    pub prune: LearnablePruning,
    /// Quantization schedule
    pub quant: LearnableQuantization,
}

impl LearnedSchedules {
    /// Create with default schedules
    pub fn new() -> LearnedSchedules {
        LearnedSchedules {
            lr: LearnableLRSchedule::new(),
            distill: LearnableDistillation::new(),
            prune: LearnablePruning::new(),
            quant: LearnableQuantization::new(),
        }
    }

    /// Get learning rate at step
    pub fn learning_rate(&self, step: dual, loss_history: &[f64]) -> dual {
        self.lr.learning_rate(step, loss_history)
    }

    /// Get distillation temperature at step
    pub fn distill_temperature(&self, step: dual) -> dual {
        self.distill.temperature(step)
    }

    /// Get target sparsity at step
    pub fn sparsity(&self, step: dual) -> dual {
        self.prune.target_sparsity(step)
    }

    /// Get target bits at step
    pub fn bits(&self, step: dual) -> dual {
        self.quant.current_bits(step)
    }

    /// Extract all gradients
    pub fn gradient(&self) -> SchedulesGradient {
        SchedulesGradient {
            lr: self.lr.gradient(),
            distill: self.distill.gradient(),
            prune: self.prune.gradient(),
            quant: self.quant.gradient(),
        }
    }

    /// Update all schedules
    pub fn update(&mut self, grad: &SchedulesGradient, lr: f64) {
        self.lr.update(&grad.lr, lr);
        self.distill.update(&grad.distill, lr);
        self.prune.update(&grad.prune, lr);
        self.quant.update(&grad.quant, lr);
    }

    /// Total L2 norm (for regularization)
    pub fn l2_norm(&self) -> dual {
        self.lr.l2_norm() +
        self.distill.l2_norm() +
        self.prune.l2_norm() +
        self.quant.l2_norm()
    }
}

impl Default for LearnedSchedules {
    fn default() -> LearnedSchedules {
        LearnedSchedules::new()
    }
}

/// Combined gradient for all schedules
#[derive(Clone)]
pub struct SchedulesGradient {
    pub lr: lr::LRGradient,
    pub distill: distill::DistillGradient,
    pub prune: prune::PruneGradient,
    pub quant: quant::QuantGradient,
}

impl SchedulesGradient {
    /// Zero gradient
    pub fn zero() -> SchedulesGradient {
        SchedulesGradient {
            lr: lr::LRGradient::default(),
            distill: distill::DistillGradient::default(),
            prune: prune::PruneGradient::default(),
            quant: quant::QuantGradient::default(),
        }
    }

    /// Scale all gradients
    pub fn scale(&mut self, factor: f64) {
        self.lr.scale(factor);
        self.distill.scale(factor);
        self.prune.scale(factor);
        self.quant.scale(factor);
    }

    /// Gradient norm
    pub fn norm(&self) -> f64 {
        (self.lr.norm().powi(2) +
         self.distill.norm().powi(2) +
         self.prune.norm().powi(2) +
         self.quant.norm().powi(2)).sqrt()
    }
}

impl Default for SchedulesGradient {
    fn default() -> SchedulesGradient {
        SchedulesGradient::zero()
    }
}
