// simplex-training::schedules::distill - Learnable Knowledge Distillation
//
// Implements differentiable distillation with:
// - Learnable temperature schedule
// - Soft/hard target mixing ratio
// - Layer-wise temperature (optional)
//
// The temperature T controls knowledge transfer:
// soft_targets = softmax(teacher_logits / T)

use simplex_std::dual::dual;
use simplex_std::{Clone, Default};

/// Gradient for distillation parameters
#[derive(Clone)]
pub struct DistillGradient {
    pub d_initial_temp: f64,
    pub d_temp_decay: f64,
    pub d_min_temp: f64,
    pub d_alpha_start: f64,
    pub d_alpha_decay: f64,
}

impl DistillGradient {
    pub fn scale(&mut self, factor: f64) {
        self.d_initial_temp = self.d_initial_temp * factor;
        self.d_temp_decay = self.d_temp_decay * factor;
        self.d_min_temp = self.d_min_temp * factor;
        self.d_alpha_start = self.d_alpha_start * factor;
        self.d_alpha_decay = self.d_alpha_decay * factor;
    }

    pub fn norm(&self) -> f64 {
        (self.d_initial_temp * self.d_initial_temp +
         self.d_temp_decay * self.d_temp_decay +
         self.d_min_temp * self.d_min_temp +
         self.d_alpha_start * self.d_alpha_start +
         self.d_alpha_decay * self.d_alpha_decay).sqrt()
    }
}

impl Default for DistillGradient {
    fn default() -> DistillGradient {
        DistillGradient {
            d_initial_temp: 0.0,
            d_temp_decay: 0.0,
            d_min_temp: 0.0,
            d_alpha_start: 0.0,
            d_alpha_decay: 0.0,
        }
    }
}

/// Learnable distillation with temperature schedule
#[derive(Clone)]
pub struct LearnableDistillation {
    /// Starting temperature
    pub initial_temp: dual,
    /// Temperature decay rate
    pub temp_decay: dual,
    /// Minimum temperature
    pub min_temp: dual,
    /// Initial soft target weight (alpha)
    pub alpha_start: dual,
    /// Alpha decay rate (shift to hard targets)
    pub alpha_decay: dual,
}

impl LearnableDistillation {
    /// Create with default values
    pub fn new() -> LearnableDistillation {
        LearnableDistillation {
            initial_temp: dual::variable(4.0),  // Common starting temp
            temp_decay: dual::variable(0.0001),
            min_temp: dual::constant(1.0),
            alpha_start: dual::variable(0.7),   // 70% soft, 30% hard
            alpha_decay: dual::variable(0.0001),
        }
    }

    /// Get temperature at step
    pub fn temperature(&self, step: dual) -> dual {
        let decay = (dual::zero() - self.temp_decay * step).exp();
        let temp = self.initial_temp * decay;
        temp.max(self.min_temp)
    }

    /// Get soft/hard mixing coefficient at step
    ///
    /// Returns alpha where:
    /// - loss = alpha * soft_loss + (1 - alpha) * hard_loss
    pub fn alpha(&self, step: dual) -> dual {
        let decay = (dual::zero() - self.alpha_decay * step).exp();
        self.alpha_start * decay
    }

    /// Compute distillation loss (differentiable)
    ///
    /// Combines:
    /// - KL divergence from student to teacher soft targets
    /// - Cross-entropy on hard labels
    pub fn distill_loss(
        &self,
        student_logits: &[dual],
        teacher_logits: &[f64],
        hard_labels: &[i64],
        step: dual
    ) -> dual {
        let temp = self.temperature(step);
        let alpha = self.alpha(step);

        // Compute soft targets from teacher
        let soft_targets = softmax_with_temp(teacher_logits, temp.val);

        // Compute student log probabilities
        let student_log_probs = log_softmax_dual(student_logits, temp);

        // KL divergence: sum(p * (log(p) - log(q)))
        var kl_loss = dual::zero();
        for i in 0..student_logits.len() {
            let p = soft_targets[i];
            if p > 1e-10 {
                let log_p = p.ln();
                let log_q = student_log_probs[i];
                kl_loss = kl_loss + dual::constant(p) * (dual::constant(log_p) - log_q);
            }
        }

        // Scale KL by TÂ² (standard practice)
        kl_loss = kl_loss * temp * temp;

        // Hard target loss (cross-entropy)
        var hard_loss = dual::zero();
        for i in 0..hard_labels.len() {
            let target_idx = hard_labels[i] as usize;
            if target_idx < student_logits.len() {
                hard_loss = hard_loss - log_softmax_dual(student_logits, dual::constant(1.0))[target_idx];
            }
        }
        hard_loss = hard_loss / dual::constant(hard_labels.len() as f64);

        // Combined loss
        alpha * kl_loss + (dual::constant(1.0) - alpha) * hard_loss
    }

    /// Extract gradients
    pub fn gradient(&self) -> DistillGradient {
        DistillGradient {
            d_initial_temp: self.initial_temp.der,
            d_temp_decay: self.temp_decay.der,
            d_min_temp: self.min_temp.der,
            d_alpha_start: self.alpha_start.der,
            d_alpha_decay: self.alpha_decay.der,
        }
    }

    /// Update parameters
    pub fn update(&mut self, grad: &DistillGradient, lr: f64) {
        self.initial_temp = dual::new(
            self.initial_temp.val - lr * grad.d_initial_temp,
            1.0
        );
        self.temp_decay = dual::new(
            self.temp_decay.val - lr * grad.d_temp_decay,
            1.0
        );
        self.alpha_start = dual::new(
            self.alpha_start.val - lr * grad.d_alpha_start,
            1.0
        );
        self.alpha_decay = dual::new(
            self.alpha_decay.val - lr * grad.d_alpha_decay,
            1.0
        );

        // Clamp to valid ranges
        if self.initial_temp.val < 1.0 {
            self.initial_temp = dual::new(1.0, self.initial_temp.der);
        }
        if self.initial_temp.val > 20.0 {
            self.initial_temp = dual::new(20.0, self.initial_temp.der);
        }
        if self.alpha_start.val < 0.0 {
            self.alpha_start = dual::new(0.0, self.alpha_start.der);
        }
        if self.alpha_start.val > 1.0 {
            self.alpha_start = dual::new(1.0, self.alpha_start.der);
        }
    }

    /// L2 norm for regularization
    pub fn l2_norm(&self) -> dual {
        self.initial_temp * self.initial_temp +
        self.alpha_start * self.alpha_start
    }
}

impl Default for LearnableDistillation {
    fn default() -> LearnableDistillation {
        LearnableDistillation::new()
    }
}

// =============================================================================
// Helper Functions
// =============================================================================

/// Softmax with temperature (non-dual)
fn softmax_with_temp(logits: &[f64], temp: f64) -> Vec<f64> {
    var max_val = logits[0];
    for l in logits {
        if *l > max_val {
            max_val = *l;
        }
    }

    var exps = Vec::new();
    var sum = 0.0;
    for l in logits {
        let e = ((*l - max_val) / temp).exp();
        exps.push(e);
        sum = sum + e;
    }

    var result = Vec::new();
    for e in exps.iter() {
        result.push(*e / sum);
    }
    result
}

/// Log-softmax with temperature (dual)
fn log_softmax_dual(logits: &[dual], temp: dual) -> Vec<dual> {
    var max_val = logits[0];
    for l in logits {
        if l.val > max_val.val {
            max_val = *l;
        }
    }

    var sum_exp = dual::zero();
    for l in logits {
        let scaled = (*l - max_val) / temp;
        sum_exp = sum_exp + scaled.exp();
    }

    let log_sum_exp = sum_exp.ln();

    var result = Vec::new();
    for l in logits {
        let scaled = (*l - max_val) / temp;
        result.push(scaled - log_sum_exp);
    }
    result
}
