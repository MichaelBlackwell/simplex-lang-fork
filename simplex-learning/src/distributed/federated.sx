// Federated learning for hive-wide coordination
//
// Enables decentralized learning across specialists while preserving
// local adaptations and preventing catastrophic interference.

use crate::tensor::Tensor;

/// Federated learning configuration
pub struct FederatedConfig {
    /// Minimum nodes before aggregation
    pub min_nodes: usize,

    /// Maximum staleness (rounds) for gradients
    pub max_staleness: u64,

    /// Local epochs before sync
    pub local_epochs: usize,

    /// Aggregation strategy
    pub aggregation: AggregationStrategy,

    /// Communication compression
    pub compression: CompressionConfig,

    /// Privacy settings
    pub privacy: PrivacyConfig,
}

impl Default for FederatedConfig {
    fn default() -> Self {
        FederatedConfig {
            min_nodes: 2,
            max_staleness: 5,
            local_epochs: 1,
            aggregation: AggregationStrategy::FedAvg,
            compression: CompressionConfig::default(),
            privacy: PrivacyConfig::default(),
        }
    }
}

/// Aggregation strategy for federated updates
pub enum AggregationStrategy {
    /// Simple averaging (FedAvg)
    FedAvg,

    /// Weighted by sample count
    WeightedAvg,

    /// Weighted by validation performance
    PerformanceWeighted,

    /// Median aggregation (Byzantine-resilient)
    Median,

    /// Trimmed mean (Byzantine-resilient)
    TrimmedMean { trim_ratio: f64 },

    /// Attention-weighted aggregation
    AttentionWeighted,

    /// Custom aggregation function
    Custom(Box<dyn Fn(&[NodeUpdate]) -> Vec<Tensor>>),
}

/// Compression configuration
pub struct CompressionConfig {
    /// Enable gradient compression
    pub enabled: bool,

    /// Top-k sparsification ratio
    pub top_k_ratio: f64,

    /// Quantization bits (0 = no quantization)
    pub quantization_bits: u8,

    /// Error feedback for compression
    pub error_feedback: bool,
}

impl Default for CompressionConfig {
    fn default() -> Self {
        CompressionConfig {
            enabled: false,
            top_k_ratio: 0.01,
            quantization_bits: 0,
            error_feedback: true,
        }
    }
}

/// Privacy configuration for differential privacy
pub struct PrivacyConfig {
    /// Enable differential privacy
    pub enabled: bool,

    /// Noise multiplier (sigma)
    pub noise_multiplier: f64,

    /// Gradient clipping norm
    pub max_grad_norm: f64,

    /// Privacy budget (epsilon)
    pub epsilon: f64,

    /// Delta for (epsilon, delta)-DP
    pub delta: f64,
}

impl Default for PrivacyConfig {
    fn default() -> Self {
        PrivacyConfig {
            enabled: false,
            noise_multiplier: 1.0,
            max_grad_norm: 1.0,
            epsilon: 1.0,
            delta: 1e-5,
        }
    }
}

/// Update from a single node
pub struct NodeUpdate {
    /// Node identifier
    pub node_id: String,

    /// Updated parameters (or gradients)
    pub params: Vec<Tensor>,

    /// Number of samples used
    pub sample_count: usize,

    /// Local validation accuracy
    pub validation_acc: f64,

    /// Round number
    pub round: u64,

    /// Metadata
    pub metadata: std::collections::HashMap<String, String>,
}

/// Federated learner coordinator
pub struct FederatedLearner {
    /// Configuration
    config: FederatedConfig,

    /// Current global model
    global_params: Vec<Tensor>,

    /// Pending updates
    pending_updates: Vec<NodeUpdate>,

    /// Current round
    round: u64,

    /// Compression error buffer (for error feedback)
    compression_error: Vec<Tensor>,

    /// Privacy accountant
    privacy_spent: f64,

    /// Node statistics
    node_stats: std::collections::HashMap<String, NodeStats>,
}

/// Statistics for a single node
struct NodeStats {
    updates_contributed: u64,
    total_samples: usize,
    avg_validation_acc: f64,
    last_round: u64,
}

impl FederatedLearner {
    /// Create new federated learner
    pub fn new(config: FederatedConfig, initial_params: Vec<Tensor>) -> Self {
        let compression_error = if config.compression.error_feedback {
            initial_params.iter().map(|p| Tensor::zeros(p.shape())).collect()
        } else {
            Vec::new()
        };

        FederatedLearner {
            config,
            global_params: initial_params,
            pending_updates: Vec::new(),
            round: 0,
            compression_error,
            privacy_spent: 0.0,
            node_stats: std::collections::HashMap::new(),
        }
    }

    /// Submit update from a node
    pub fn submit_update(&mut self, update: NodeUpdate) -> bool {
        // Check staleness
        if self.round > update.round + self.config.max_staleness {
            return false; // Too stale
        }

        // Update node stats
        let stats = self.node_stats.entry(update.node_id.clone())
            .or_insert(NodeStats {
                updates_contributed: 0,
                total_samples: 0,
                avg_validation_acc: 0.0,
                last_round: 0,
            });

        stats.updates_contributed += 1;
        stats.total_samples += update.sample_count;
        stats.avg_validation_acc = 0.9 * stats.avg_validation_acc + 0.1 * update.validation_acc;
        stats.last_round = update.round;

        self.pending_updates.push(update);

        // Try to aggregate if we have enough updates
        if self.pending_updates.len() >= self.config.min_nodes {
            self.aggregate();
            true
        } else {
            false
        }
    }

    /// Aggregate pending updates
    fn aggregate(&mut self) {
        if self.pending_updates.is_empty() {
            return;
        }

        let aggregated = match &self.config.aggregation {
            AggregationStrategy::FedAvg => self.fed_avg(),
            AggregationStrategy::WeightedAvg => self.weighted_avg(),
            AggregationStrategy::PerformanceWeighted => self.performance_weighted(),
            AggregationStrategy::Median => self.median_aggregation(),
            AggregationStrategy::TrimmedMean { trim_ratio } => self.trimmed_mean(*trim_ratio),
            AggregationStrategy::AttentionWeighted => self.attention_weighted(),
            AggregationStrategy::Custom(f) => f(&self.pending_updates),
        };

        // Apply differential privacy if enabled
        let aggregated = if self.config.privacy.enabled {
            self.apply_differential_privacy(aggregated)
        } else {
            aggregated
        };

        // Update global model
        self.global_params = aggregated;
        self.pending_updates.clear();
        self.round += 1;
    }

    /// Simple FedAvg aggregation
    fn fed_avg(&self) -> Vec<Tensor> {
        let n = self.pending_updates.len() as f64;
        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        for update in &self.pending_updates {
            for (i, param) in update.params.iter().enumerate() {
                for j in 0..param.numel() {
                    let v = result[i].get(j) + param.get(j) / n;
                    result[i].set(j, v);
                }
            }
        }

        result
    }

    /// Weighted average by sample count
    fn weighted_avg(&self) -> Vec<Tensor> {
        let total_samples: usize = self.pending_updates.iter()
            .map(|u| u.sample_count)
            .sum();

        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        for update in &self.pending_updates {
            let weight = update.sample_count as f64 / total_samples as f64;
            for (i, param) in update.params.iter().enumerate() {
                for j in 0..param.numel() {
                    let v = result[i].get(j) + param.get(j) * weight;
                    result[i].set(j, v);
                }
            }
        }

        result
    }

    /// Weighted by validation performance
    fn performance_weighted(&self) -> Vec<Tensor> {
        let total_perf: f64 = self.pending_updates.iter()
            .map(|u| u.validation_acc)
            .sum();

        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        for update in &self.pending_updates {
            let weight = update.validation_acc / (total_perf + 1e-8);
            for (i, param) in update.params.iter().enumerate() {
                for j in 0..param.numel() {
                    let v = result[i].get(j) + param.get(j) * weight;
                    result[i].set(j, v);
                }
            }
        }

        result
    }

    /// Median aggregation (Byzantine-resilient)
    fn median_aggregation(&self) -> Vec<Tensor> {
        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        for i in 0..result.len() {
            let numel = result[i].numel();
            for j in 0..numel {
                let mut values: Vec<f64> = self.pending_updates.iter()
                    .map(|u| u.params[i].get(j))
                    .collect();
                values.sort_by(|a, b| a.partial_cmp(b).unwrap());
                let median = values[values.len() / 2];
                result[i].set(j, median);
            }
        }

        result
    }

    /// Trimmed mean aggregation
    fn trimmed_mean(&self, trim_ratio: f64) -> Vec<Tensor> {
        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        let n = self.pending_updates.len();
        let trim_count = (n as f64 * trim_ratio) as usize;

        for i in 0..result.len() {
            let numel = result[i].numel();
            for j in 0..numel {
                let mut values: Vec<f64> = self.pending_updates.iter()
                    .map(|u| u.params[i].get(j))
                    .collect();
                values.sort_by(|a, b| a.partial_cmp(b).unwrap());

                // Trim and average
                let trimmed: Vec<f64> = values[trim_count..n-trim_count].to_vec();
                let mean = trimmed.iter().sum::<f64>() / trimmed.len() as f64;
                result[i].set(j, mean);
            }
        }

        result
    }

    /// Attention-weighted aggregation
    fn attention_weighted(&self) -> Vec<Tensor> {
        // Compute attention weights based on similarity to current global model
        let mut attention_scores: Vec<f64> = Vec::new();

        for update in &self.pending_updates {
            let mut similarity = 0.0;
            let mut norm1 = 0.0;
            let mut norm2 = 0.0;

            for (i, param) in update.params.iter().enumerate() {
                for j in 0..param.numel() {
                    let a = param.get(j);
                    let b = self.global_params[i].get(j);
                    similarity += a * b;
                    norm1 += a * a;
                    norm2 += b * b;
                }
            }

            let cosine_sim = similarity / ((norm1.sqrt() * norm2.sqrt()) + 1e-8);
            attention_scores.push(cosine_sim.exp());
        }

        // Normalize attention
        let total: f64 = attention_scores.iter().sum();
        for score in &mut attention_scores {
            *score /= total + 1e-8;
        }

        // Weighted sum
        let mut result = self.global_params.iter()
            .map(|p| Tensor::zeros(p.shape()))
            .collect::<Vec<_>>();

        for (idx, update) in self.pending_updates.iter().enumerate() {
            let weight = attention_scores[idx];
            for (i, param) in update.params.iter().enumerate() {
                for j in 0..param.numel() {
                    let v = result[i].get(j) + param.get(j) * weight;
                    result[i].set(j, v);
                }
            }
        }

        result
    }

    /// Apply differential privacy
    fn apply_differential_privacy(&mut self, params: Vec<Tensor>) -> Vec<Tensor> {
        let mut result = params;

        for param in &mut result {
            for i in 0..param.numel() {
                // Add Gaussian noise
                let noise = gaussian_noise(0.0, self.config.privacy.noise_multiplier);
                param.set(i, param.get(i) + noise);
            }
        }

        // Update privacy budget
        self.privacy_spent += compute_privacy_cost(
            self.config.privacy.noise_multiplier,
            self.pending_updates.len(),
        );

        result
    }

    /// Get current global model
    pub fn global_params(&self) -> &[Tensor] {
        &self.global_params
    }

    /// Get current round
    pub fn round(&self) -> u64 {
        self.round
    }

    /// Get privacy budget spent
    pub fn privacy_spent(&self) -> f64 {
        self.privacy_spent
    }

    /// Check if privacy budget exceeded
    pub fn privacy_budget_exceeded(&self) -> bool {
        self.privacy_spent > self.config.privacy.epsilon
    }

    /// Compress parameters for transmission
    pub fn compress(&mut self, params: &[Tensor]) -> Vec<CompressedTensor> {
        if !self.config.compression.enabled {
            return params.iter()
                .map(|p| CompressedTensor::Uncompressed(p.clone()))
                .collect();
        }

        let mut result = Vec::new();

        for (i, param) in params.iter().enumerate() {
            // Add error feedback
            let mut adjusted = param.clone();
            if self.config.compression.error_feedback && i < self.compression_error.len() {
                for j in 0..adjusted.numel() {
                    adjusted.set(j, adjusted.get(j) + self.compression_error[i].get(j));
                }
            }

            // Top-k sparsification
            let k = (param.numel() as f64 * self.config.compression.top_k_ratio) as usize;
            let compressed = top_k_sparsify(&adjusted, k);

            // Quantization
            let compressed = if self.config.compression.quantization_bits > 0 {
                quantize(&compressed, self.config.compression.quantization_bits)
            } else {
                compressed
            };

            // Compute error for feedback
            if self.config.compression.error_feedback && i < self.compression_error.len() {
                for j in 0..param.numel() {
                    let error = adjusted.get(j) - decompress_value(&compressed, j);
                    self.compression_error[i].set(j, error);
                }
            }

            result.push(compressed);
        }

        result
    }
}

/// Compressed tensor representation
pub enum CompressedTensor {
    /// Uncompressed
    Uncompressed(Tensor),

    /// Sparse (indices + values)
    Sparse {
        shape: Vec<usize>,
        indices: Vec<usize>,
        values: Vec<f64>,
    },

    /// Quantized
    Quantized {
        shape: Vec<usize>,
        data: Vec<u8>,
        scale: f64,
        zero_point: f64,
    },
}

// Helper functions (simplified implementations)

fn gaussian_noise(mean: f64, std: f64) -> f64 {
    // Box-Muller transform with proper random generation
    use std::f64::consts::PI;

    // Use thread-local Xorshift64 for random values
    thread_local! {
        static RNG_STATE: std::cell::RefCell<u64> = std::cell::RefCell::new(
            std::time::SystemTime::now()
                .duration_since(std::time::UNIX_EPOCH)
                .map(|d| d.as_nanos() as u64)
                .unwrap_or(0x123456789ABCDEF0)
                ^ 0xFEDCBA9876543210
        );
    }

    fn xorshift64(state: &mut u64) -> u64 {
        let mut x = *state;
        x ^= x << 13;
        x ^= x >> 7;
        x ^= x << 17;
        *state = x;
        x
    }

    let (u1, u2) = RNG_STATE.with(|state| {
        let mut s = state.borrow_mut();
        // Ensure u1 > 0 to avoid ln(0)
        let v1 = (xorshift64(&mut s) as f64 / u64::MAX as f64).max(1e-10);
        let v2 = xorshift64(&mut s) as f64 / u64::MAX as f64;
        (v1, v2)
    });

    mean + std * (-2.0 * u1.ln()).sqrt() * (2.0 * PI * u2).cos()
}

fn compute_privacy_cost(noise_multiplier: f64, batch_size: usize) -> f64 {
    // Simplified privacy accounting
    1.0 / (noise_multiplier * noise_multiplier) * (batch_size as f64).ln()
}

fn top_k_sparsify(tensor: &Tensor, k: usize) -> CompressedTensor {
    let mut indexed: Vec<(usize, f64)> = (0..tensor.numel())
        .map(|i| (i, tensor.get(i).abs()))
        .collect();

    indexed.sort_by(|a, b| b.1.partial_cmp(&a.1).unwrap());

    let indices: Vec<usize> = indexed[..k.min(tensor.numel())].iter()
        .map(|(i, _)| *i)
        .collect();

    let values: Vec<f64> = indices.iter()
        .map(|&i| tensor.get(i))
        .collect();

    CompressedTensor::Sparse {
        shape: tensor.shape().to_vec(),
        indices,
        values,
    }
}

fn quantize(tensor: &CompressedTensor, bits: u8) -> CompressedTensor {
    // Simplified - just pass through
    tensor.clone()
}

fn decompress_value(tensor: &CompressedTensor, index: usize) -> f64 {
    match tensor {
        CompressedTensor::Uncompressed(t) => t.get(index),
        CompressedTensor::Sparse { indices, values, .. } => {
            indices.iter()
                .position(|&i| i == index)
                .map(|pos| values[pos])
                .unwrap_or(0.0)
        }
        CompressedTensor::Quantized { data, scale, zero_point, .. } => {
            if index < data.len() {
                (data[index] as f64 - *zero_point) * scale
            } else {
                0.0
            }
        }
    }
}

impl Clone for CompressedTensor {
    fn clone(&self) -> Self {
        match self {
            CompressedTensor::Uncompressed(t) => CompressedTensor::Uncompressed(t.clone()),
            CompressedTensor::Sparse { shape, indices, values } => CompressedTensor::Sparse {
                shape: shape.clone(),
                indices: indices.clone(),
                values: values.clone(),
            },
            CompressedTensor::Quantized { shape, data, scale, zero_point } => CompressedTensor::Quantized {
                shape: shape.clone(),
                data: data.clone(),
                scale: *scale,
                zero_point: *zero_point,
            },
        }
    }
}
