// Performance Benchmarks for simplex-learning
//
// Measures key performance metrics:
// - Learning latency overhead
// - Memory usage over time
// - Throughput (examples/second)
// - Component-specific performance

use simplex_learning::{
    Tensor,
    optim::{Adam, StreamingAdam, StreamingSGD},
    memory::{ReplayBuffer, EWC, TensorReplayBuffer, rng},
    calibration::{ECE, TemperatureScaling, OnlineCalibration},
    safety::{GradientBounds, SafetyApplier},
    distributed::{HiveLearningCoordinator, HiveLearningConfig},
    runtime::{ContinuousLearner, LearnerConfig},
};

use std::time::{Instant, Duration};

/// Benchmark result
struct BenchmarkResult {
    name: String,
    iterations: usize,
    total_time_ms: f64,
    avg_time_ms: f64,
    min_time_ms: f64,
    max_time_ms: f64,
    throughput: f64,  // iterations per second
}

impl BenchmarkResult {
    fn print(&self) {
        println!("Benchmark: {}", self.name);
        println!("  Iterations: {}", self.iterations);
        println!("  Total time: {:.2} ms", self.total_time_ms);
        println!("  Average:    {:.4} ms/iter", self.avg_time_ms);
        println!("  Min:        {:.4} ms", self.min_time_ms);
        println!("  Max:        {:.4} ms", self.max_time_ms);
        println!("  Throughput: {:.0} iter/sec", self.throughput);
        println!();
    }
}

/// Run a benchmark
fn benchmark<F: FnMut()>(name: &str, iterations: usize, mut f: F) -> BenchmarkResult {
    // Warmup
    for _ in 0..10 {
        f();
    }

    // Actual benchmark
    let mut times: Vec<f64> = Vec::with_capacity(iterations);

    for _ in 0..iterations {
        let start = Instant::now();
        f();
        times.push(start.elapsed().as_secs_f64() * 1000.0);
    }

    let total: f64 = times.iter().sum();
    let avg = total / iterations as f64;
    let min = times.iter().cloned().fold(f64::INFINITY, f64::min);
    let max = times.iter().cloned().fold(0.0, f64::max);
    let throughput = iterations as f64 / (total / 1000.0);

    BenchmarkResult {
        name: name.to_string(),
        iterations,
        total_time_ms: total,
        avg_time_ms: avg,
        min_time_ms: min,
        max_time_ms: max,
        throughput,
    }
}

// ============================================================================
// TENSOR OPERATION BENCHMARKS
// ============================================================================

fn bench_tensor_ops() {
    println!("=== Tensor Operation Benchmarks ===\n");

    // Matrix multiplication
    let a = Tensor::randn(&[64, 128]);
    let b = Tensor::randn(&[128, 64]);

    let result = benchmark("matmul [64x128] x [128x64]", 1000, || {
        let _ = a.matmul(&b);
    });
    result.print();

    // Larger matmul
    let a_large = Tensor::randn(&[256, 512]);
    let b_large = Tensor::randn(&[512, 256]);

    let result = benchmark("matmul [256x512] x [512x256]", 100, || {
        let _ = a_large.matmul(&b_large);
    });
    result.print();

    // Element-wise operations
    let x = Tensor::randn(&[1024]);
    let y = Tensor::randn(&[1024]);

    let result = benchmark("add [1024]", 10000, || {
        let _ = x.add(&y);
    });
    result.print();

    let result = benchmark("mul [1024]", 10000, || {
        let _ = x.mul(&y);
    });
    result.print();

    // Activation functions
    let result = benchmark("relu [1024]", 10000, || {
        let _ = x.relu();
    });
    result.print();

    let result = benchmark("sigmoid [1024]", 10000, || {
        let _ = x.sigmoid();
    });
    result.print();

    let result = benchmark("softmax [1024]", 1000, || {
        let _ = x.softmax(-1);
    });
    result.print();
}

// ============================================================================
// OPTIMIZER BENCHMARKS
// ============================================================================

fn bench_optimizers() {
    println!("=== Optimizer Benchmarks ===\n");

    // SGD step
    let params = vec![Tensor::randn(&[100, 100])];
    let mut sgd = StreamingSGD::new(&params, 0.01, 0.9);

    // Add fake gradients
    for p in &params {
        p.set_grad(Tensor::randn(p.shape()));
    }

    let result = benchmark("SGD step [100x100]", 1000, || {
        let mut params_clone = params.clone();
        sgd.step(&mut params_clone);
    });
    result.print();

    // Adam step
    let params = vec![Tensor::randn(&[100, 100])];
    let mut adam = Adam::new(&params, 0.001, 0.9, 0.999, 1e-8, 0.0);

    for p in &params {
        p.set_grad(Tensor::randn(p.shape()));
    }

    let result = benchmark("Adam step [100x100]", 1000, || {
        let mut params_clone = params.clone();
        adam.step(&mut params_clone);
    });
    result.print();

    // Larger model
    let large_params = vec![
        Tensor::randn(&[512, 256]),
        Tensor::randn(&[256, 128]),
        Tensor::randn(&[128, 64]),
    ];
    let mut adam_large = Adam::new(&large_params, 0.001, 0.9, 0.999, 1e-8, 0.01);

    for p in &large_params {
        p.set_grad(Tensor::randn(p.shape()));
    }

    let result = benchmark("Adam step [512x256, 256x128, 128x64]", 100, || {
        let mut params_clone = large_params.clone();
        adam_large.step(&mut params_clone);
    });
    result.print();
}

// ============================================================================
// REPLAY BUFFER BENCHMARKS
// ============================================================================

fn bench_replay_buffer() {
    println!("=== Replay Buffer Benchmarks ===\n");

    // Add to buffer
    let mut buffer = TensorReplayBuffer::new(10000);

    let result = benchmark("ReplayBuffer add", 10000, || {
        buffer.add(TensorExperience {
            input: Tensor::randn(&[1, 64]),
            target: Tensor::randn(&[1, 10]),
            loss: 0.5,
            priority: 1.0,
            task_id: 0,
        });
    });
    result.print();

    // Sample from buffer
    let result = benchmark("ReplayBuffer sample(32)", 1000, || {
        let _ = buffer.sample(32);
    });
    result.print();

    // Prioritized sampling
    let result = benchmark("ReplayBuffer sample_prioritized(32)", 1000, || {
        let _ = buffer.sample_prioritized(32, 0.6);
    });
    result.print();
}

use simplex_learning::memory::TensorExperience;

// ============================================================================
// EWC BENCHMARKS
// ============================================================================

fn bench_ewc() {
    println!("=== EWC Benchmarks ===\n");

    let params = vec![
        Tensor::randn(&[256, 128]),
        Tensor::randn(&[128, 64]),
    ];

    let mut ewc = EWC::new(0.5, 100);

    // Consolidation
    let result = benchmark("EWC consolidate", 10, || {
        ewc.consolidate(&params, |p| Tensor::randn(p.shape()));
    });
    result.print();

    // Penalty computation
    ewc.consolidate(&params, |p| Tensor::randn(p.shape()));

    let result = benchmark("EWC penalty", 1000, || {
        let _ = ewc.penalty(&params);
    });
    result.print();

    // Gradient computation
    let result = benchmark("EWC gradient", 1000, || {
        let _ = ewc.gradient(&params);
    });
    result.print();
}

// ============================================================================
// CALIBRATION BENCHMARKS
// ============================================================================

fn bench_calibration() {
    println!("=== Calibration Benchmarks ===\n");

    // ECE computation
    let mut ece = ECE::new(15);

    let result = benchmark("ECE update", 10000, || {
        let conf = rng::next_f64();
        let correct = rng::next_f64() > 0.5;
        ece.update(conf, correct);
    });
    result.print();

    let result = benchmark("ECE compute", 1000, || {
        let _ = ece.compute();
    });
    result.print();

    // Temperature scaling
    let mut temp = TemperatureScaling::new(1.0);
    let logits = Tensor::randn(&[32, 10]);

    let result = benchmark("Temperature scale [32x10]", 1000, || {
        let _ = temp.scale(&logits);
    });
    result.print();

    // Online calibration
    let mut online_cal = OnlineCalibration::new(0.05);

    let result = benchmark("OnlineCalibration update", 1000, || {
        let output = Tensor::randn(&[1, 10]).softmax(-1);
        let target = Tensor::zeros(&[1, 10]);
        online_cal.process_batch(&output, &target);
    });
    result.print();
}

// ============================================================================
// SAFETY BENCHMARKS
// ============================================================================

fn bench_safety() {
    println!("=== Safety Benchmarks ===\n");

    let mut params = vec![
        Tensor::randn(&[256, 128]),
        Tensor::randn(&[128, 64]),
    ];

    for p in &mut params {
        p.set_grad(Tensor::randn(p.shape()));
    }

    // Gradient clipping
    let bounds = GradientBounds::default();

    let result = benchmark("Gradient clipping", 1000, || {
        let mut params_clone = params.clone();
        bounds.apply(&mut params_clone);
    });
    result.print();

    // Safety applier (full safety check)
    let applier = SafetyApplier::new(bounds.clone());

    let result = benchmark("SafetyApplier apply", 1000, || {
        let mut params_clone = params.clone();
        applier.apply(&mut params_clone);
    });
    result.print();
}

// ============================================================================
// CONTINUOUS LEARNER BENCHMARKS
// ============================================================================

fn bench_continuous_learner() {
    println!("=== Continuous Learner Benchmarks ===\n");

    // Simple configuration
    let params = vec![
        Tensor::randn(&[64, 32]),
        Tensor::randn(&[32, 16]),
    ];

    let config = LearnerConfig {
        learning_rate: 0.001,
        use_replay: true,
        replay_buffer_size: 1000,
        use_ewc: false,
        use_calibration: false,
        warmup_steps: 0,
        ..Default::default()
    };

    let mut learner = ContinuousLearner::new(config, params);
    learner.start();

    let result = benchmark("ContinuousLearner.learn (simple)", 1000, || {
        let input = Tensor::randn(&[1, 64]);
        let output = Tensor::randn(&[1, 16]);
        let target = Tensor::randn(&[1, 16]);
        learner.learn(&input, &output, &target, 0.5);
    });
    result.print();

    // Full configuration
    let params = vec![
        Tensor::randn(&[64, 32]),
        Tensor::randn(&[32, 16]),
    ];

    let config = LearnerConfig {
        learning_rate: 0.001,
        use_replay: true,
        replay_buffer_size: 5000,
        use_ewc: true,
        ewc_lambda: 0.4,
        use_calibration: true,
        target_ece: 0.05,
        warmup_steps: 0,
        gradient_accumulation: 4,
        ..Default::default()
    };

    let mut learner_full = ContinuousLearner::new(config, params);
    learner_full.start();

    let result = benchmark("ContinuousLearner.learn (full)", 500, || {
        let input = Tensor::randn(&[1, 64]);
        let output = Tensor::randn(&[1, 16]);
        let target = Tensor::randn(&[1, 16]);
        learner_full.learn(&input, &output, &target, 0.5);
    });
    result.print();
}

// ============================================================================
// DISTRIBUTED LEARNING BENCHMARKS
// ============================================================================

fn bench_distributed() {
    println!("=== Distributed Learning Benchmarks ===\n");

    let config = HiveLearningConfig::builder()
        .sync_interval(10)
        .staleness_tolerance(3)
        .max_specialists(10)
        .build();

    let mut coordinator = HiveLearningCoordinator::new(config);

    // Register specialists
    for i in 0..5 {
        let params = vec![Tensor::randn(&[64, 32])];
        coordinator.register_specialist(&format!("specialist_{}", i), params);
    }

    let result = benchmark("Coordinator submit_gradients", 1000, || {
        let gradients = vec![Tensor::randn(&[64, 32])];
        coordinator.submit_gradients("specialist_0", gradients, 1);
    });
    result.print();

    let result = benchmark("Coordinator step", 100, || {
        coordinator.step();
    });
    result.print();
}

// ============================================================================
// LATENCY OVERHEAD COMPARISON
// ============================================================================

fn bench_latency_overhead() {
    println!("=== Latency Overhead Comparison ===\n");

    let input_dim = 64;
    let hidden_dim = 128;
    let output_dim = 32;

    // Baseline: forward pass only
    let w1 = Tensor::randn(&[input_dim, hidden_dim]);
    let w2 = Tensor::randn(&[hidden_dim, output_dim]);

    let baseline = benchmark("Forward pass only", 1000, || {
        let input = Tensor::randn(&[1, input_dim]);
        let hidden = input.matmul(&w1).relu();
        let _ = hidden.matmul(&w2);
    });

    // With learning
    let params = vec![
        Tensor::randn(&[input_dim, hidden_dim]),
        Tensor::randn(&[hidden_dim, output_dim]),
    ];

    let config = LearnerConfig {
        learning_rate: 0.001,
        use_replay: true,
        replay_buffer_size: 1000,
        use_ewc: true,
        ewc_lambda: 0.4,
        use_calibration: true,
        target_ece: 0.05,
        warmup_steps: 0,
        ..Default::default()
    };

    let mut learner = ContinuousLearner::new(config, params);
    learner.start();

    let with_learning = benchmark("Forward + learning", 1000, || {
        let input = Tensor::randn(&[1, input_dim]);
        let output = Tensor::randn(&[1, output_dim]);
        let target = Tensor::randn(&[1, output_dim]);
        learner.learn(&input, &output, &target, 0.5);
    });

    println!("Baseline (forward only):");
    baseline.print();

    println!("With learning:");
    with_learning.print();

    let overhead = ((with_learning.avg_time_ms - baseline.avg_time_ms) / baseline.avg_time_ms) * 100.0;
    println!("Learning overhead: {:.1}%\n", overhead);

    if overhead < 20.0 {
        println!("SUCCESS: Latency overhead is below 20% target");
    } else {
        println!("WARNING: Latency overhead exceeds 20% target");
    }
}

// ============================================================================
// MEMORY STABILITY BENCHMARK
// ============================================================================

fn bench_memory_stability() {
    println!("=== Memory Stability Benchmark ===\n");

    let params = vec![Tensor::randn(&[100, 100])];

    let config = LearnerConfig {
        learning_rate: 0.001,
        use_replay: true,
        replay_buffer_size: 1000,  // Fixed size
        use_ewc: true,
        ewc_lambda: 0.4,
        use_calibration: true,
        warmup_steps: 0,
        ..Default::default()
    };

    let mut learner = ContinuousLearner::new(config, params);
    learner.start();

    // Track step times over many iterations
    let mut times: Vec<f64> = Vec::new();

    for i in 0..10000 {
        let start = Instant::now();
        let input = Tensor::randn(&[1, 100]);
        let output = Tensor::randn(&[1, 100]);
        let target = Tensor::randn(&[1, 100]);
        learner.learn(&input, &output, &target, 0.5);
        times.push(start.elapsed().as_secs_f64() * 1000.0);
    }

    // Analyze stability
    let first_1k_avg = times[0..1000].iter().sum::<f64>() / 1000.0;
    let last_1k_avg = times[9000..10000].iter().sum::<f64>() / 1000.0;

    println!("First 1000 steps avg: {:.4} ms", first_1k_avg);
    println!("Last 1000 steps avg:  {:.4} ms", last_1k_avg);

    let degradation = ((last_1k_avg - first_1k_avg) / first_1k_avg) * 100.0;
    println!("Performance degradation: {:.1}%\n", degradation);

    if degradation < 10.0 {
        println!("SUCCESS: No significant performance degradation");
    } else if degradation < 50.0 {
        println!("WARNING: Moderate performance degradation detected");
    } else {
        println!("FAILURE: Significant performance degradation");
    }
}

// ============================================================================
// MAIN
// ============================================================================

fn main() {
    println!("╔════════════════════════════════════════════════════════════════╗");
    println!("║        simplex-learning Performance Benchmarks                  ║");
    println!("╚════════════════════════════════════════════════════════════════╝\n");

    bench_tensor_ops();
    bench_optimizers();
    bench_replay_buffer();
    bench_ewc();
    bench_calibration();
    bench_safety();
    bench_continuous_learner();
    bench_distributed();

    println!("╔════════════════════════════════════════════════════════════════╗");
    println!("║                    Key Performance Tests                         ║");
    println!("╚════════════════════════════════════════════════════════════════╝\n");

    bench_latency_overhead();
    bench_memory_stability();

    println!("╔════════════════════════════════════════════════════════════════╗");
    println!("║                    Benchmark Complete                            ║");
    println!("╚════════════════════════════════════════════════════════════════╝");
}

main();
