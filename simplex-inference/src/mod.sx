// simplex-inference - High-Performance Inference Optimization Runtime
//
// This library provides generic inference optimizations for any Simplex application
// using SLM/LLM models. It's designed to be hardware-agnostic and can be used with
// CPU, GPU (CUDA), or specialized AI accelerators (Inferentia, TPU).
//
// # Core Components
//
// - `Batcher<T>`: Generic request batcher for continuous batching
// - `PromptCache`: LRU cache for tokenized prompt prefixes
// - `ResponseCache<K, V>`: TTL-based cache for deterministic responses
// - `Router`: Smart routing based on query complexity analysis
// - `InferencePool`: Pooled model instances for concurrent inference
//
// # Usage Example
//
// ```simplex
// use simplex_inference::{Batcher, PromptCache, ResponseCache, Router};
//
// // Create inference pipeline with all optimizations
// let pipeline = InferencePipeline::builder()
//     .with_batching(BatchConfig { max_size: 8, timeout_ms: 50 })
//     .with_prompt_cache(1000)
//     .with_response_cache(CacheConfig { capacity: 10000, ttl_ms: 3600000 })
//     .with_routing(RouterConfig::default())
//     .build();
//
// // Process requests - optimizations applied automatically
// let result = pipeline.infer(request).await?;
// ```
//
// # Hardware Backends
//
// The library supports multiple backends via feature flags:
// - `cpu`: Pure CPU inference (default)
// - `cuda`: NVIDIA GPU acceleration
// - `neuron`: AWS Inferentia/Trainium
// - `metal`: Apple Silicon (M1/M2/M3)

pub use batcher::{Batcher, BatchConfig, BatchHandle};
pub use cache::{PromptCache, ResponseCache, CacheConfig, CacheStats};
pub use router::{Router, RouterConfig, ModelTier, QueryComplexity};
pub use pool::{InferencePool, PoolConfig, ModelHandle};
pub use pipeline::{InferencePipeline, PipelineBuilder, InferenceRequest, InferenceResult};
pub use metrics::{InferenceMetrics, MetricsCollector};

mod batcher;
mod cache;
mod router;
mod pool;
mod pipeline;
mod metrics;

// =============================================================================
// Core Traits - Generic Interfaces for Extensibility
// =============================================================================

/// Trait for types that can be batched together
pub trait Batchable: Clone + Send + Sync + 'static {
    /// Unique identifier for deduplication
    fn batch_key(&self) -> String;

    /// Priority for ordering within batch (higher = first)
    fn priority(&self) -> u32 { 0 }

    /// Maximum time this request can wait in queue (ms)
    fn max_wait_ms(&self) -> u64 { 1000 }
}

/// Trait for model backends that support batched inference
pub trait BatchInference: Send + Sync + 'static {
    type Request: Batchable;
    type Response: Clone + Send + Sync + 'static;
    type Error: std::error::Error + Send + Sync + 'static;

    /// Process a batch of requests
    fn infer_batch(&self, requests: Vec<Self::Request>) -> Result<Vec<Self::Response>, Self::Error>;

    /// Process a batch of requests asynchronously
    async fn infer_batch_async(&self, requests: Vec<Self::Request>) -> Result<Vec<Self::Response>, Self::Error> {
        // Default implementation: wrap sync in spawn_blocking
        let this = self.clone();
        spawn_blocking(move || this.infer_batch(requests)).await?
    }

    /// Maximum batch size supported by this backend
    fn max_batch_size(&self) -> usize { 8 }

    /// Whether this backend supports streaming output
    fn supports_streaming(&self) -> bool { false }
}

/// Trait for cacheable responses
pub trait Cacheable: Clone + Send + Sync + 'static {
    /// Compute cache key from request
    fn cache_key(&self) -> String;

    /// Whether this response should be cached
    fn is_cacheable(&self) -> bool { true }

    /// TTL override for this specific response (None = use default)
    fn ttl_ms(&self) -> Option<u64> { None }
}

/// Trait for query complexity analysis
pub trait ComplexityAnalyzer: Send + Sync + 'static {
    type Query;

    /// Analyze query and return complexity score (0.0 - 1.0)
    fn analyze(&self, query: &Self::Query) -> f32;

    /// Determine model tier based on complexity
    fn select_tier(&self, complexity: f32) -> ModelTier {
        match complexity {
            c if c < 0.2 => ModelTier::Tiny,
            c if c < 0.5 => ModelTier::Light,
            _ => ModelTier::Full,
        }
    }
}

// =============================================================================
// Native FFI Declarations
// =============================================================================

// These are implemented in the C runtime via llama.cpp bindings

/// Create a new model instance
fn slm_model_load(path: &str, config: &ModelLoadConfig) -> Result<ModelHandle, SlmError>;

/// Unload a model and free resources
fn slm_model_unload(handle: ModelHandle);

/// Run batch inference
fn slm_batch_infer(
    handle: ModelHandle,
    prompts: &[String],
    config: &InferConfig,
) -> Result<Vec<String>, SlmError>;

/// Run streaming inference (single request)
fn slm_stream_infer(
    handle: ModelHandle,
    prompt: &str,
    config: &InferConfig,
    callback: impl FnMut(&str) -> bool,
) -> Result<(), SlmError>;

/// Tokenize a prompt (for caching)
fn slm_tokenize(handle: ModelHandle, text: &str) -> Result<Vec<i32>, SlmError>;

/// Detokenize tokens back to text
fn slm_detokenize(handle: ModelHandle, tokens: &[i32]) -> Result<String, SlmError>;

/// Get model metadata
fn slm_model_info(handle: ModelHandle) -> ModelInfo;

// =============================================================================
// Configuration Types
// =============================================================================

#[derive(Clone)]
pub struct ModelLoadConfig {
    /// Number of GPU layers (0 = CPU only)
    pub gpu_layers: i32,
    /// Context size in tokens
    pub context_size: u32,
    /// Number of threads for CPU inference
    pub threads: u32,
    /// Use memory mapping for model weights
    pub use_mmap: bool,
    /// Use memory locking for model weights
    pub use_mlock: bool,
    /// Flash attention (requires compatible GPU)
    pub flash_attention: bool,
}

impl Default for ModelLoadConfig {
    fn default() -> Self {
        ModelLoadConfig {
            gpu_layers: 0,
            context_size: 4096,
            threads: 4,
            use_mmap: true,
            use_mlock: false,
            flash_attention: false,
        }
    }
}

#[derive(Clone)]
pub struct InferConfig {
    /// Maximum tokens to generate
    pub max_tokens: u32,
    /// Temperature for sampling
    pub temperature: f32,
    /// Top-p (nucleus) sampling
    pub top_p: f32,
    /// Top-k sampling
    pub top_k: i32,
    /// Repetition penalty
    pub repeat_penalty: f32,
    /// Stop sequences
    pub stop_sequences: Vec<String>,
}

impl Default for InferConfig {
    fn default() -> Self {
        InferConfig {
            max_tokens: 512,
            temperature: 0.7,
            top_p: 0.9,
            top_k: 40,
            repeat_penalty: 1.1,
            stop_sequences: vec![],
        }
    }
}

pub struct ModelInfo {
    pub name: String,
    pub parameters: u64,
    pub context_size: u32,
    pub embedding_size: u32,
    pub vocab_size: u32,
    pub quantization: String,
}

#[derive(Debug)]
pub enum SlmError {
    ModelNotFound(String),
    LoadError(String),
    InferenceError(String),
    TokenizationError(String),
    OutOfMemory,
    Timeout,
}

impl std::error::Error for SlmError {}

impl std::fmt::Display for SlmError {
    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {
        match self {
            SlmError::ModelNotFound(path) => write!(f, "Model not found: {}", path),
            SlmError::LoadError(msg) => write!(f, "Model load error: {}", msg),
            SlmError::InferenceError(msg) => write!(f, "Inference error: {}", msg),
            SlmError::TokenizationError(msg) => write!(f, "Tokenization error: {}", msg),
            SlmError::OutOfMemory => write!(f, "Out of memory"),
            SlmError::Timeout => write!(f, "Operation timed out"),
        }
    }
}

// Async helpers
async fn spawn_blocking<F, T>(f: F) -> Result<T, SlmError>
where
    F: FnOnce() -> T + Send + 'static,
    T: Send + 'static,
{
    simplex_std::task::spawn_blocking(f).await
        .map_err(|_| SlmError::InferenceError("Task panicked".to_string()))
}
