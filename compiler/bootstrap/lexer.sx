// Simplex Lexer
// Self-hosted lexer with full token support for all language features

// Token types as enum
enum TokenKind {
    Eof,
    Ident,
    Int,
    Float,
    String,
    FString,   // f"..." interpolated string
    LParen,
    RParen,
    LBrace,
    RBrace,
    LBracket,
    RBracket,
    Comma,
    Colon,
    Semi,
    Arrow,
    DoubleColon,
    DotDot,
    Eq,
    EqEq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
    Plus,
    Minus,
    Star,
    Slash,
    Bang,
    Dot,
    KwFn,
    KwLet,
    KwIf,
    KwElse,
    KwWhile,
    KwFor,
    KwIn,
    KwReturn,
    KwEnum,
    KwTrue,
    KwFalse,
    KwPub,
    KwStruct,
    KwImpl,
    KwSelf,
    KwMatch,
    FatArrow,
    Underscore,
    AmpAmp,
    PipePipe,
    Percent,
    Amp,
    Pipe,
    Caret,
    LtLt,
    GtGt,
    KwBreak,
    KwContinue,
    Question,
    KwTrait,
    KwType,
    KwUse,
    KwMod,
    KwConst,
    KwAsync,
    KwAwait,
    KwYield,
    KwDyn,
    // Actor model keywords
    KwActor,
    KwReceive,
    KwInit,
    // AI/Cognitive keywords
    KwSpecialist,
    KwInfer,
    KwHive,
    // Mutable binding
    KwVar,
    // Spawn expression
    KwSpawn
}

// Lexer state - we pass this to all functions instead of using self
fn lexer_new(source: i64) -> i64 {
    // Allocate lexer state: source(0), pos(1), len(2)
    let lexer: i64 = malloc(24);
    store_ptr(lexer, 0, source);
    store_i64(lexer, 1, 0);
    let len: i64 = string_len(source);
    store_i64(lexer, 2, len);
    return lexer;
}

fn lexer_pos(lexer: i64) -> i64 {
    return load_i64(lexer, 1);
}

fn lexer_len(lexer: i64) -> i64 {
    return load_i64(lexer, 2);
}

fn lexer_source(lexer: i64) -> i64 {
    return load_ptr(lexer, 0);
}

fn lexer_at_end(lexer: i64) -> bool {
    let pos: i64 = lexer_pos(lexer);
    let len: i64 = lexer_len(lexer);
    return pos >= len;
}

fn lexer_peek(lexer: i64) -> i64 {
    if lexer_at_end(lexer) {
        return 0;
    }
    let src: i64 = lexer_source(lexer);
    let pos: i64 = lexer_pos(lexer);
    return string_char_at(src, pos);
}

fn lexer_advance(lexer: i64) -> i64 {
    let c: i64 = lexer_peek(lexer);
    if c != 0 {
        let pos: i64 = lexer_pos(lexer);
        store_i64(lexer, 1, pos + 1);
    }
    return c;
}

fn is_whitespace(c: i64) -> bool {
    if c == 32 { return true; }
    if c == 9 { return true; }
    if c == 10 { return true; }
    if c == 13 { return true; }
    return false;
}

fn is_digit(c: i64) -> bool {
    if c >= 48 {
        if c <= 57 { return true; }
    }
    return false;
}

fn is_alpha(c: i64) -> bool {
    // a-z: 97-122
    if c >= 97 {
        if c <= 122 { return true; }
    }
    // A-Z: 65-90
    if c >= 65 {
        if c <= 90 { return true; }
    }
    return false;
}

fn is_ident_start(c: i64) -> bool {
    if is_alpha(c) { return true; }
    if c == 95 { return true; }
    return false;
}

fn is_ident_continue(c: i64) -> bool {
    if is_ident_start(c) { return true; }
    if is_digit(c) { return true; }
    return false;
}

fn lexer_skip_whitespace(lexer: i64) -> i64 {
    while is_whitespace(lexer_peek(lexer)) {
        lexer_advance(lexer);
    }
    return 0;
}

fn lexer_skip_line_comment(lexer: i64) -> i64 {
    while lexer_peek(lexer) != 10 {
        if lexer_at_end(lexer) {
            return 0;
        }
        lexer_advance(lexer);
    }
    return 0;
}

// Create a token - returns ptr to token struct
// Token: kind(0), text(1), pos(2)
fn token_new(kind: i64, text: i64, pos: i64) -> i64 {
    let tok: i64 = malloc(24);
    store_i64(tok, 0, kind);
    store_ptr(tok, 1, text);
    store_i64(tok, 2, pos);
    return tok;
}

fn token_kind(tok: i64) -> i64 {
    return load_i64(tok, 0);
}

fn token_text(tok: i64) -> i64 {
    return load_ptr(tok, 1);
}

// Check if identifier matches keyword
fn check_keyword(text: i64) -> i64 {
    if string_eq(text, "fn") { return TokenKind::KwFn; }
    if string_eq(text, "let") { return TokenKind::KwLet; }
    if string_eq(text, "if") { return TokenKind::KwIf; }
    if string_eq(text, "else") { return TokenKind::KwElse; }
    if string_eq(text, "while") { return TokenKind::KwWhile; }
    if string_eq(text, "return") { return TokenKind::KwReturn; }
    if string_eq(text, "enum") { return TokenKind::KwEnum; }
    if string_eq(text, "true") { return TokenKind::KwTrue; }
    if string_eq(text, "false") { return TokenKind::KwFalse; }
    if string_eq(text, "pub") { return TokenKind::KwPub; }
    if string_eq(text, "struct") { return TokenKind::KwStruct; }
    if string_eq(text, "for") { return TokenKind::KwFor; }
    if string_eq(text, "in") { return TokenKind::KwIn; }
    if string_eq(text, "impl") { return TokenKind::KwImpl; }
    if string_eq(text, "self") { return TokenKind::KwSelf; }
    if string_eq(text, "match") { return TokenKind::KwMatch; }
    if string_eq(text, "break") { return TokenKind::KwBreak; }
    if string_eq(text, "continue") { return TokenKind::KwContinue; }
    if string_eq(text, "trait") { return TokenKind::KwTrait; }
    if string_eq(text, "type") { return TokenKind::KwType; }
    if string_eq(text, "use") { return TokenKind::KwUse; }
    if string_eq(text, "mod") { return TokenKind::KwMod; }
    if string_eq(text, "const") { return TokenKind::KwConst; }
    if string_eq(text, "async") { return TokenKind::KwAsync; }
    if string_eq(text, "await") { return TokenKind::KwAwait; }
    if string_eq(text, "yield") { return TokenKind::KwYield; }
    if string_eq(text, "dyn") { return TokenKind::KwDyn; }
    // Actor model keywords
    if string_eq(text, "actor") { return TokenKind::KwActor; }
    if string_eq(text, "receive") { return TokenKind::KwReceive; }
    if string_eq(text, "init") { return TokenKind::KwInit; }
    // AI/Cognitive keywords
    if string_eq(text, "specialist") { return TokenKind::KwSpecialist; }
    if string_eq(text, "infer") { return TokenKind::KwInfer; }
    if string_eq(text, "hive") { return TokenKind::KwHive; }
    // Mutable binding
    if string_eq(text, "var") { return TokenKind::KwVar; }
    // Spawn expression
    if string_eq(text, "spawn") { return TokenKind::KwSpawn; }
    if string_eq(text, "_") { return TokenKind::Underscore; }
    return TokenKind::Ident;
}

// Main tokenizer function
fn lexer_next_token(lexer: i64) -> i64 {
    lexer_skip_whitespace(lexer);

    if lexer_at_end(lexer) {
        return token_new(TokenKind::Eof, string_from(""), lexer_pos(lexer));
    }

    let start: i64 = lexer_pos(lexer);
    let c: i64 = lexer_advance(lexer);

    // Single character tokens
    if c == 40 { return token_new(TokenKind::LParen, string_from("("), start); }
    if c == 41 { return token_new(TokenKind::RParen, string_from(")"), start); }
    if c == 123 { return token_new(TokenKind::LBrace, string_from("{"), start); }
    if c == 125 { return token_new(TokenKind::RBrace, string_from("}"), start); }
    if c == 91 { return token_new(TokenKind::LBracket, string_from("["), start); }
    if c == 93 { return token_new(TokenKind::RBracket, string_from("]"), start); }
    if c == 44 { return token_new(TokenKind::Comma, string_from(","), start); }
    if c == 59 { return token_new(TokenKind::Semi, string_from(";"), start); }
    if c == 43 { return token_new(TokenKind::Plus, string_from("+"), start); }
    if c == 42 { return token_new(TokenKind::Star, string_from("*"), start); }
    if c == 37 { return token_new(TokenKind::Percent, string_from("%"), start); }
    if c == 38 {
        if lexer_peek(lexer) == 38 {
            lexer_advance(lexer);
            return token_new(TokenKind::AmpAmp, string_from("&&"), start);
        }
        return token_new(TokenKind::Amp, string_from("&"), start);
    }
    if c == 124 {
        if lexer_peek(lexer) == 124 {
            lexer_advance(lexer);
            return token_new(TokenKind::PipePipe, string_from("||"), start);
        }
        return token_new(TokenKind::Pipe, string_from("|"), start);
    }
    if c == 94 {
        return token_new(TokenKind::Caret, string_from("^"), start);
    }
    if c == 46 {
        if lexer_peek(lexer) == 46 {
            lexer_advance(lexer);
            return token_new(TokenKind::DotDot, string_from(".."), start);
        }
        return token_new(TokenKind::Dot, string_from("."), start);
    }

    // Two character tokens
    if c == 58 {
        if lexer_peek(lexer) == 58 {
            lexer_advance(lexer);
            return token_new(TokenKind::DoubleColon, string_from("::"), start);
        }
        return token_new(TokenKind::Colon, string_from(":"), start);
    }

    if c == 45 {
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new(TokenKind::Arrow, string_from("->"), start);
        }
        return token_new(TokenKind::Minus, string_from("-"), start);
    }

    if c == 61 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new(TokenKind::EqEq, string_from("=="), start);
        }
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new(TokenKind::FatArrow, string_from("=>"), start);
        }
        return token_new(TokenKind::Eq, string_from("="), start);
    }

    if c == 33 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new(TokenKind::Ne, string_from("!="), start);
        }
        return token_new(TokenKind::Bang, string_from("!"), start);
    }

    // ? for try/error propagation
    if c == 63 {
        return token_new(TokenKind::Question, string_from("?"), start);
    }

    if c == 60 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new(TokenKind::Le, string_from("<="), start);
        }
        if lexer_peek(lexer) == 60 {
            lexer_advance(lexer);
            return token_new(TokenKind::LtLt, string_from("<<"), start);
        }
        return token_new(TokenKind::Lt, string_from("<"), start);
    }

    if c == 62 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new(TokenKind::Ge, string_from(">="), start);
        }
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new(TokenKind::GtGt, string_from(">>"), start);
        }
        return token_new(TokenKind::Gt, string_from(">"), start);
    }

    if c == 47 {
        if lexer_peek(lexer) == 47 {
            lexer_skip_line_comment(lexer);
            return lexer_next_token(lexer);
        }
        return token_new(TokenKind::Slash, string_from("/"), start);
    }

    // f-string literals: f"..." with {expr} interpolation
    // Check for 'f' followed by '"'
    if c == 102 {
        if lexer_peek(lexer) == 34 {
            lexer_advance(lexer);  // consume the opening "
            let text: i64 = string_from("");
            while lexer_peek(lexer) != 34 {
                if lexer_at_end(lexer) {
                    return token_new(TokenKind::Eof, string_from(""), start);
                }
                let sc: i64 = lexer_advance(lexer);
                // Handle escape sequences
                if sc == 92 {
                    if lexer_at_end(lexer) == false {
                        let esc: i64 = lexer_advance(lexer);
                        if esc == 110 {
                            text = string_concat(text, string_from_char(10));
                        } else {
                            if esc == 116 {
                                text = string_concat(text, string_from_char(9));
                            } else {
                                if esc == 34 {
                                    text = string_concat(text, string_from_char(34));
                                } else {
                                    if esc == 92 {
                                        text = string_concat(text, string_from_char(92));
                                    } else {
                                        if esc == 123 {
                                            // \{ -> literal brace
                                            text = string_concat(text, string_from_char(123));
                                        } else {
                                            if esc == 125 {
                                                // \} -> literal brace
                                                text = string_concat(text, string_from_char(125));
                                            } else {
                                                text = string_concat(text, string_from_char(esc));
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                } else {
                    text = string_concat(text, string_from_char(sc));
                }
            }
            lexer_advance(lexer);  // consume closing "
            return token_new(TokenKind::FString, text, start);
        }
    }

    // Identifiers and keywords
    if is_ident_start(c) {
        while is_ident_continue(lexer_peek(lexer)) {
            lexer_advance(lexer);
        }
        let end: i64 = lexer_pos(lexer);
        let text: i64 = string_slice(lexer_source(lexer), start, end);
        let kind: i64 = check_keyword(text);
        return token_new(kind, text, start);
    }

    // Numbers (int or float)
    if is_digit(c) {
        while is_digit(lexer_peek(lexer)) {
            lexer_advance(lexer);
        }
        // Check for decimal point followed by digits (float)
        if lexer_peek(lexer) == 46 {
            // Look ahead to see if there's a digit after the dot
            let saved_pos: i64 = lexer_pos(lexer);
            lexer_advance(lexer);  // consume .
            if is_digit(lexer_peek(lexer)) {
                // It's a float
                while is_digit(lexer_peek(lexer)) {
                    lexer_advance(lexer);
                }
                let end: i64 = lexer_pos(lexer);
                let text: i64 = string_slice(lexer_source(lexer), start, end);
                return token_new(TokenKind::Float, text, start);
            } else {
                // Not a float, backtrack (it's an int followed by a dot)
                // We need to restore position - but our simple lexer doesn't support this well
                // So just return the int up to the dot
                let end: i64 = saved_pos;
                let text: i64 = string_slice(lexer_source(lexer), start, end);
                // Reset position to before the dot (lexer_pos is now at saved_pos+1, we need saved_pos)
                store_i64(lexer, 1, saved_pos);
                return token_new(TokenKind::Int, text, start);
            }
        }
        let end: i64 = lexer_pos(lexer);
        let text: i64 = string_slice(lexer_source(lexer), start, end);
        return token_new(TokenKind::Int, text, start);
    }

    // String literals - build string with escape sequence conversion
    if c == 34 {
        let text: i64 = string_from("");
        while lexer_peek(lexer) != 34 {
            if lexer_at_end(lexer) {
                return token_new(TokenKind::Eof, string_from(""), start);
            }
            let sc: i64 = lexer_advance(lexer);
            // Handle escape sequences
            if sc == 92 {
                if lexer_at_end(lexer) == false {
                    let esc: i64 = lexer_advance(lexer);
                    if esc == 110 {
                        // \n -> newline (ASCII 10)
                        text = string_concat(text, string_from_char(10));
                    } else {
                        if esc == 116 {
                            // \t -> tab (ASCII 9)
                            text = string_concat(text, string_from_char(9));
                        } else {
                            if esc == 34 {
                                // \" -> quote
                                text = string_concat(text, string_from_char(34));
                            } else {
                                if esc == 92 {
                                    // \\ -> backslash
                                    text = string_concat(text, string_from_char(92));
                                } else {
                                    // Unknown escape - just add the char
                                    text = string_concat(text, string_from_char(esc));
                                }
                            }
                        }
                    }
                }
            } else {
                text = string_concat(text, string_from_char(sc));
            }
        }
        lexer_advance(lexer);
        return token_new(TokenKind::String, text, start);
    }

    // Unknown character - return EOF for now
    return token_new(TokenKind::Eof, string_from(""), start);
}

// Tokenize entire source into a list
fn tokenize(source: i64) -> i64 {
    let lexer: i64 = lexer_new(source);
    let tokens: i64 = vec_new();

    while true {
        let tok: i64 = lexer_next_token(lexer);
        vec_push(tokens, tok);
        if token_kind(tok) == TokenKind::Eof {
            return tokens;
        }
    }

    return tokens;
}
