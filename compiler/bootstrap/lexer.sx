// Simplex Lexer
// Self-hosted lexer with full token support for all language features
//
// Copyright (c) 2025-2026 Rod Higgins
// Licensed under AGPL-3.0 - see LICENSE file
// https://github.com/senuamedia/simplex-lang

// Token types as enum
enum TokenKind {
    Eof,
    Ident,
    Int,
    Float,
    String,
    FString,   // f"..." interpolated string
    LParen,
    RParen,
    LBrace,
    RBrace,
    LBracket,
    RBracket,
    Comma,
    Colon,
    Semi,
    Arrow,
    DoubleColon,
    DotDot,
    Eq,
    EqEq,
    Ne,
    Lt,
    Gt,
    Le,
    Ge,
    Plus,
    Minus,
    Star,
    Slash,
    Bang,
    Dot,
    KwFn,
    KwLet,
    KwIf,
    KwElse,
    KwWhile,
    KwFor,
    KwIn,
    KwReturn,
    KwEnum,
    KwTrue,
    KwFalse,
    KwPub,
    KwStruct,
    KwImpl,
    KwSelf,
    KwMatch,
    FatArrow,
    Underscore,
    AmpAmp,
    PipePipe,
    Percent,
    Amp,
    Pipe,
    Caret,
    LtLt,
    GtGt,
    KwBreak,
    KwLoop,
    KwContinue,
    Question,
    KwTrait,
    KwType,
    KwUse,
    KwMod,
    KwConst,
    KwAsync,
    KwAwait,
    KwYield,
    KwDyn,
    // Actor model keywords
    KwActor,
    KwReceive,
    KwInit,
    // AI/Cognitive keywords
    KwSpecialist,
    KwInfer,
    KwHive,
    KwAnima,
    // Mutable binding
    KwVar,
    // Spawn expression
    KwSpawn,
    // Where clause
    KwWhere,
    // Belief guard tokens (TASK-013-A)
    At,              // @ symbol for belief guards
    KwConfidence,    // confidence(belief_id)
    KwDerivative     // .derivative accessor
}

// Lexer state - we pass this to all functions instead of using self
fn lexer_new(source: i64) -> i64 {
    // Allocate lexer state: source(0), pos(1), len(2), line(3), col(4)
    let lexer: i64 = malloc(40);
    store_ptr(lexer, 0, source);
    store_i64(lexer, 1, 0);
    let len: i64 = string_len(source);
    store_i64(lexer, 2, len);
    store_i64(lexer, 3, 1);  // line starts at 1
    store_i64(lexer, 4, 1);  // col starts at 1
    return lexer;
}

fn lexer_pos(lexer: i64) -> i64 {
    return load_i64(lexer, 1);
}

fn lexer_len(lexer: i64) -> i64 {
    return load_i64(lexer, 2);
}

fn lexer_line(lexer: i64) -> i64 {
    return load_i64(lexer, 3);
}

fn lexer_col(lexer: i64) -> i64 {
    return load_i64(lexer, 4);
}

fn lexer_source(lexer: i64) -> i64 {
    return load_ptr(lexer, 0);
}

fn lexer_at_end(lexer: i64) -> bool {
    let pos: i64 = lexer_pos(lexer);
    let len: i64 = lexer_len(lexer);
    return pos >= len;
}

fn lexer_peek(lexer: i64) -> i64 {
    if lexer_at_end(lexer) {
        return 0;
    }
    let src: i64 = lexer_source(lexer);
    let pos: i64 = lexer_pos(lexer);
    return string_char_at(src, pos);
}

fn lexer_advance(lexer: i64) -> i64 {
    let c: i64 = lexer_peek(lexer);
    if c != 0 {
        let pos: i64 = lexer_pos(lexer);
        store_i64(lexer, 1, pos + 1);
        // Track line/column
        if c == 10 {
            // Newline: increment line, reset column
            store_i64(lexer, 3, load_i64(lexer, 3) + 1);
            store_i64(lexer, 4, 1);
        } else {
            // Other char: increment column
            store_i64(lexer, 4, load_i64(lexer, 4) + 1);
        }
    }
    return c;
}

fn is_whitespace(c: i64) -> bool {
    if c == 32 { return true; }
    if c == 9 { return true; }
    if c == 10 { return true; }
    if c == 13 { return true; }
    return false;
}

fn is_digit(c: i64) -> bool {
    if c >= 48 {
        if c <= 57 { return true; }
    }
    return false;
}

fn is_alpha(c: i64) -> bool {
    // a-z: 97-122
    if c >= 97 {
        if c <= 122 { return true; }
    }
    // A-Z: 65-90
    if c >= 65 {
        if c <= 90 { return true; }
    }
    return false;
}

fn is_ident_start(c: i64) -> bool {
    if is_alpha(c) { return true; }
    if c == 95 { return true; }
    return false;
}

fn is_ident_continue(c: i64) -> bool {
    if is_ident_start(c) { return true; }
    if is_digit(c) { return true; }
    return false;
}

fn lexer_skip_whitespace(lexer: i64) -> i64 {
    while is_whitespace(lexer_peek(lexer)) {
        lexer_advance(lexer);
    }
    return 0;
}

fn lexer_skip_line_comment(lexer: i64) -> i64 {
    while lexer_peek(lexer) != 10 {
        if lexer_at_end(lexer) {
            return 0;
        }
        lexer_advance(lexer);
    }
    return 0;
}

// Create a token - returns ptr to token struct
// Token: kind(0), text(1), pos(2), line(3), col(4)
fn token_new(kind: i64, text: i64, pos: i64) -> i64 {
    let tok: i64 = malloc(40);
    store_i64(tok, 0, kind);
    store_ptr(tok, 1, text);
    store_i64(tok, 2, pos);
    store_i64(tok, 3, 0);  // line (set by token_new_loc)
    store_i64(tok, 4, 0);  // col (set by token_new_loc)
    return tok;
}

// Create a token with line/column info
fn token_new_loc(kind: i64, text: i64, pos: i64, line: i64, col: i64) -> i64 {
    let tok: i64 = malloc(40);
    store_i64(tok, 0, kind);
    store_ptr(tok, 1, text);
    store_i64(tok, 2, pos);
    store_i64(tok, 3, line);
    store_i64(tok, 4, col);
    return tok;
}

fn token_kind(tok: i64) -> i64 {
    return load_i64(tok, 0);
}

fn token_text(tok: i64) -> i64 {
    return load_ptr(tok, 1);
}

fn token_line(tok: i64) -> i64 {
    return load_i64(tok, 3);
}

fn token_col(tok: i64) -> i64 {
    return load_i64(tok, 4);
}

// Check if identifier matches keyword
fn check_keyword(text: i64) -> i64 {
    if string_eq(text, "fn") { return TokenKind::KwFn; }
    if string_eq(text, "let") { return TokenKind::KwLet; }
    if string_eq(text, "if") { return TokenKind::KwIf; }
    if string_eq(text, "else") { return TokenKind::KwElse; }
    if string_eq(text, "while") { return TokenKind::KwWhile; }
    if string_eq(text, "return") { return TokenKind::KwReturn; }
    if string_eq(text, "enum") { return TokenKind::KwEnum; }
    if string_eq(text, "true") { return TokenKind::KwTrue; }
    if string_eq(text, "false") { return TokenKind::KwFalse; }
    if string_eq(text, "pub") { return TokenKind::KwPub; }
    if string_eq(text, "struct") { return TokenKind::KwStruct; }
    if string_eq(text, "for") { return TokenKind::KwFor; }
    if string_eq(text, "in") { return TokenKind::KwIn; }
    if string_eq(text, "impl") { return TokenKind::KwImpl; }
    if string_eq(text, "self") { return TokenKind::KwSelf; }
    if string_eq(text, "match") { return TokenKind::KwMatch; }
    if string_eq(text, "break") { return TokenKind::KwBreak; }
    if string_eq(text, "loop") { return TokenKind::KwLoop; }
    if string_eq(text, "continue") { return TokenKind::KwContinue; }
    if string_eq(text, "trait") { return TokenKind::KwTrait; }
    if string_eq(text, "type") { return TokenKind::KwType; }
    if string_eq(text, "use") { return TokenKind::KwUse; }
    if string_eq(text, "mod") { return TokenKind::KwMod; }
    if string_eq(text, "const") { return TokenKind::KwConst; }
    if string_eq(text, "async") { return TokenKind::KwAsync; }
    if string_eq(text, "await") { return TokenKind::KwAwait; }
    if string_eq(text, "yield") { return TokenKind::KwYield; }
    if string_eq(text, "dyn") { return TokenKind::KwDyn; }
    // Actor model keywords
    if string_eq(text, "actor") { return TokenKind::KwActor; }
    if string_eq(text, "receive") { return TokenKind::KwReceive; }
    if string_eq(text, "init") { return TokenKind::KwInit; }
    // AI/Cognitive keywords
    if string_eq(text, "specialist") { return TokenKind::KwSpecialist; }
    if string_eq(text, "infer") { return TokenKind::KwInfer; }
    if string_eq(text, "hive") { return TokenKind::KwHive; }
    if string_eq(text, "anima") { return TokenKind::KwAnima; }
    // Mutable binding
    if string_eq(text, "var") { return TokenKind::KwVar; }
    // Spawn expression
    if string_eq(text, "spawn") { return TokenKind::KwSpawn; }
    // Where clause
    if string_eq(text, "where") { return TokenKind::KwWhere; }
    // Belief guard keywords (TASK-013-A)
    if string_eq(text, "confidence") { return TokenKind::KwConfidence; }
    if string_eq(text, "derivative") { return TokenKind::KwDerivative; }
    if string_eq(text, "_") { return TokenKind::Underscore; }
    return TokenKind::Ident;
}

// Main tokenizer function
fn lexer_next_token(lexer: i64) -> i64 {
    lexer_skip_whitespace(lexer);

    if lexer_at_end(lexer) {
        return token_new_loc(TokenKind::Eof, string_from(""), lexer_pos(lexer), lexer_line(lexer), lexer_col(lexer));
    }

    let start: i64 = lexer_pos(lexer);
    let start_line: i64 = lexer_line(lexer);
    let start_col: i64 = lexer_col(lexer);
    let c: i64 = lexer_advance(lexer);

    // Single character tokens
    if c == 40 { return token_new_loc(TokenKind::LParen, string_from("("), start, start_line, start_col); }
    if c == 41 { return token_new_loc(TokenKind::RParen, string_from(")"), start, start_line, start_col); }
    if c == 123 { return token_new_loc(TokenKind::LBrace, string_from("{"), start, start_line, start_col); }
    if c == 125 { return token_new_loc(TokenKind::RBrace, string_from("}"), start, start_line, start_col); }
    if c == 91 { return token_new_loc(TokenKind::LBracket, string_from("["), start, start_line, start_col); }
    if c == 93 { return token_new_loc(TokenKind::RBracket, string_from("]"), start, start_line, start_col); }
    if c == 44 { return token_new_loc(TokenKind::Comma, string_from(","), start, start_line, start_col); }
    if c == 59 { return token_new_loc(TokenKind::Semi, string_from(";"), start, start_line, start_col); }
    if c == 43 { return token_new_loc(TokenKind::Plus, string_from("+"), start, start_line, start_col); }
    if c == 42 { return token_new_loc(TokenKind::Star, string_from("*"), start, start_line, start_col); }
    if c == 37 { return token_new_loc(TokenKind::Percent, string_from("%"), start, start_line, start_col); }
    if c == 38 {
        if lexer_peek(lexer) == 38 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::AmpAmp, string_from("&&"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Amp, string_from("&"), start, start_line, start_col);
    }
    if c == 124 {
        if lexer_peek(lexer) == 124 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::PipePipe, string_from("||"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Pipe, string_from("|"), start, start_line, start_col);
    }
    if c == 94 {
        return token_new_loc(TokenKind::Caret, string_from("^"), start, start_line, start_col);
    }
    if c == 46 {
        if lexer_peek(lexer) == 46 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::DotDot, string_from(".."), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Dot, string_from("."), start, start_line, start_col);
    }

    // Two character tokens
    if c == 58 {
        if lexer_peek(lexer) == 58 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::DoubleColon, string_from("::"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Colon, string_from(":"), start, start_line, start_col);
    }

    if c == 45 {
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::Arrow, string_from("->"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Minus, string_from("-"), start, start_line, start_col);
    }

    if c == 61 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::EqEq, string_from("=="), start, start_line, start_col);
        }
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::FatArrow, string_from("=>"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Eq, string_from("="), start, start_line, start_col);
    }

    if c == 33 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::Ne, string_from("!="), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Bang, string_from("!"), start, start_line, start_col);
    }

    // ? for try/error propagation
    if c == 63 {
        return token_new_loc(TokenKind::Question, string_from("?"), start, start_line, start_col);
    }

    if c == 60 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::Le, string_from("<="), start, start_line, start_col);
        }
        if lexer_peek(lexer) == 60 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::LtLt, string_from("<<"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Lt, string_from("<"), start, start_line, start_col);
    }

    if c == 62 {
        if lexer_peek(lexer) == 61 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::Ge, string_from(">="), start, start_line, start_col);
        }
        if lexer_peek(lexer) == 62 {
            lexer_advance(lexer);
            return token_new_loc(TokenKind::GtGt, string_from(">>"), start, start_line, start_col);
        }
        return token_new_loc(TokenKind::Gt, string_from(">"), start, start_line, start_col);
    }

    if c == 47 {
        if lexer_peek(lexer) == 47 {
            lexer_skip_line_comment(lexer);
            return lexer_next_token(lexer);
        }
        return token_new_loc(TokenKind::Slash, string_from("/"), start, start_line, start_col);
    }

    // f-string literals: f"..." with {expr} interpolation
    // Check for 'f' followed by '"'
    if c == 102 {
        if lexer_peek(lexer) == 34 {
            lexer_advance(lexer);  // consume the opening "
            let text: i64 = string_from("");
            while lexer_peek(lexer) != 34 {
                if lexer_at_end(lexer) {
                    return token_new_loc(TokenKind::Eof, string_from(""), start, start_line, start_col);
                }
                let sc: i64 = lexer_advance(lexer);
                // Handle escape sequences
                if sc == 92 {
                    if lexer_at_end(lexer) == false {
                        let esc: i64 = lexer_advance(lexer);
                        if esc == 110 {
                            text = string_concat(text, string_from_char(10));
                        } else {
                            if esc == 116 {
                                text = string_concat(text, string_from_char(9));
                            } else {
                                if esc == 34 {
                                    text = string_concat(text, string_from_char(34));
                                } else {
                                    if esc == 92 {
                                        text = string_concat(text, string_from_char(92));
                                    } else {
                                        if esc == 123 {
                                            // \{ -> literal brace
                                            text = string_concat(text, string_from_char(123));
                                        } else {
                                            if esc == 125 {
                                                // \} -> literal brace
                                                text = string_concat(text, string_from_char(125));
                                            } else {
                                                text = string_concat(text, string_from_char(esc));
                                            }
                                        }
                                    }
                                }
                            }
                        }
                    }
                } else {
                    text = string_concat(text, string_from_char(sc));
                }
            }
            lexer_advance(lexer);  // consume closing "
            return token_new_loc(TokenKind::FString, text, start, start_line, start_col);
        }
    }

    // Identifiers and keywords
    if is_ident_start(c) {
        while is_ident_continue(lexer_peek(lexer)) {
            lexer_advance(lexer);
        }
        let end: i64 = lexer_pos(lexer);
        let text: i64 = string_slice(lexer_source(lexer), start, end);
        let kind: i64 = check_keyword(text);
        return token_new_loc(kind, text, start, start_line, start_col);
    }

    // Numbers (int or float)
    if is_digit(c) {
        while is_digit(lexer_peek(lexer)) {
            lexer_advance(lexer);
        }
        // Check for decimal point followed by digits (float)
        if lexer_peek(lexer) == 46 {
            // Look ahead to see if there's a digit after the dot
            let saved_pos: i64 = lexer_pos(lexer);
            lexer_advance(lexer);  // consume .
            if is_digit(lexer_peek(lexer)) {
                // It's a float
                while is_digit(lexer_peek(lexer)) {
                    lexer_advance(lexer);
                }
                let end: i64 = lexer_pos(lexer);
                let text: i64 = string_slice(lexer_source(lexer), start, end);
                return token_new_loc(TokenKind::Float, text, start, start_line, start_col);
            } else {
                // Not a float, backtrack (it's an int followed by a dot)
                // We need to restore position - but our simple lexer doesn't support this well
                // So just return the int up to the dot
                let end: i64 = saved_pos;
                let text: i64 = string_slice(lexer_source(lexer), start, end);
                // Reset position to before the dot (lexer_pos is now at saved_pos+1, we need saved_pos)
                store_i64(lexer, 1, saved_pos);
                return token_new_loc(TokenKind::Int, text, start, start_line, start_col);
            }
        }
        let end: i64 = lexer_pos(lexer);
        let text: i64 = string_slice(lexer_source(lexer), start, end);
        return token_new_loc(TokenKind::Int, text, start, start_line, start_col);
    }

    // String literals - build string with escape sequence conversion
    if c == 34 {
        let text: i64 = string_from("");
        while lexer_peek(lexer) != 34 {
            if lexer_at_end(lexer) {
                return token_new_loc(TokenKind::Eof, string_from(""), start, start_line, start_col);
            }
            let sc: i64 = lexer_advance(lexer);
            // Handle escape sequences
            if sc == 92 {
                if lexer_at_end(lexer) == false {
                    let esc: i64 = lexer_advance(lexer);
                    if esc == 110 {
                        // \n -> newline (ASCII 10)
                        text = string_concat(text, string_from_char(10));
                    } else {
                        if esc == 116 {
                            // \t -> tab (ASCII 9)
                            text = string_concat(text, string_from_char(9));
                        } else {
                            if esc == 34 {
                                // \" -> quote
                                text = string_concat(text, string_from_char(34));
                            } else {
                                if esc == 92 {
                                    // \\ -> backslash
                                    text = string_concat(text, string_from_char(92));
                                } else {
                                    // Unknown escape - just add the char
                                    text = string_concat(text, string_from_char(esc));
                                }
                            }
                        }
                    }
                }
            } else {
                text = string_concat(text, string_from_char(sc));
            }
        }
        lexer_advance(lexer);
        return token_new_loc(TokenKind::String, text, start, start_line, start_col);
    }

    // @ symbol for belief guards (TASK-013-A)
    if c == 64 {
        return token_new_loc(TokenKind::At, string_from("@"), start, start_line, start_col);
    }

    // Unknown character - return EOF for now
    return token_new_loc(TokenKind::Eof, string_from(""), start, start_line, start_col);
}

// Tokenize entire source into a list
fn tokenize(source: i64) -> i64 {
    let lexer: i64 = lexer_new(source);
    let tokens: i64 = vec_new();

    while true {
        let tok: i64 = lexer_next_token(lexer);
        vec_push(tokens, tok);
        if token_kind(tok) == TokenKind::Eof {
            return tokens;
        }
    }

    return tokens;
}
